{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ighus/black_friday_pratice_problem/blob/master/predict_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX51-ZSxGjyf"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração de ambiente"
      ],
      "metadata": {
        "id": "IK8uai2vJ6t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  DEV_AMBIENT = 'colab'\n",
        "else:\n",
        "  DEV_AMBIENT = 'local'"
      ],
      "metadata": {
        "id": "IpV3BTJbKIWW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lCKX8eKpGjyg"
      },
      "outputs": [],
      "source": [
        "if DEV_AMBIENT == 'local':\n",
        "  # Configuração se deve ser utilizado a placa de video ou processador, \n",
        "  # se for descomentado, usará o processador\n",
        "  # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "  # print(\"Gpu: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "  # Configuração de limitador de uso da memória da placa de video [precisa ser ajustado]\n",
        "  folder_path = 'train.csv'\n",
        "else:\n",
        "  # from google.colab import drive\n",
        "  # from zipfile import ZipFile\n",
        "  # drive.mount('/content/gdrive')\n",
        "\n",
        "  # !unzip \"gdrive/MyDrive/Igor.ffp/Projetos DS/Bases/tcc-image-analysis-cluster/data.zip\" > /dev/null\n",
        "  # !unzip \"gdrive/MyDrive/Igor.ffp/Projetos DS/Bases/tcc-image-analysis-cluster/train_images.zip\" > /dev/null\n",
        "\n",
        "  folder_path = 'https://raw.githubusercontent.com/Ighus/black_friday_pratice_problem/master/train.csv'\n",
        "\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from termcolor import colored\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XZZoGga9Gjyh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(folder_path)\n",
        "# test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TRhfwol1Gjyh",
        "outputId": "72b25f2f-8b5d-4690-b776-1d11d83962e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 550068 entries, 0 to 550067\n",
            "Data columns (total 12 columns):\n",
            " #   Column                      Non-Null Count   Dtype  \n",
            "---  ------                      --------------   -----  \n",
            " 0   User_ID                     550068 non-null  int64  \n",
            " 1   Product_ID                  550068 non-null  object \n",
            " 2   Gender                      550068 non-null  object \n",
            " 3   Age                         550068 non-null  object \n",
            " 4   Occupation                  550068 non-null  int64  \n",
            " 5   City_Category               550068 non-null  object \n",
            " 6   Stay_In_Current_City_Years  550068 non-null  object \n",
            " 7   Marital_Status              550068 non-null  int64  \n",
            " 8   Product_Category_1          550068 non-null  int64  \n",
            " 9   Product_Category_2          376430 non-null  float64\n",
            " 10  Product_Category_3          166821 non-null  float64\n",
            " 11  Purchase                    550068 non-null  int64  \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 50.4+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UYjqccSRGjyi",
        "outputId": "2dd64d8c-1672-49e8-975e-a28035378527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            User_ID     Occupation  Marital_Status  Product_Category_1  \\\n",
              "count  5.500680e+05  550068.000000   550068.000000       550068.000000   \n",
              "mean   1.003029e+06       8.076707        0.409653            5.404270   \n",
              "std    1.727592e+03       6.522660        0.491770            3.936211   \n",
              "min    1.000001e+06       0.000000        0.000000            1.000000   \n",
              "25%    1.001516e+06       2.000000        0.000000            1.000000   \n",
              "50%    1.003077e+06       7.000000        0.000000            5.000000   \n",
              "75%    1.004478e+06      14.000000        1.000000            8.000000   \n",
              "max    1.006040e+06      20.000000        1.000000           20.000000   \n",
              "\n",
              "       Product_Category_2  Product_Category_3       Purchase  \n",
              "count       376430.000000       166821.000000  550068.000000  \n",
              "mean             9.842329           12.668243    9263.968713  \n",
              "std              5.086590            4.125338    5023.065394  \n",
              "min              2.000000            3.000000      12.000000  \n",
              "25%              5.000000            9.000000    5823.000000  \n",
              "50%              9.000000           14.000000    8047.000000  \n",
              "75%             15.000000           16.000000   12054.000000  \n",
              "max             18.000000           18.000000   23961.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34f79066-b9a9-4133-a1ad-32bf0cd32705\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.500680e+05</td>\n",
              "      <td>550068.000000</td>\n",
              "      <td>550068.000000</td>\n",
              "      <td>550068.000000</td>\n",
              "      <td>376430.000000</td>\n",
              "      <td>166821.000000</td>\n",
              "      <td>550068.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.003029e+06</td>\n",
              "      <td>8.076707</td>\n",
              "      <td>0.409653</td>\n",
              "      <td>5.404270</td>\n",
              "      <td>9.842329</td>\n",
              "      <td>12.668243</td>\n",
              "      <td>9263.968713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.727592e+03</td>\n",
              "      <td>6.522660</td>\n",
              "      <td>0.491770</td>\n",
              "      <td>3.936211</td>\n",
              "      <td>5.086590</td>\n",
              "      <td>4.125338</td>\n",
              "      <td>5023.065394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000001e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.001516e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5823.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.003077e+06</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>8047.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.004478e+06</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12054.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.006040e+06</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>23961.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34f79066-b9a9-4133-a1ad-32bf0cd32705')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34f79066-b9a9-4133-a1ad-32bf0cd32705 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34f79066-b9a9-4133-a1ad-32bf0cd32705');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pk2BnXd4Gjyi",
        "outputId": "7f879319-8eda-4b09-dc39-1046f38dd042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
              "0  1000001  P00069042      F  0-17          10             A   \n",
              "1  1000001  P00248942      F  0-17          10             A   \n",
              "2  1000001  P00087842      F  0-17          10             A   \n",
              "3  1000001  P00085442      F  0-17          10             A   \n",
              "4  1000002  P00285442      M   55+          16             C   \n",
              "\n",
              "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
              "0                          2               0                   3   \n",
              "1                          2               0                   1   \n",
              "2                          2               0                  12   \n",
              "3                          2               0                  12   \n",
              "4                         4+               0                   8   \n",
              "\n",
              "   Product_Category_2  Product_Category_3  Purchase  \n",
              "0                 NaN                 NaN      8370  \n",
              "1                 6.0                14.0     15200  \n",
              "2                 NaN                 NaN      1422  \n",
              "3                14.0                 NaN      1057  \n",
              "4                 NaN                 NaN      7969  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7dbb09-f0d9-44c1-a6d9-3ebfc0c69586\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Product_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00069042</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00248942</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00087842</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00085442</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000002</td>\n",
              "      <td>P00285442</td>\n",
              "      <td>M</td>\n",
              "      <td>55+</td>\n",
              "      <td>16</td>\n",
              "      <td>C</td>\n",
              "      <td>4+</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7dbb09-f0d9-44c1-a6d9-3ebfc0c69586')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b7dbb09-f0d9-44c1-a6d9-3ebfc0c69586 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b7dbb09-f0d9-44c1-a6d9-3ebfc0c69586');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nM3nk-3Gjyi"
      },
      "source": [
        "Varificação se existem dados faltantes nas colunas do dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GvFoKRzFGjyi",
        "outputId": "0eff3823-2623-4bcd-8e9c-43a0b1ef6836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "User_ID                            0\n",
              "Product_ID                         0\n",
              "Gender                             0\n",
              "Age                                0\n",
              "Occupation                         0\n",
              "City_Category                      0\n",
              "Stay_In_Current_City_Years         0\n",
              "Marital_Status                     0\n",
              "Product_Category_1                 0\n",
              "Product_Category_2            173638\n",
              "Product_Category_3            383247\n",
              "Purchase                           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lIbeq0rGjyj"
      },
      "source": [
        "Como podemos observar, as colunas Product_Category_2 e Product_Category_3 apresentam um grande volume de dados faltantes e, essa informação não pode ser descartado, por isso é preciso pensar em estratégias para essas colunas, uma delas seria dropar ambas, mas talvez esse não seria o melhor forma de lidar com a situação. Antes de tomar uma decisão, será feita uma verificação nesses dados para validar a melhor estratégia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4GWIH8OPGjyj",
        "outputId": "9e7cd103-610a-4620-cdf3-fe293f691ee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se optasse por remover as colunas com dados faltantes seriam retiradas um total de \u001b[31m556885\u001b[0m de \u001b[34m550068\u001b[0m linhas no total presentes no dataframe estudado\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"Se optasse por remover as colunas com dados faltantes seriam retiradas um total de {colored(df.Product_Category_2.isnull().sum() + df.Product_Category_3.isnull().sum(), 'red')} de {colored(len(df), 'blue')} linhas no total presentes no dataframe estudado\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wRDXw3Gjyj"
      },
      "source": [
        "Como podemos ver seriam retiradas mais linhas do que existem no dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLlidPzmGjyj"
      },
      "source": [
        "Agora podemos observar os dados das colunas e montar um nova estratégia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HTo0nDGOGjyj",
        "outputId": "e1f8c889-806d-4f8d-bbf3-ff384f7718c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([nan,  6., 14.,  2.,  8., 15., 16., 11.,  5.,  3.,  4., 12.,  9.,\n",
              "       10., 17., 13.,  7., 18.])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([nan, 14., 17.,  5.,  4., 16., 15.,  8.,  9., 13.,  6., 12.,  3.,\n",
              "       18., 11., 10.])"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(\n",
        "  df.Product_Category_2.unique(),\n",
        "  df.Product_Category_3.unique()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUWAnsCoGjyk"
      },
      "source": [
        "Existem algumas classes mas não existe nenhuma com 0, talvez seja uma boa estratégia utilizar essa classe como classe inválida para aqueles dados que não possuem dados, assim o dataframe seria totalmente preenchido e saberíamos que as classes 0 nessas colunas representariam linhas sem classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hm8m33ntGjyk",
        "outputId": "7c34eff0-fa17-4005-e60e-4a215eb13e53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "User_ID                       0\n",
              "Product_ID                    0\n",
              "Gender                        0\n",
              "Age                           0\n",
              "Occupation                    0\n",
              "City_Category                 0\n",
              "Stay_In_Current_City_Years    0\n",
              "Marital_Status                0\n",
              "Product_Category_1            0\n",
              "Product_Category_2            0\n",
              "Product_Category_3            0\n",
              "Purchase                      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.Product_Category_2.fillna(0, inplace=True)\n",
        "df.Product_Category_3.fillna(0, inplace=True)\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NjPKzsQvGjyk",
        "outputId": "321bfccc-62e9-43f4-8d34-a057a6782113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "User_ID                         int64\n",
              "Product_ID                     object\n",
              "Gender                         object\n",
              "Age                            object\n",
              "Occupation                      int64\n",
              "City_Category                  object\n",
              "Stay_In_Current_City_Years     object\n",
              "Marital_Status                  int64\n",
              "Product_Category_1              int64\n",
              "Product_Category_2            float64\n",
              "Product_Category_3            float64\n",
              "Purchase                        int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMYVH-jYGjyk"
      },
      "source": [
        "As colunas que foram adicionadas 0 estam como float devido aos valores nulos que haviam previamente, como a coluna apresenta categorias, faria sentido passar esses dados para int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uKMl0Sr7Gjyk"
      },
      "outputs": [],
      "source": [
        "df.Product_Category_2 = df.Product_Category_2.astype('int64')\n",
        "df.Product_Category_3 = df.Product_Category_3.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pJsHwZk-Gjyl",
        "outputId": "1f9730e6-9dcc-4bdb-b3d7-d2ae6fa8acf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "User_ID                        int64\n",
              "Product_ID                    object\n",
              "Gender                        object\n",
              "Age                           object\n",
              "Occupation                     int64\n",
              "City_Category                 object\n",
              "Stay_In_Current_City_Years    object\n",
              "Marital_Status                 int64\n",
              "Product_Category_1             int64\n",
              "Product_Category_2             int64\n",
              "Product_Category_3             int64\n",
              "Purchase                       int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBONH3SxGjyl"
      },
      "source": [
        "#### Análise dos campos User_ID e Product_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuGH3lKnGjyl"
      },
      "source": [
        "Agora vamos checar o as informações em relação aos produtos e usuários, primeiro verificando quantos usuários e quantos produtos temos únicos no dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ofWl5xdlGjyl",
        "outputId": "4632ba13-aed1-42aa-f72c-d3e4c2dd24f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existem \u001b[33m5891\u001b[0m usuários únicos no dataframe e \u001b[33m3631\u001b[0m produtos únicos no dataframe\n"
          ]
        }
      ],
      "source": [
        "print(f\"Existem {colored(len(df.User_ID.unique()), 'yellow')} usuários únicos no dataframe e {colored(len(df.Product_ID.unique()), 'yellow')} produtos únicos no dataframe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "208ggC5jGjyl"
      },
      "source": [
        "#### Análise dos campos Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMj_2cHoGjyl"
      },
      "source": [
        "Agora vamos observar as outras colunas e quais informações podemos retirar delas.\n",
        "O comando head, info e dtype já nos deu um spoiler de como esses campos são e como foram representados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fPYB4n7xGjym",
        "outputId": "5301c129-80d1-4e8c-9626-0c84ea606338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F', 'M']\n",
            "['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+']\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "['A', 'B', 'C']\n",
            "['0', '1', '2', '3', '4+']\n"
          ]
        }
      ],
      "source": [
        "for col_name in ['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years']:\n",
        "    print(sorted(df[col_name].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmatogogGjym"
      },
      "source": [
        "O campo gênero é binário e contém os sexos Feminino e Masculino representados em forma de caracter, já o campo de idades é do tipo string e apresenta um range de idades que o comprador se encaixa. A categoria da cidade pode representar uma classificação interna que talvez pode variar de acordo com o número de compradores em cada cidade e, fazendo um tipo de ranking de A a C. Vamos verificar em qual cidade existem mais vendas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "htXVAlR4Gjym",
        "outputId": "660c5374-b98e-46d4-b69b-3e8e74ea2d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEICAYAAACZJtWMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPL0lEQVR4nO3de6ylVXnH8e/jjAxyGy6DdArIgaIFhRZhYqEBvMRymz+ExBqmqSDVTGtr05b0MlTa0Bhxams1WFvAloZLW9BSWhNEqkCaYij0jOGuCKODOAxX6wBFCQxP/3jXwc2Zs8+Nvc85D+f7Sd6cd6937fWutWbv33nPevc5E5mJJGlhe818d0CSNDXDWpIKMKwlqQDDWpIKMKwlqQDDWpIKMKw1ZyLijyPi7+a7H1JFhrUGLiJ+JSJGI+KZiNgSEddFxLGZeX5mfqjVGYmIjIilAzjfbhHxmYj4XjvnxvZ4xTSe+4GIuPmV9kEaNsNaAxURZwOfAc4H9gHeAPwN8J4hnW8H4AbgLcBJwG7AMcCTwNuGcc5BGcQ3Ki0imenmNpANWA48A/xyn+PnAVe0/e8B2eo/A7wd+AFweE/91wPPAntPcs4PAY8Cu0xSZx2wEXgauBc4rZUfCvwY2Nb68MNWvgz4y9bHR4ELgdf1tPeHwBbg4Xb+BA7umYPLgMeBB4Fzgde0Yx8Avg58mu6byfmzGbPb4ty8stYgHQPsCFwzjbrHt6+7Z+YumfmfwJXAr/bUWQPckJmPT9LOu4GvZOYzk9TZCBxHF6R/BlwRESsz85vAbwC3tD7s3uqvB94EHAEcDOwL/ClARJwEnN3OezDwjnHn+mw7z0F034DOAM7qOf4LwHfofur42CzHrEXIsNYg7QU8kZkvzPL5lwJrIiLa4/cDl0/jnFsmq5CZX8zMhzPzxcy8CrifPksk7dxrgd/LzB9k5tN0V8CntyrvA/4hM+/JzGfpfloYe+6SVu+czHw6MzcBn2rjGPNwZn42M1/IzB/NcsxahFwz0yA9CayIiKWzCezMvDUingXeERFb6K5cvzSNc66crEJEnEF3NTzSinYB+t183BvYCdjwk/wkgCVt/6eB0Z76D/XsrwBeS7f8MeZBuivzierPdsxahLyy1iDdAjwHnDqNuv3+3OOldMsC7wf+JTN/PEU7XwNOjIidJzoYEQcAnwc+AuzVljrupgvgifrxBPAj4C2ZuXvblmfmLu34FmC/nvr7j3vu88ABPWVvADb3PJ5o3DMdsxYhw1oDk5lb6dZ2PxcRp0bEThHx2og4OSI+Oa7648CLdGu7va4ATqMLr8umcdrL6a5Wr46IQyLiNRGxV/tM9ynAznQB+ThARJwFHNbz/EeB/dqnSsjMF+nC/dMR8fr2nH0j4sRW/wvAWRFxaETsBPxJz/i3teMfj4hd2zeKs9uYJjPTMWsRMqw1UJn5KbqAOpcuIB+iu6r9t3H1ngU+Dnw9In4YEUe38oeAb9AF7H9N43zP0d3s+xbwVeAp4Da6JYlbM/NeunXjW+iC+XC6T2SMuRG4B3gkIp5oZX8EPAD8d0Q8RXf1/rPtfNcBFwA3jdVpz3muff1t4P/obiLeDPwTcMkUY5jRmLU4Rab/+YAWloi4hO5G3Lnz3ZepRMShdMsqy17BjdVSY9b8MKy1oETECHA78NbM/O789mZiEXEa8GW6G5GXAi9m5nTW6fu1N8ICH7Pmn8sgWjAi4mN0V6l/0Rtabf35mQm26+apq78OPEb3+e1twIdn21C/MUvjeWUtSQV4ZS1JBQzll2JWrFiRIyMjw2hakl61NmzY8ERm7j3RsaGE9cjICKOjo1NXlCS9JCIe7HfMZRBJKsCwlqQCDGtJKsCwlqQCDGtJKsCwlqQCDGtJKsCwlqQCDGtJKsCwlqQCDGtJKsCwlqQCDGtJKsCwlqQCDGtJKsCwlqQChvKfD9y1eSsj664dRtPSq86m9avnuwsqwCtrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAqYV1hFxakRkRBwy7A5JkrY33SvrNcDN7askaY5NGdYRsQtwLPBB4PSh90iStJ3pXFm/B/hKZn4beDIijhpynyRJ40wnrNcAV7b9K+mzFBIRayNiNCJGtz27dVD9kyQxxf/BGBF7Au8CDo+IBJYAGRF/kJnZWzczLwYuBli28o25XWOSpFmb6sr6vcDlmXlAZo5k5v7Ad4Hjht81SdKYqcJ6DXDNuLKr8VMhkjSnJl0Gycx3TlB2wfC6I0maiL/BKEkFGNaSVIBhLUkFGNaSVIBhLUkFGNaSVIBhLUkFGNaSVIBhLUkFGNaSVIBhLUkFGNaSVIBhLUkFTPpX92br8H2XM7p+9TCalqRFyStrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSrAsJakAgxrSSpg6TAavWvzVkbWXTuMpiUN2Kb1q+e7C5oGr6wlqQDDWpIKMKwlqQDDWpIKMKwlqQDDWpIKMKwlqQDDWpIKMKwlqQDDWpIKMKwlqQDDWpIKMKwlqQDDWpIKmFZYR8RPRcSVEbExIjZExJcj4k3D7pwkqTPl37OOiACuAS7NzNNb2c8D+wDfHm73JEkwvf984J3A85l54VhBZt4xvC5JksabzjLIYcCGqSpFxNqIGI2I0W3Pbn3lPZMkvWRgNxgz8+LMXJWZq5bstHxQzUqSmF5Y3wMcNeyOSJL6m05Y3wgsi4i1YwUR8XMRcdzwuiVJ6jVlWGdmAqcB724f3bsH+ATwyLA7J0nqTOfTIGTmw8D7htwXSVIf/gajJBVgWEtSAYa1JBVgWEtSAYa1JBVgWEtSAYa1JBVgWEtSAYa1JBVgWEtSAYa1JBVgWEtSAdP6Q04zdfi+yxldv3oYTUvSouSVtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVYFhLUgGGtSQVsHQYjd61eSsj664dRtOStGBtWr96aG17ZS1JBRjWklSAYS1JBRjWklSAYS1JBRjWklSAYS1JBRjWklSAYS1JBRjWklSAYS1JBRjWklSAYS1JBRjWklTAlGEdEdsi4vaIuCMivhERvzgXHZMk/cR0/p71jzLzCICIOBH4BPD2ofZKkvQyM10G2Q3432F0RJLU33SurF8XEbcDOwIrgXcNt0uSpPFmugxyDHBZRByWmdlbKSLWAmsBluy298A7KkmL2YyWQTLzFmAFsF0aZ+bFmbkqM1ct2Wn5oPonSWKGYR0RhwBLgCeH0x1J0kRmsmYNEMCZmbltiH2SJI0zZVhn5pK56IgkqT9/g1GSCjCsJakAw1qSCjCsJakAw1qSCjCsJakAw1qSCjCsJakAw1qSCjCsJakAw1qSCjCsJakAw1qSCpjOn0idscP3Xc7o+tXDaFqSFiWvrCWpAMNakgowrCWpAMNakgowrCWpAMNakgowrCWpAMNakgowrCWpAMNakgowrCWpAMNakgowrCWpAMNakgowrCWpAMNakgowrCWpgMjMwTca8TRw38Abrm0F8MR8d2KBcU6255xsbzHNyQGZufdEB4by33oB92XmqiG1XVJEjDonL+ecbM852Z5z0nEZRJIKMKwlqYBhhfXFQ2q3Mudke87J9pyT7TknDOkGoyRpsFwGkaQCDGtJKmDgYR0RJ0XEfRHxQESsG3T78y0iNkXEXRFxe0SMtrI9I+KrEXF/+7pHK4+IuKDNxZ0RcWRPO2e2+vdHxJk95Ue19h9oz425H+XkIuKSiHgsIu7uKRv6HPQ7x0LQZ07Oi4jN7bVye0Sc0nPsnDa++yLixJ7yCd8/EXFgRNzayq+KiB1a+bL2+IF2fGRuRjy1iNg/Im6KiHsj4p6I+J1WvqhfK7OWmQPbgCXARuAgYAfgDuDNgzzHfG/AJmDFuLJPAuva/jrgz9v+KcB1QABHA7e28j2B77Sve7T9Pdqx21rdaM89eb7HPMEcHA8cCdw9l3PQ7xwLYeszJ+cBvz9B3Te398Yy4MD2nlky2fsH+AJwetu/EPhw2/9N4MK2fzpw1XzPRc84VwJHtv1dgW+3sS/q18qs53PA/zjHANf3PD4HOGe+BzngMW5i+7C+D1jZ9lfS/VIQwEXAmvH1gDXART3lF7WylcC3espfVm8hbcDIuGAa+hz0O8dC2SaYk/OYOKxf9r4Arm/vnQnfPy2IngCWtvKX6o09t+0vbfVivueiz/z8O/BLvlZmtw16GWRf4KGex99vZa8mCfxHRGyIiLWtbJ/M3NL2HwH2afv95mOy8u9PUF7BXMxBv3MsZB9pP9Jf0vOj+EznZC/gh5n5wrjyl7XVjm9t9ReUtjzzVuBWfK3MijcYZ+7YzDwSOBn4rYg4vvdgdt/KF/XnIediDorM898CPwMcAWwBPjW/3ZkfEbELcDXwu5n5VO8xXyvTN+iw3gzs3/N4v1b2qpGZm9vXx4BrgLcBj0bESoD29bFWvd98TFa+3wTlFczFHPQ7x4KUmY9m5rbMfBH4PN1rBWY+J08Cu0fE0nHlL2urHV/e6i8IEfFauqD+x8z811bsa2UWBh3W/wO8sd253oHuhseXBnyOeRMRO0fErmP7wAnA3XRjHLtDfSbd2hyt/Ix2l/toYGv70ex64ISI2KP9aHwC3RrkFuCpiDi63dU+o6ethW4u5qDfORaksbBoTqN7rUA3jtPbJzkOBN5Id6NswvdPuzK8CXhve/74+R2bk/cCN7b68679+/098M3M/KueQ75WZmMINxFOobvruxH46Hwvyg94bAfR3aG/A7hnbHx0a4Q3APcDXwP2bOUBfK7NxV3Aqp62fg14oG1n9ZSvontTbwT+mgV4swj4Z7of65+nWyf84FzMQb9zLIStz5xc3sZ8J114rOyp/9E2vvvo+cRPv/dPe+3d1ubqi8CyVr5je/xAO37QfM9FT5+PpVt+uBO4vW2nLPbXymw3f91ckgrwBqMkFWBYS1IBhrUkFWBYS1IBhrUkFWBYS1IBhrUkFfD/I1La4xc7Z0QAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df['City_Category'].value_counts().plot(kind='barh', title='City_Category');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTevAvcGjym"
      },
      "source": [
        "Como pudemos ver, talvez essa categorização não represente um ranking de fato, pois não existem um valor crescente de acordo com a ordem alfabética das cidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14UcdXPBGjym"
      },
      "source": [
        "Agora que foi observado a categoria da cidade, podemos visualizar os demais dados e observar suas distribuições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4Z3Cqi9NGjym",
        "outputId": "d5803608-f5e0-4980-9434-f19fb71e80ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2MAAANeCAYAAACf1dZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5hlZ1kn7N9Dd4wEkuaQADGJNIcgKIEIbQCVo8MxjqCiE0QJKEZQVNDRCeoHgRnmiggyKI4QIEQxoCCgmEQBRQS+EbAjgQTDmcakCYTDpDkEkCTP/LFXy05b1enqrqq3quu+r2tfvfa7Ts9etbtW/fb7rrWruwMAAMDqutHoAgAAADYiYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQxWUVU9vqreOboOAADGE8YgSVWdUlXvrqqvVNWV0/TPV1WNrg0A1oOqeltV/d+qOnR0LbBeCGNseFX1q0lemOR3ktwmya2TPCnJ9yX5loGlXU9VbRpdAwAspKq2Jrlvkk7yQ0OLgXVEGGNDq6otSZ6d5Oe7+8+7+0s9897ufmx3f72qDq2q51XVv1bVZ6rqxVV142n9B1TV5VX1q1OP2hVV9YS57d+yqt5YVV+sqvckucMe+79zVb2lqr5QVR+qqh+fm3dOVf1hVV1QVV9J8sDVOSoAsGSPS/KuJOckOXV343Qe/KvpPPhPVfU/5ofr7+08CBuBMMZGd58khyb5y70sc2aSOyU5MckdkxyT5Blz82+TZMvU/jNJ/qCqbj7N+4MkX0tydJKfnh5Jkqq6SZK3JHlVklslOSXJ/66q75zb9k8keU6Sw5O41gyAtepxSc6dHg+tqltP7X+Q5CuZnStPzfWD2r6cB+GgJoyx0R2Z5HPdfc3uhqr6P1V1VVV9tarun+S0JE/r7i9095eS/M/MThi7fSPJs7v7G919QZIvJ/mOaVjhjyZ5Rnd/pbsvSfJHc+v9YJId3f2K7r6mu9+b5HVJfmxumb/s7v+/u6/r7q+twOsHgANSVd+f5LZJXtPdFyb5WJKfmDsPPrO7r+7uf8nSz4NwUNs8ugAY7PNJjqyqzbsDWXd/b5JU1eWZXT92WJIL5+7lUUnmr9/6/HyYS3J1kpsmOSqz/2OXzc375Nz0bZPcq6qummvbnOSVc8/n1wWAtejUJG/u7s9Nz181tb06//E8OD+9L+dBOKgJY2x0/5jk60kemdmncXv6XJKvJvmu7t65xG1/Nsk1SY5L8sGp7dvn5l+W5B+6+8F72UYvcZ8AsGqma6h/PMmmqvr01Hxokptl9oHmNUmOTfLhad5xc6vvy3kQDmqGKbKhdfdVSZ6V2Rj1R1fV4VV1o6o6MclNklyX5KVJXlBVt0qSqjqmqh66D9u+Nsnrk5xRVYdNY+BPnVvkvCR3qqqfqqpDpsf3VNVdlvllAsBKeVSSa5N8Z2bXVp+Y5C5J3pHZdWTz58E7T227OQ+y4QljbHjd/dwkv5Lk15N8Znq8JMl/S/J/pn8/muRdVfXFJH+b5Dv2cfNPyWzI4qczu8PUK+b2+6UkD8ns+rNPTcv8dmafKALAenBqkld0979296d3P5K8KMljMzsPbsnsHPfKzIYufj1xHoQkqW6joAAAWHlV9dtJbtPdp97gwrAB6BkDAGBFTN8jdreaOSmzr4B5w+i6YK1wAw8AAFbK4ZkNTfy2zC4DeH72/t2esKEYpggAADCAYYoAAAADrOgwxSOPPLK3bt26krsAYA248MILP9fdR42uY71wfgTYOPZ2jlzRMLZ169Zs3759JXcBwBpQVZ8cXcN64vwIsHHs7RxpmCIAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAm0cXAAAbzcU7d2Xr6eev6D52nHnyim4fgAOnZwwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQyADa+qHlZVH6qqj1bV6Yss8zdVdVVVnbdH+zuq6qLp8amq+ovVqRqA9W7z6AIAYKSq2pTkD5I8OMnlSf6pqt7Y3f+yx6K/k+SwJD8339jd953b1uuS/OXKVgzAwULPGAAb3UlJPtrdH+/uf0vyp0keuedC3f13Sb602Eaq6ogkD0qiZwyAfSKMAbDRHZPksrnnl09tS/WoJH/X3V9caGZVnVZV26tq+7VX79qPzQNwsBHGAGB5PCbJqxeb2d1ndfe27t626bAtq1gWAGuVMAbARrczyXFzz49N8tm5m3L80A1toKqOzGy44/krVCMAByE38ABgo/unJMdX1e0yC2anJPmJ7n7WErbx6CTndffXVqJAAA5OesYA2NC6+5okT0nypiSXJnlNd39gz+Wq6h1JXpvkB6rq8qp66NzsU7KXIYoAsBA9YwBseN19QZILbmCZ++5l3gOWuyYADn56xgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAbwPWMAsMpOOGZLtp958ugyABhMzxgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAK3pr+4t37srW089fyV0wyA63ZAYAgAOiZwwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGAAYQwAAGCAzaMLAICN5uKdu7L19PNHl/Hvdpx58ugSADYkPWMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAMKmqHVV1cVVdVFXbp7Yzqmrn1HZRVT1idJ0AHBw2jy4AANaYB3b35/Zoe0F3P2+xFarqjCQ7uvuclSwMgIOLnjEAAIABhDEA+KZO8uaqurCqTptrf0pVvb+qzq6qm+/PhqvqtKraXlXbr7161/JUC8C6JowBwDd9f3ffI8nDk/xCVd0vyR8muUOSE5NckeT5SVJVJ+y+jizJk5I8e+66slvuueHuPqu7t3X3tk2HbVm1FwTA2uWaMQCYdPfO6d8rq+oNSU7q7rfvnl9VL01y3rTMxZkFNNeMAbBf9IwBQJKquklVHb57OslDklxSVUfPLfbDSS4ZUR8ABx89YwAwc+skb6iqZHZ+fFV3/01VvbKqTszserIdSX5uXIkAHEyEMQBI0t0fT3L3Bdp/ah/WPWMlagLg4GaYIgAAwADCGAAAwADCGAAAwADCGAAAwABLuoFHVV2b5OK5pkd1945lrQgAAGADWOrdFL/a3SeuSCUAAAAbiGGKAAAAAyy1Z+zGVXXRNP2J7v7hPReoqtOSnJYkm4446gDLA4CDzwnHbMn2M08eXQYAgy37MMXuPivJWUly6NHH9/4WBgAAcDAzTBEAAGAAYQwAAGAAYQwAAGCAJYWx7r7pShUCAACwkegZAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGGDz6AIAYKO5eOeubD39/NFlDLPjzJNHlwCwJugZAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYA2DDqKodVXVxVV1UVdunth+rqg9U1XVVtW0v655RVTundS+qqkdM7Vur6qtz7S9erdcDwPq2eXQBALDKHtjdn5t7fkmSH0nykn1Y9wXd/bwF2j/W3ScuS3UAbBjCGAAbWndfmiRVNboUADYYwxQB2Eg6yZur6sKqOm0/1n9KVb2/qs6uqpvPtd+uqt5bVf9QVfddaMWqOq2qtlfV9muv3rVfxQNwcBHGANhIvr+775Hk4Ul+oarut4R1/zDJHZKcmOSKJM+f2q9I8u3d/d1JfiXJq6rqiD1X7u6zuntbd2/bdNiWA3oRABwchDEANozu3jn9e2WSNyQ5abFlq+oV0w05LpjW+Ux3X9vd1yV56e51u/vr3f35afrCJB9LcqeVfSUAHAyEMQA2hKq6SVUdvns6yUMyu3nHgrr7Cd19Ynfvvmvi0XOzf3j3ulV1VFVtmqZvn+T4JB9fmVcBwMHEDTwA2ChuneQN0406Nid5VXf/TVX9cJLfT3JUkvOr6qLufugC6z+3qk7M7LqzHUl+bmq/X5JnV9U3klyX5End/YWVfSkAHAyEMQA2hO7+eJK7L9D+hsyGLN7Q+j+1SPvrkrzugAsEYMNZ0TB2wjFbsv3Mk1dyFwAAAOuSa8YAAAAGEMYAAAAGEMYAAAAGEMYAAAAGEMYAAAAGEMYAAAAG8D1jALDKfPULAImeMQAAgCGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAE2jy4AADaai3fuytbTzx9dxrqx48yTR5cAsCL0jAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAGwoVTVpqp6b1WdNz2vqnpOVX24qi6tql9aZL1zquoTVXXR9Dhxbv3fq6qPVtX7q+oeq/l6AFi/No8uAABW2S8nuTTJEdPzxyc5Lsmdu/u6qrrVXtb9te7+8z3aHp7k+OlxryR/OP0LAHulZwyADaOqjk1ycpKXzTU/Ocmzu/u6JOnuK5e42Ucm+eOeeVeSm1XV0ctSMAAHNWEMgI3kfyX59STXzbXdIcl/qartVfXXVXX8XtZ/zjQU8QVVdejUdkySy+aWuXxqu56qOm3ax/Zrr951gC8DgIOBMAbAhlBVP5jkyu6+cI9Zhyb5WndvS/LSJGcvsomnJ7lzku9Jcosk/20p++/us7p7W3dv23TYlqUVD8BBSRgDYKP4viQ/VFU7kvxpkgdV1Z9k1pP1+mmZNyS5W5JU1ZumG3W8LEm6+4ppKOLXk7wiyUnTOjszu+Zst2OnNgDYK2EMgA2hu5/e3cd299YkpyR5a3f/ZJK/SPLAabH7J/nwtPxDu/vE7n5ikuy+DqyqKsmjklwyrfPGJI+b7qp47yS7uvuK1XpdAKxf7qYIwEZ3ZpJzq+ppSb6c5ImLLHduVR2VpJJclORJU/sFSR6R5KNJrk7yhJUtF4CDhTAGwIbT3W9L8rZp+qrM7rB4Q+s8aJH2TvILy1geABuEYYoAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAAD+J4xAFhlJxyzJdvPvMGvNgPgIKdnDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYIDNowsAgI3m4p27svX080eXAf9ux5knjy4BNiQ9YwAAAAMIYwAAAAMIYwAAAAMIYwAAAAMIYwAAAAMIYwAAAAOs6K3t3boXYG1zO2sAGEfPGAAAwADCGAAAwADCGAAbQlWdXVVXVtUlc20nVtW7quqiqtpeVSctsu65VfWhqrpk2s4hU/sDqmrXtP5FVfWM1Xo9AKx/whgAG8U5SR62R9tzkzyru09M8ozp+ULOTXLnJCckuXGSJ87Ne0d3nzg9nr28JQNwMBPGANgQuvvtSb6wZ3OSI6bpLUk+tci6F/QkyXuSHLtihQKwYazo3RQBYI17apI3VdXzMvuA8nv3tvA0PPGnkvzyXPN9qup9mQW5/9rdH1ipYgE4uOgZA2Aje3KSp3X3cUmeluTlN7D8/07y9u5+x/T8n5PctrvvnuT3k/zFYitW1WnTdWnbr7161zKUDsB6J4wBsJGdmuT10/Rrk5yUJFX1pumGHC/bvWBVPTPJUUl+ZXdbd3+xu788TV+Q5JCqOnKhHXX3Wd29rbu3bTpsy8q8GgDWFcMUAdjIPpXk/kneluRBST6SJN390PmFquqJSR6a5Ae6+7q59tsk+Ux393Qnxhsl+fzqlA7AeieMAbAhVNWrkzwgyZFVdXmSZyb52SQvrKrNSb6W5LRFVn9xkk8m+ceqSpLXT3dOfHSSJ1fVNUm+muSU6SYfAHCDhDEANoTufswis+65D+sueL7s7hcledGB1AXAxuWaMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAGEMQAAgAF86TMArLITjtmS7WeePLoMAAbTMwYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADCAMAYAADDADYaxquqq+pO555ur6rNVdd7KlgYAAHDw2peesa8kuWtV3Xh6/uAkO1euJAAAgIPfvg5TvCDJydP0Y5K8emXKAQAA2Bg27+Nyf5rkGdPQxLslOTvJfRdasKpOS3Jakmw64qjlqBEADioX79yVraefP7oMWFY7zjz5hhcCrmefesa6+/1JtmbWK3bBDSx7Vndv6+5tmw7bcuAVAgAAHIT2tWcsSd6Y5HlJHpDklitSDQAAwAaxlDB2dpKruvviqnrACtUDAACwIexzGOvuy5P83grWAgAAsGHcYBjr7psu0Pa2JG9bgXoAAAA2hH29tT0AAADLSBgDAAAYQBgDAAAYQBgDAAAYQBgDYEOoqm+tqvdU1fuq6gNV9aypvarqOVX14aq6tKp+6Qa283tV9eW554+vqs9W1UXT44kr/VoAODgs5XvGAGA9+3qSB3X3l6vqkCTvrKq/TnKXJMcluXN3X1dVt1psA1W1LcnNF5j1Z939lBWpGoCDlp4xADaEntndo3XI9OgkT07y7O6+blruyoXWr6pNSX4nya+vQrkAbADCGAAbRlVtqqqLklyZ5C3d/e4kd0jyX6pqe1X9dVUdv8jqT0nyxu6+YoF5P1pV76+qP6+q41aofAAOMsIYABtGd1/b3ScmOTbJSVV11ySHJvlad29L8tIkZ++5XlV9W5IfS/L7C2z2r5Js7e67JXlLkj9aaN9VddoU+LZfe/Wu5XlBAKxrwhgAG053X5Xk75M8LMnlSV4/zXpDkrslSVW9abohx8uSfHeSOyb5aFXtSHJYVX102tbnu/vr0/ovS3LPRfZ5Vndv6+5tmw7bskKvDID1xA08ANgQquqoJN/o7quq6sZJHpzkt5P8RZIHJvlEkvsn+XCSdPdD99jEbea29eXuvuM0ffTc0MUfSnLpir4QAA4awhgAG8XRSf5ouhHHjZK8prvPq6p3Jjm3qp6W5MtJlnpr+l+qqh9Kck2SLyR5/DLWDMBBTBgDYEPo7vdnNtxwz/arkpy8xG3ddG766UmefsAFArDhuGYMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAF/6DACr7IRjtmT7mUv6nmkADkJ6xgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAbYvJIbP+GYLdl+5skruQsAAIB1Sc8YAADAAMIYAADAAMIYAADAAMIYAADAAMIYAADAACt6N0UA4D+6eOeubD39/NFlALAXO1bhrvB6xgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgAAAAYQxgDYEKrquKr6+6r6l6r6QFX98ty8X6yqD07tz11k/f9eVe+vqouq6s1V9W1T+wOqatfUflFVPWO1XhMA69vm0QUAwCq5Jsmvdvc/V9XhSS6sqrckuXWSRya5e3d/vaputcj6v9Pd/1+SVNUvJXlGkidN897R3T+4wvUDcJARxgDYELr7iiRXTNNfqqpLkxyT5GeTnNndX5/mXbnI+l+ce3qTJL2yFQNwsDNMEYANp6q2JvnuJO9Ocqck962qd1fVP1TV9+xlvedU1WVJHptZz9hu96mq91XVX1fVd61g6QAcRIQxADaUqrppktcleerU27U5yS2S3DvJryV5TVXVQut2929293FJzk3ylKn5n5PctrvvnuT3k/zFIvs9raq2V9X2a6/etayvCYD1SRgDYMOoqkMyC2Lndvfrp+bLk7y+Z96T5LokR1bVK6YbclywwKbOTfKjyWz4Ynd/eZq+IMkhVXXknit091ndva27t206bMsKvDoA1hthDIANYertenmSS7v7d+dm/UWSB07L3CnJtyT5XHc/obtP7O5HTPOOn1vnkUk+OLXfZndPWlWdlNm59fMr/XoAWP/cwAOAjeL7kvxUkour6qKp7TeSnJ3k7Kq6JMm/JTm1uxe6OceZVfUdmfWcfTLfvJPio5M8uaquSfLVJKcssj4AXI8wBsCG0N3vTLLgtWBJfnIf1v/RRdpflORFB1AaABuUYYoAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAADCGMAAAAD+NJnAFhlJxyzJdvPPHl0GQAMpmcMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABggOruldt41ZeSfGjFdrD8jkzyudFF7KP1VGui3pW0nmpN1LvSRtV72+4+asB+16V1eH5cy9bb/9G1ynFcPo7l8jiYjuOi58jNK7zjD3X3thXex7Kpqu3rpd71VGui3pW0nmpN1LvS1lu9G9i6Oj+uZd7zy8NxXD6O5fLYKMfRMEUAAIABhDEAAIABVjqMnbXC219u66ne9VRrot6VtJ5qTdS70tZbvRuVn9PycSyXh+O4fBzL5bEhjuOK3sADAACAhRmmCAAAMIAwBgAAMMCKhLGqelhVfaiqPlpVp6/EPm5g/zuq6uKquqiqtk9tt6iqt1TVR6Z/bz61V1X93lTr+6vqHnPbOXVa/iNVdepc+z2n7X90WreWWN/ZVXVlVV0y17bi9S22j/2o9Yyq2jkd34uq6hFz854+7fdDVfXQufYF3xNVdbuqevfU/mdV9S1T+6HT849O87fu47E9rqr+vqr+pao+UFW/vFaP715qXZPHt6q+tareU1Xvm+p91v7uY7lex37We05VfWLu+J44tQ97L8xtb1NVvbeqzlvLx5YDs9jPYqOrNX7uXstqHf1dsZYtchzX5Dl5Lat19LfYmtDdy/pIsinJx5LcPsm3JHlfku9c7v3cQA07khy5R9tzk5w+TZ+e5Len6Uck+eskleTeSd49td8iycenf28+Td98mveeadma1n34Euu7X5J7JLlkNetbbB/7UesZSf7rAst+5/TzPjTJ7ab3waa9vSeSvCbJKdP0i5M8eZr++SQvnqZPSfJn+3hsj05yj2n68CQfnupac8d3L7WuyeM7vd6bTtOHJHn3dByWtI/lfB37We85SR69wPJD/69Ny/5KklclOW9/fn6rdWw99v+xt5/FRn9kjZ+71/Ij6+jvirX8WOQ4npE1eE5ey4+so7/F1sJjJX4A90nyprnnT0/y9FV+E+zIf/yF/qEkR8+9ST40Tb8kyWP2XC7JY5K8ZK79JVPb0Uk+ONd+veWWUOPWPf6zr3h9i+1jP2o9Iwv/YrrezzrJm6b3w4Lviek/0OeSbN7zvbN73Wl687Rc7cdx/sskD17Lx3eBWtf88U1yWJJ/TnKvpe5jOV/HftZ7ThYOY0PfC0mOTfJ3SR6U5Lz9+fmNOLYeS3ss9rMYXddaeGQdnLvX8iPr6O+KtfxY4DiekTV+Tl7rj6yjv8VGPFZimOIxSS6be3751LaaOsmbq+rCqjptart1d18xTX86ya2n6cXq3Vv75Qu0H6jVqG+xfeyPp0xdyWfPdQEvtdZbJrmqu69ZoNZ/X2eav2tafp9N3fzfnVmPyJo+vnvUmqzR41uzYXQXJbkyyVsy+/RvqftYztexpHq7e/fxfc50fF9QVYfuWe8+1rXc74X/leTXk1w3Pd+fn9+qHVv221o4R65V6/HcvZat6fPeOrMmz8nrwXr6W2yUg/UGHt/f3fdI8vAkv1BV95uf2bO43EMq2werUd8B7uMPk9whyYlJrkjy/OWqa7lU1U2TvC7JU7v7i/Pz1trxXaDWNXt8u/va7j4xs16ck5LceXBJe7VnvVV118w+obxzku/JbOjDf1vhGm7wvVBVP5jkyu6+cCVrgTVuXZ+717K1dt5bZ9bsOXmtW09/i420EmFsZ5Lj5p4fO7Wtmu7eOf17ZZI3ZPZH42eq6ugkmf69clp8sXr31n7sAu0HajXqW2wfS9Ldn5n+yL0uyUszO777U+vnk9ysqjYvUOu/rzPN3zItf4Oq6pDM/vOf292vn5rX5PFdqNa1fnynGq9K8veZDbNY6j6W83Ustd6HdfcVPfP1JK/I/h/f5XwvfF+SH6qqHUn+NLOhii/MOji2LNnwc+RatU7P3WvZmjzvrTfr4Zy8Fq2nv8VGW4kw9k9Jjp/uGPMtmV2I+MYV2M+CquomVXX47ukkD0lyyVTDqdNip2Y2fnzj+h4AACAASURBVDVT++OmO7ncO8muqXvzTUkeUlU3n7qkH5LZ2N4rknyxqu493bnlcXPbOhCrUd9i+1iS3W/yyQ9ndnx3b/+Umt0V6HZJjs/sAssF3xPTJxZ/n+TRi7zu3bU+Oslbp+VvqLZK8vIkl3b3787NWnPHd7Fa1+rxraqjqupm0/SNMxv/fel+7GM5X8dS6/3g3C/pSvKoXP/4DnkvdPfTu/vY7t46ve63dvdj9/K6hx5bDsjQc+RatY7P3WvZmjvvrUdr9Zy8lq2nv8XWhL1dULa/j8zuivLhzK4n+c2V2Mde9n37zO5c874kH9i9/8zG3v5dko8k+dskt5jaK8kfTLVenGTb3LZ+OslHp8cT5tq3Zfaf8WNJXpQlXmSZ5NWZdXV/I7Nxrj+zGvUtto/9qPWVUy3vz+xNf/Tc8r857fdDmbtT1WLvienn9Z7pNbw2yaFT+7dOzz86zb/9Ph7b78+sS/r9SS6aHo9Yi8d3L7WuyeOb5G5J3jvVdUmSZ+zvPpbrdexnvW+dju8lSf4k37zj4tD/a3PbfEC+eTfFNXlsPQ7ssdjPYiM/sg7O3Wv5kXX0d8VafixyHNfkOXktP7KO/hZbC4/dhQMAALCKDtYbeAAAAKxpwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAMAAwhgAAOtCVX25qm4/ug5YLsIYG1pVffv0i33Tfq5/RlX9yXLXBQAroaoeX1UXV9XVVfXpqvrDqrrZ6LoWUlVvq6onzrd19027++OjaoLlJoyxblTVjqr6t6o6co/291ZVV9XWpW6zu/91+sV+7bSt//CLfzlU1c9U1Qer6ktV9ZmquqCqDp/mnVNV/2MJ23p8Vb1zuWsE4OBWVb+a5LeT/FqSLUnuneS2Sd5SVd8ysjbYqIQx1ptPJHnM7idVdUKSw/ZnQ1W1ebmKuoH93D/J/0zymO4+PMldkvzZauwbAJKkqo5I8qwkv9jdf9Pd3+juHUl+PMnWJD9ZVZuq6jeq6mPTh4cXVtVx0/rfVVVvqaovTB8q/sbUfr0PFKvqAVV1+dzzHVX19Kr6l6r6v1X1iqr61mnezavqvKr67DTvvKo6dpr3nCT3TfKiaQTLi6b2rqo7TtNbquqPp/U/WVW/VVU3muY9vqreWVXPm7b9iap6+AofZlgyYYz15pVJHjf3/NQkf7z7SVWdPPWUfbGqLquqM+bmbZ1+if9MVf1rkrfOtW3eyy/+F07b+uJ0YrrvEmv+niT/2N3vTZLu/kJ3/1F3f6mqTkvy2CS/Pu3zr6Z9nj53MvyXqvrhqf0uSV6c5D7T8ldN7dfr0ZvvPauZF1TVldNruLiq7rrE1wDA+va9Sb41yevnG7v7y0kuSPLgJL+S2Qeej0hyRJKfTnL1NJLjb5P8TZJvS3LHJH+3hH0/NslDk9whyZ2S/NbUfqMkr8isd+7bk3w1yYumun4zyTuSPGUawfKUBbb7+5n18N0+yf0z+/vgCXPz75XkQ0mOTPLcJC+vqlpC3bDihDHWm3clOaKq7jJd53VKkvlrtr6S2S/jmyU5OcmTq+pRe2zj/pn1Tj10vnEvv/j/KcmJSW6R5FVJXrv7U7199O4kD62qZ1XV91XVoXP7PCvJuUmeO+3zP0+zPpZZMNyS2SeZf1JVR3f3pUmelFm4u2l378s4/4ckuV9mJ8AtmX0K+vkl1A/A+ndkks919zULzLtimv/EJL/V3R/qmfd19+eT/GCST3f387v7a939pe5+9xL2/aLuvqy7v5DkOZlGuHT357v7dd19dXd/aZp3/33Z4NzfAE+f6tmR5PlJfmpusU9290unSxH+KMnRSW69hLphxQljrEe7e8cenOTSJDt3z+jut3X3xd19XXe/P8mr8x9/sZ/R3V/p7q/uy866+0+mE8Y13f38JIcm+Y59Lba735HkR5LcI8n5ST5fVb9be7lpSHe/trs/Nb2OP0vykSQn7es+9/CNJIcnuXOS6u5Lu/uK/dwWAOvT55IcucgQ/aOn+cdl9mHgnhZr31eXzU1/MrPetVTVYVX1kmmI4ReTvD3JzfZ2fpxzZJJDpu3Nb/uYueef3j3R3VdPkzfdj/phxQhjrEevTPITSR6fuSGKSVJV96qqv5/Gj+/KrBfpyD3WvyxLUFX/taourapd07DALQtsc6+6+6+nXq9bJHnkVPuiNwqpqsdV1UVVddW0z7sudZ9z+35rZsM+/iDJlVV11nTtAAAbxz8m+XpmHw7+u6q6aZKHZzbs8LLMhhLu6bLMhgIu5Cu5/rXbt1lgmePmpr89yaem6V/N7MPNe3X3EZmN4kiS3UMJe5F9JrPw+I3MhjjOb3vnwovD2iSMse509yczu5HHI7LH2PfMhhG+Mclx3b0ls+ur9hwfvrdf7tebN10f9uuZDe27+TQscNcC29zX2q/r7r9L8tbMAtZC+7xtkpcmeUqSW077vCR7Pznt9WTY3b/X3fdM8p2ZDVf8tf2pH4D1qbt3ZTbs/fer6mFVdUjN7kL8miSXZ/ZB58uS/PeqOn663vhuVXXLJOclObqqnlpVh1bV4VV1r2nTFyV5RFXdoqpuk+SpC+z+F6rq2Kq6RZLfzDdvYnV4ZteJXTXNe+Ye630mi4TAaejha5I8Z6rntpld8+brZlhXhDHWq59J8qDu/soe7Ycn+UJ3f62qTsqsB20p9vzFf3iSa5J8NsnmqnpGZhc177OqemRVnVKzu0bVVNf9M7v+baF93iSzwPXZaf0n5JvBbffyx9b1b0N8UZIfmYZ83DGz47N7/98z9Rgekllo+1qS65byGgBY/7r7uUl+I8nzknwxs2uaL0vyA9399SS/m1nAefM0/+VJbjxdz/XgJP85s6F/H0nywGmzr0zyviQ7pvUWulvwq6Z5H89suOPuuy/+ryQ3zqyX612Z3SBk3guTPHq6G+LvLbDdX8zsvPbxJO+c9nP2Ph0MWCOqe2+dBLB2VNWOJE/s7r/do31zZkMVbpdkW2YX8N4iyT9kdnK4WXf/5PQJ4CeSHLL7AuY926rqPpld5HtUZieYp2XWS/XozH7hvyDJz++uY7pb4x27+yf3Uvf9Mvu07+6ZXW92RZKXTSfFVNXxSV6b2a2F39bdj5ru7PjkzELTHye5Z5JXdvfLphD2hiT3SXJddx9Zs+9ee9XU9v4kb0nyn7r7+6vqB6a6b59ZEHtTkp+b7qAFACtmsXM3MCOMAQCwIoQx2DvDFAEAAAbQMwbLoKoem+QlC8z6ZHd/12rXAwDA2ieMAQAADLDQF/8tmyOPPLK3bt26krsAYA248MILP9fdR42uY71wfgTYOPZ2jlzRMLZ169Zs3759JXcBwBpQVZ8cXcN64vwIsHHs7RzpBh4AAAADCGMAAAADCGMAAAADrOg1YwDAf3Txzl3Zevr5i87fcebJq1gNAKPoGQOAZVBVT6uqD1TVJVX16qr61tE1AbC2CWMAcICq6pgkv5RkW3ffNcmmJKeMrQqAtU4YA4DlsTnJjatqc5LDknxqcD0ArHHCGAAcoO7emeR5Sf41yRVJdnX3m+eXqarTqmp7VW2/9updI8oEYI0RxgDgAFXVzZM8MsntknxbkptU1U/OL9PdZ3X3tu7etumwLSPKBGCNEcYA4MD9pySf6O7Pdvc3krw+yfcOrgmANU4YA4AD969J7l1Vh1VVJfmBJJcOrgmANc73jAHAAerud1fVnyf55yTXJHlvkrMWW/6EY7Zku+8SA9jwhDEAWAbd/cwkzxxdBwDrh2GKAAAAAwhjAAAAAwhjAAAAAwhjAAAAAwhjAAAAAwhjAAAAAwhjAAAAA/ieMQBYZRfv3JWtp58/bP87fOE0wJqgZwwAlkFV/XJVXVJVH6iqp46uB4C1TxgDgANUVXdN8rNJTkpy9yQ/WFV3HFsVAGudMAYAB+4uSd7d3Vd39zVJ/iHJjwyuCYA1ThgDgAN3SZL7VtUtq+qwJI9IctzgmgBY49zAAwAOUHdfWlW/neTNSb6S5KIk184vU1WnJTktSTYdcdSq1wjA2qNnDACWQXe/vLvv2d33S/J/k3x4j/lndfe27t626bAtY4oEYE3RMwYAy6CqbtXdV1bVt2d2vdi9R9cEwNomjAHA8nhdVd0yyTeS/EJ3XzW6IADWNmEMAJZBd993X5c94Zgt2e6LlwE2PNeMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADOBLnwFglV28c1e2nn7+6DKywxdPAwylZwwA9lFVnV1VV1bVJXNtJ1bVu6rqoqraXlUnjawRgPVDGAOAfXdOkoft0fbcJM/q7hOTPGN6DgA3SBgDgH3U3W9P8oU9m5McMU1vSfKpVS0KgHXLNWMAcGCemuRNVfW8zD7k/N6FFqqq05KcliSbjjhq9aoDYM3SMwYAB+bJSZ7W3ccleVqSly+0UHef1d3bunvbpsO2rGqBAKxNK9oztlbuFgWsLHdkY4M7NckvT9OvTfKygbUAsI7oGQOAA/OpJPefph+U5CMDawFgHXHNGADso6p6dZIHJDmyqi5P8swkP5vkhVW1OcnXMl0XBgA3RBgDgH3U3Y9ZZNY9l7KdE47Zku2G9wJseIYpAgAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADCCMAQAADOBLnwFglV28c1e2nn7+6DIWtMOXUQOsGj1jALCPqursqrqyqi6Za/vvVfX+qrqoqt5cVd82skYA1g9hDAD23TlJHrZH2+909926+8Qk5yV5xqpXBcC6JIwBwD7q7rcn+cIebV+ce3qTJL2qRQGwbrlmDAAOUFU9J8njkuxK8sBFljktyWlJsumIo1avOADWLD1jAHCAuvs3u/u4JOcmecoiy5zV3du6e9umw7asboEArEnCGAAsn3OT/OjoIgBYH4QxADgAVXX83NNHJvngqFoAWF9cMwYA+6iqXp3kAUmOrKrLkzwzySOq6juSXJfkk0medEPbOeGYLdnu+7wANjxhDAD2UXc/ZoHml696IQAcFAxTBAAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGMD3jAHAKrt4565sPf380WXskx2+nBpgxegZA4B9VFVnV9WVVXXJXNvdq+ofq+riqvqrqjpiZI0ArB/CGADsu3OSPGyPtpclOb27T0jyhiS/ttpFAbA+CWMAsI+6++1JvrBH852SvH2afkuSH13VogBYt4QxADgwH0jyyGn6x5IcN7AWANYRYQwADsxPJ/n5qrowyeFJ/m2hharqtKraXlXbr71616oWCMDa5G6KAHAAuvuDSR6SJFV1pyQL3n6wu89KclaSHHr08b1qBQKwZukZA4ADUFW3mv69UZLfSvLisRUBsF4IYwCwj6rq1Un+Mcl3VNXlVfUzSR5TVR9O8sEkn0ryipE1ArB+GKYIAPuoux+zyKwXLmU7JxyzJdt9mTLAhqdnDAAAYABhDAAAYABhDAAAYABhDAAAYABhDAAAYIAlhbGqOruqrqyqS1aqIAAAgI1gqT1j5yT5f+3df5BlaVkf8O9TA4sOYAMukq1ZylmtdVPEUcAOQqEGRciCFqhlzG5FXZXUJEYsqJhQg1YF/I+Y+CulhY6yLklwERGUYlDcIGbLFC42sDC77K6sZAwztexIKBt0qiQ7PPmjz0Izdu/20Lfve2/P51N1q899z5l7n2fO7Xv72+f0ea/dgzoAAAAuKRcVxrr71iSf3KNaAAAALhkzn/S5qo4mOZokB77sibN+eABYeifPrOfwsROjy9hzp0xsDfCQZn4Bj+4+3t2r3b164ODKrB8eAIbZ6m+nq+qfVdWdVfXZqlodWR8Ay8XVFAFg527K3//b6TuSfE+SW+deDQBLbeanKQLAftXdt1bV4QvG7kqSqhpREgBL7GIvbX9zkvckuaaqTlfVS/amLADYX6rqaFWtVdXa+XPro8sBYAFc1JGx7r5+rwoBgP2su48nOZ4kj7ri6h5cDgALwN+MAQAADCCMAQAADCCMAcAObfW301X13VV1OsmzkpyoqneOrRKAZeFqigCwQw/xt9NvvZjHOXJoJWsmRAa45DkyBgAAMIAwBgAAMIAwBgAAMIAwBgAAMIAwBgAAMIAwBgAAMIAwBgAAMIB5xgBgzk6eWc/hYydGlzFXp8yrBvD3ODIGAAAwgDAGADtUVTdW1dmqumPT2Kur6kxV3T7dXjiyRgCWhzAGADt3U5Jrtxj/+e5+6nR7x5xrAmBJCWMAsEPdfWuST46uA4D9QRgDgN17aVV9aDqN8fFbbVBVR6tqrarWzp9bn3d9ACwgYQwAdue1Sb46yVOT3JfkZ7faqLuPd/dqd68eOLgyz/oAWFDCGADsQnff393nu/uzSX4tyTNG1wTAchDGAGAXquqKTXe/O8kd220LAJuZ9BkAdqiqbk7ynCSXV9XpJK9K8pyqemqSTnIqyb96uMc5cmglayZBBrjkCWMAsEPdff0Ww6+beyEA7AtOUwQAABhAGAMAABhAGAMAABhAGAMAABhAGAMAABhAGAMAABhAGAMAABjAPGMAMGcnz6zn8LETo8uYq1MmuQb4e4QxAJiBqjqV5NNJzid5oLtXx1YEwKITxgBgdr61uz8xuggAloO/GQMAABhAGAOA2egkf1hV76uqo6OLAWDxOU0RAGbjm7r7TFV9RZJbquru7r71wZVTQDuaJAe+7ImjagRggTgyBgAz0N1npq9nk7w1yTMuWH+8u1e7e/XAwZURJQKwYIQxANilqnp0VT32weUkz09yx9iqAFh0TlMEgN17UpK3VlWy8dn6m939B2NLAmDRCWMAsEvd/dEkX7/T7Y8cWsmaSZABLnlOUwQAABhgT4+M+c0fAADA1hwZAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGMCkzwAwZyfPrOfwsROjyxjqlKlvABwZA4Cdqqobq+psVd2xaewJVXVLVX1k+vr4kTUCsDyEMQDYuZuSXHvB2LEk7+ruq5O8a7oPAA9LGAOAHeruW5N88oLhFyd5/bT8+iTfNdeiAFhawhgA7M6Tuvu+afnjSZ601UZVdbSq1qpq7fy59flVB8DCEsYAYEa6u5P0NuuOd/dqd68eOLgy58oAWETCGADszv1VdUWSTF/PDq4HgCUhjAHA7rwtyQ3T8g1Jfm9gLQAsEWEMAHaoqm5O8p4k11TV6ap6SZLXJHleVX0kybdP9wHgYZn0GQB2qLuv32bVcy/mcY4cWsmaSY8BLnmOjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAwgjAEAAAxgnjEAmLOTZ9Zz+NiJ0WXsa6fM4wYsAUfGAAAABhDGAGCXqupLquq9VfXBqrqzqn56dE0ALD6nKQLA7v1dkm/r7r+pqkcm+ZOq+v3u/tPRhQGwuIQxANil7u4kfzPdfeR063EVAbAMnKYIADNQVQeq6vYkZ5Pc0t23XbD+aFWtVdXa+XPrY4oEYKEIYwAwA919vrufmuTKJM+oqq+9YP3x7l7t7tUDB1fGFAnAQhHGAGCGuvuvk7w7ybWjawFgsQljALBLVfXEqnrctPylSZ6X5O6xVQGw6FzAAwB274okr6+qA9n4Reebuvvt22185NBK1kxKDHDJE8YAYJe6+0NJnja6DgCWi9MUAQAABhDGAAAABhDGAAAABhDGAAAABhDGAAAABhDGAAAABhDGAAAABjDPGADM2ckz6zl87MToMrjAKRNxA3PmyBgAzEBVPa6q3lxVd1fVXVX1rNE1AbDYHBkDgNn4xSR/0N3fW1WXJTk4uiAAFpswBgC7VFUrSb4lyQ8lSXd/JslnRtYEwOJzmiIA7N5VSf4qyW9U1Qeq6ter6tGbN6iqo1W1VlVr58+tj6kSgIUijAHA7j0iydOTvLa7n5bkb5Mc27xBdx/v7tXuXj1wcGVEjQAsGGEMAHbvdJLT3X3bdP/N2QhnALAtYQwAdqm7P57kY1V1zTT03CQfHlgSAEvABTwAYDZ+PMkbpispfjTJDw+uB4AFJ4wBwAx09+1JVney7ZFDK1kzwTDAJc9pigAAAAMIYwAAAAMIYwAAAAMIYwAAAAMIYwAAAAMIYwAAAAMIYwAAAAMIYwAAAAOY9BkA5uzkmfUcPnZidBkMcsqE38DEkTEA2KGqurGqzlbVHReM/3hV3V1Vd1bVz4yqD4DlIowBwM7dlOTazQNV9a1JXpzk67v7HyX5zwPqAmAJCWMAsEPdfWuST14w/KNJXtPdfzdtc3buhQGwlIQxANidr0nyzVV1W1X9z6r6x1ttVFVHq2qtqtbOn1ufc4kALCJhDAB25xFJnpDkmUn+fZI3VVVduFF3H+/u1e5ePXBwZd41ArCAhDEA2J3TSd7SG96b5LNJLh9cEwBLQBgDgN353STfmiRV9TVJLkvyiaEVAbAUzDMGADtUVTcneU6Sy6vqdJJXJbkxyY3T5e4/k+SG7u5xVQKwLIQxANih7r5+m1XffzGPc+TQStZM/AtwyXOaIgAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwADmGQOAOTt5Zj2Hj50YXQYL5pS55+CS48gYAADAAMIYAOxSVT25qt5dVR+uqjur6mWjawJg8TlNEQB274EkP9Hd76+qxyZ5X1Xd0t0fHl0YAIvLkTEA2KXuvq+73z8tfzrJXUkOja0KgEUnjAHADFXV4SRPS3LbBeNHq2qtqtbOn1sfURoAC0YYA4AZqarHJPmdJC/v7k9tXtfdx7t7tbtXDxxcGVMgAAtFGAOAGaiqR2YjiL2hu98yuh4AFp8wBgC7VFWV5HVJ7urunxtdDwDLwdUUAWD3np3kB5KcrKrbp7Gf7O53bLXxkUMrWTPBL8AlTxgDgF3q7j9JUqPrAGC5OE0RAABgAGEMAABgAGEMAABgAGEMAABgAGEMAABggD29muLJM+s5fOzEXj4FALtwyuXVAWAYR8YAAAAGMM8YAMyZM0d4OI5aw6XBkTEA2KGqurGqzlbVHVus+4mq6qq6fERtACwfYQwAdu6mJNdeOFhVT07y/CT/Z94FAbC8hDEA2KHuvjXJJ7dY9fNJXpGk51sRAMtMGAOAXaiqFyc5090ffJjtjlbVWlWtnT+3PqfqAFhkLuABAF+kqjqY5CezcYriQ+ru40mOJ8mjrrjaETQAHBkDgF346iRXJflgVZ1KcmWS91fVPxhaFQBLwZExAPgidffJJF/x4P0pkK129yeGFQXA0nBkDAB2qKpuTvKeJNdU1emqesnomgBYXo6MAcAOdff1D7P+8E4e58ihlayZ1BfgkufIGAAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwADCGAAAwAAmfQaAOTt5Zj2Hj50YXQaXkFMmGYeF5MgYAOxQVd1YVWer6o5NY/+pqu6uqg9V1Vur6nEjawRgeQhjALBzNyW59oKxW5J8bXd/XZI/T/LKeRcFwHISxgBgh7r71iSfvGDsD7v7genunya5cu6FAbCUhDEAmJ0fSfL7W62oqqNVtVZVa+fPrc+5LAAWkTAGADNQVT+V5IEkb9hqfXcf7+7V7l49cHBlvsUBsJBcTREAdqmqfijJdyZ5bnf34HIAWBLCGADsQlVdm+QVSf5Jd58bXQ8Ay0MYA4AdqqqbkzwnyeVVdTrJq7Jx9cRHJbmlqpLkT7v7Xz/U4xw5tJI18z4BXPKEMQDYoe6+fovh1829EAD2BRfwAAAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGMA8YwAwZyfPrOfwsROjy4BtnTIpOcyFI2MAAAADCGMAsANV9eSqendVfbiq7qyql03jT6iqW6rqI9PXx4+uFYDlIIwBwM48kOQnuvspSZ6Z5Meq6ilJjiV5V3dfneRd030AeFjCGADsQHff193vn5Y/neSuJIeSvDjJ66fNXp/ku8ZUCMCyEcYA4CJV1eEkT0tyW5Indfd906qPJ3nSNv/maFWtVdXa+XPrc6kTgMV2UWGsqq6tqnuq6t6qchoGAJecqnpMkt9J8vLu/tTmdd3dSXqrf9fdx7t7tbtXDxxcmUOlACy6HYexqjqQ5JeTvCDJU5JcP50rDwCXhKp6ZDaC2Bu6+y3T8P1VdcW0/ookZ0fVB8ByuZgjY89Icm93f7S7P5Pkjdk4Tx4A9r2qqiSvS3JXd//cplVvS3LDtHxDkt+bd20ALKeLmfT5UJKPbbp/Osk3XrhRVR1NcjRJDnzZE3dVHAAskGcn+YEkJ6vq9mnsJ5O8JsmbquolSf4yyfc93AMdObSSNZPqAlzyLiaM7Uh3H09yPEkedcXVW543DwDLprv/JElts/q586wFgP3hYk5TPJPkyZvuXzmNAQAAcJEuJoz9WZKrq+qqqrosyXXZOE8eAACAi7Tj0xS7+4GqemmSdyY5kOTG7r5zzyoDAADYxy7qb8a6+x1J3rFHtQAAAFwyLmrSZwAAAGZDGAMAABhAGAMAABhg5vOMAQAP7eSZ9Rw+dmJ0GbCwTpkUnUuEI2MAsENVdWNVna2qOzaN/VZV3T7dTlXV7SNrBGB5EYdCkQAADKBJREFUODIGADt3U5JfSvJfHxzo7n/+4HJV/WyS9fmXBcAyEsYAYIe6+9aqOrzVuqqqJN+X5NvmWRMAy8tpigAwG9+c5P7u/shWK6vqaFWtVdXa+XMOngEgjAHArFyf5ObtVnb38e5e7e7VAwdX5lgWAIvKaYoAsEtV9Ygk35PkG0bXAsDycGQMAHbv25Pc3d2nRxcCwPIQxgBgh6rq5iTvSXJNVZ2uqpdMq67LQ5yiCABbcZoiAOxQd1+/zfgPXczjHDm0kjWT2gJc8hwZAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGEAYAwAAGMCkzwAwZyfPrOfwsROjywAuwikTtbMHHBkDgF2qqhur6mxV3TG6FgCWhzAGALt3U5JrRxcBwHIRxgBgl7r71iSfHF0HAMtFGAOAOaiqo1W1VlVr58+tjy4HgAUgjAHAHHT38e5e7e7VAwdXRpcDwAIQxgAAAAYQxgAAAAYwzxgA7FJV3ZzkOUkur6rTSV7V3a/bbvsjh1ayZs4igEueMAYAu9Td14+uAYDl4zRFAACAAYQxAACAAYQxAACAAYQxAACAAYQxAACAAYQxAACAAYQxAACAAcwzBgBzdvLMeg4fOzG6DIAtnTIp/dw4MgYAu1RV11TV7Ztun6qql4+uC4DFtqdHxo4cWsmaZA3APtfd9yR5apJU1YEkZ5K8dWhRACw8R8YAYLaem+QvuvsvRxcCwGITxgBgtq5LcvPoIgBYfMIYAMxIVV2W5EVJfnuLdUeraq2q1s6fW59/cQAsHGEMAGbnBUne3933X7iiu49392p3rx44uDKgNAAWjTAGALNzfZyiCMAOCWMAMANV9egkz0vyltG1ALAcTPoMADPQ3X+b5Mt3sq2pXwBIHBkDAAAYQhgDAAAYQBgDAAAYQBgDAAAYQBgDAAAYQBgDAAAYQBgDAAAYQBgDAAAYwKTPADBnJ8+s5/CxE6PLALgknHrNd4wuYVuOjAHADFTVtVV1T1XdW1XHRtcDwOITxgBgl6rqQJJfTvKCJE9Jcn1VPWVsVQAsOmEMAHbvGUnu7e6PdvdnkrwxyYsH1wTAghPGAGD3DiX52Kb7p6exz6mqo1W1VlVr58+tz7U4ABaTMAYAc9Ddx7t7tbtXDxxcGV0OAAtAGAOA3TuT5Mmb7l85jQHAtoQxANi9P0tydVVdVVWXJbkuydsG1wTAgjPPGADsUnc/UFUvTfLOJAeS3Njddw4uC4AFJ4wBwAx09zuSvGMn2x45tJK1BZ6EFID5cJoiAADAAMIYAADAAMIYAADAAMIYAADAAMIYAADAAMIYAADAAMIYAADAAMIYAADAACZ9BoA5O3lmPYePnRhdBgAP4dRrvmPPn8ORMQCYkao6UFUfqKq3j64FgMUnjAHA7LwsyV2jiwBgOQhjADADVXVlku9I8uujawFgOQhjADAbv5DkFUk+u9XKqjpaVWtVtXb+3Pp8KwNgIQljALBLVfWdSc529/u226a7j3f3anevHji4MsfqAFhUwhgA7N6zk7yoqk4leWOSb6uq/z62JAAWnTAGALvU3a/s7iu7+3CS65L8UXd//+CyAFhw5hkDgDk7cmgla3OYvwaAxSaMAcAMdfcfJ/njwWUAsAScpggAADCAMAYAADCAMAYAADBAdffePXjVp5Pcs2dPMN7lST4xuog9sp97S/S37PS3eL6yu584uohlsY8+H5fxtbqd/dKLPhaLPhbLqD62/Yzc6wt43NPdq3v8HMNU1dp+7W8/95bob9npj31gX3w+7qfX6n7pRR+LRR+LZRH7cJoiAADAAMIYAADAAHsdxo7v8eOPtp/728+9Jfpbdvpj2e2Xfbxf+kj2Ty/6WCz6WCwL18eeXsADAACArTlNEQAAYABhDAAAYIA9CWNVdW1V3VNV91bVsb14jlmqqlNVdbKqbq+qtWnsCVV1S1V9ZPr6+Gm8quq/TL19qKqevulxbpi2/0hV3bBp/Bumx793+re1x/3cWFVnq+qOTWN73s92zzGH3l5dVWem/Xd7Vb1w07pXTnXeU1X/dNP4lq/Rqrqqqm6bxn+rqi6bxh813b93Wn941r1Nz/Pkqnp3VX24qu6sqpdN4/tl/23X377Yh1X1JVX13qr64NTfT3+xNc2qbxbPdvtwpG3eW5fuc3Dke+iM+xj2XrIXqupAVX2gqt6+rH3UPvlZsaoeV1Vvrqq7q+quqnrWsvVRVdfU539euL2qPlVVL1+2Pj6nu2d6S3IgyV8k+aoklyX5YJKnzPp5ZlzzqSSXXzD2M0mOTcvHkvzHafmFSX4/SSV5ZpLbpvEnJPno9PXx0/Ljp3Xvnbat6d++YI/7+ZYkT09yxzz72e455tDbq5P8uy22fcr0+ntUkqum1+WBh3qNJnlTkuum5V9J8qPT8r9J8ivT8nVJfmuP9t0VSZ4+LT82yZ9PfeyX/bddf/tiH07/p4+Zlh+Z5Lbp//qiappl326LdXuofTi4riGfG3vQx7D30Bn3Mey9ZI/2y79N8ptJ3j7dX7o+sk9+Vkzy+iT/clq+LMnjlrGPTf0cSPLxJF+5rH3sxX/Ks5K8c9P9VyZ55V7uiBnUvNU32D1JrpiWr8jGBJ1J8qtJrr9wuyTXJ/nVTeO/Oo1dkeTuTeNfsN0e9nQ4X/ihuuf9bPccc+jt1dn6B/kveO0leef0+tzyNTp9w30iySMufC0/+G+n5UdM29Uc9uPvJXneftp/2/S37/ZhkoNJ3p/kGy+2pln27bZYt+324ei6ploOZ86fG3PoaW7voXvYw9zeS/ao/iuTvCvJtyV5+0O9Xy14H6ey5D8rJllJ8r9zwWffsvVxQe3PT/K/lrmPvThN8VCSj226f3oaW2Sd5A+r6n1VdXQae1J33zctfzzJk6bl7fp7qPHTW4zP2zz62e455uGl06HnG+vzp9ddbG9fnuSvu/uBC8a/4LGm9evT9ntmOj3jadn4jei+238X9Jfsk304nY5ze5KzSW7Jxm9tL7amWfbNYlmmz8il/hwc8B46U4PeS/bCLyR5RZLPTve/mPfpRehjP/yseFWSv0ryG9Npo79eVY/O8vWx2XVJbp6Wl7IPF/DY8E3d/fQkL0jyY1X1LZtX9kYs7iGV7YF59DPn/7PXJvnqJE9Ncl+Sn53T8+6ZqnpMkt9J8vLu/tTmdfth/23R377Zh919vrufmo3fBj8jyT8cXBLs2rJ9Do5+D52F/fBeUlXfmeRsd79vdC0zsB9+VnxENk5Hfm13Py3J32bjdL7PWZI+kiTT3xq+KMlvX7humfrYizB2JsmTN92/chpbWN19Zvp6Nslbs/Gmd39VXZEk09ez0+bb9fdQ41duMT5v8+hnu+fYU919//Sh9dkkv5aN/ZdcfG//N8njquoRF4x/wWNN61em7Weuqh6ZjR8i3tDdb5mG983+26q//bYPk6S7/zrJu7NxGs3F1jTLvlksy/QZuZSfgwPfQ/fEnN9LZu3ZSV5UVaeSvDEbpyr+4hL2sV9+Vjyd5HR3P3hGypuzEc6WrY8HvSDJ+7v7/un+UvaxF2Hsz5JcXRtXyrksG4cP37YHzzMTVfXoqnrsg8vZOPf0jmzUfMO02Q3ZOO880/gPTldmeWaS9emQ6DuTPL+qHj+dYvX8bJzHfF+ST1XVM6crsfzgpseap3n0s91z7KkHv/Em352N/fdgPdfVxtWZrkpydTb+IHPL1+j0W5R3J/neLXrY3Nv3JvmjaftZ91JJXpfkru7+uU2r9sX+266//bIPq+qJVfW4aflLs/G3Knd9ETXNsm8WyzJ9Ri7d5+Dg99BZ9jHkvWTWfXT3K7v7yu4+PD3HH3X3v1i2PvbLz4rd/fEkH6uqa6ah5yb58LL1scn1+fwpig/Wu3x9zPIP0B68ZeOqJX+ejfObf2ovnmOGtX5VNq6+88Ekdz5YbzbOUX5Xko8k+R9JnjCNV5Jfnno7mWR102P9SJJ7p9sPbxpfzcY37V8k+aXs8YUfsvHCvC/J/8vGb0FeMo9+tnuOOfT236baP5SNb7grNm3/U1Od92TTlXC2e41Or4f3Tj3/dpJHTeNfMt2/d1r/VXu0774pG4fVP5Tk9un2wn20/7brb1/swyRfl+QDUx93JPkPX2xNs+rbbfFu2+3DwTUN+dzYgz6GvYfOuI9h7yV7+Bp7Tj5/NcWl6iP76GfFbPw5wNr02vrdbFxFcBn7eHQ2jpqubBpbuj66+3M/hAEAADBHLuABAAAwgDAGAAAwgDAGAAAwgDAGAAAwgDAGAAAwgDAGAAAwgDAGAAAwwP8HXMKAP6j+vmMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df['Stay_In_Current_City_Years'].value_counts().plot(kind='barh', title='Stay_In_Current_City_Years')\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=[15, 15])\n",
        "\n",
        "df['Gender'].value_counts().plot(kind='barh', ax=axes[0,0], title='Gender')\n",
        "df['Age'].value_counts().plot(kind='barh', ax=axes[0,1], title='Age')\n",
        "df['Marital_Status'].value_counts().plot(kind='barh', ax=axes[1,0], title='Marital_Status')\n",
        "df['Occupation'].value_counts().plot(kind='barh', ax=axes[1,1], title='Occupation');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C-LX-matGjym",
        "outputId": "3f47de8d-dde9-4919-98fb-8e870d0a1ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\n",
        "\n",
        "df['User_ID'].hist(ax=axes[0])\n",
        "df['Product_ID'].apply(lambda x:int(x[1:])).hist(ax=axes[1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Rkm3wKS6Gjym"
      },
      "outputs": [],
      "source": [
        "# Aplicando transformações\n",
        "df.User_ID = df.User_ID.astype(str)\n",
        "df.Product_Category_2.fillna(0, inplace=True)\n",
        "df.Product_Category_3.fillna(0, inplace=True)\n",
        "df.Product_Category_1 = df.Product_Category_1.astype(int)\n",
        "df.Product_Category_2 = df.Product_Category_2.astype(int)\n",
        "df.Product_Category_3 = df.Product_Category_3.astype(int)\n",
        "\n",
        "# # Embaralhando o dataframe para não ter dados enviesados por ordem\n",
        "# df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# # separando train/test\n",
        "# size = len(df)\n",
        "# data_idx = int(size * 0.8)\n",
        "# # write to disk\n",
        "# df.iloc[data_idx:].to_csv('', index=False)\n",
        "# df.iloc[:data_idx].to_csv(EVAL, index=False)\n",
        "# print(len(df.iloc[data_idx:]), 'training records')\n",
        "# print(len(df.iloc[:data_idx]), 'eval records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9q8LSQwGjyn"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaWX9CBfGjyn"
      },
      "source": [
        "## Imports de predição"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Fz5xyZbHGjyn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cols_with_no_nans(df,col_type):\n",
        "    '''\n",
        "    Arguments :\n",
        "    df : The dataframe to process\n",
        "    col_type : \n",
        "          num : to only get numerical columns with no nans\n",
        "          no_num : to only get nun-numerical columns with no nans\n",
        "          all : to get any columns with no nans    \n",
        "    '''\n",
        "    if (col_type == 'num'):\n",
        "        predictors = df.select_dtypes(exclude=['object'])\n",
        "    elif (col_type == 'no_num'):\n",
        "        predictors = df.select_dtypes(include=['object'])\n",
        "    elif (col_type == 'all'):\n",
        "        predictors = df\n",
        "    else :\n",
        "        print('Error : choose a type (num, no_num, all)')\n",
        "        return 0\n",
        "    cols_with_no_nans = []\n",
        "    for col in predictors.columns:\n",
        "        if not df[col].isnull().any():\n",
        "            cols_with_no_nans.append(col)\n",
        "    return cols_with_no_nans"
      ],
      "metadata": {
        "id": "hLoqZ2ZsfmzO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = df.Purchase\n",
        "\n",
        "num_cols = get_cols_with_no_nans(df , 'num')\n",
        "cat_cols = get_cols_with_no_nans(df , 'no_num')"
      ],
      "metadata": {
        "id": "5ZpoC3wZfpIz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Number of numerical columns with no nan values :',len(num_cols))\n",
        "print ('Number of nun-numerical columns with no nan values :',len(cat_cols))"
      ],
      "metadata": {
        "id": "xThh1LKufxbV",
        "outputId": "696c0d52-cf33-4480-e368-b8a11041059e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of numerical columns with no nan values : 6\n",
            "Number of nun-numerical columns with no nan values : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined = df[num_cols + cat_cols]\n",
        "combined.hist(figsize = (12,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9PVMhkuOf8Rf",
        "outputId": "087f78d8-9ab5-49a2-ac44-b3eec6fd89b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x720 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = df[num_cols + cat_cols]\n",
        "train_data['Target'] = target\n",
        "\n",
        "C_mat = train_data.corr()\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "\n",
        "sb.heatmap(C_mat, vmax = .8, square = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C-yuweHdm6I-",
        "outputId": "2bacc501-809b-43e2-94a8-d6f553cc6954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4gAAANjCAYAAADoMzSPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdf7RldXkn6M9LCQHF4GpjmERNsLVQaUSiqGuMGoya0cTW2Nrjr5jIJI0xo3HRmgmdmSbduGYmiZ1Jxo6QoAFM0KBo7CaGCdhGDIOO8rtAECWiBjVoTELEgGjdd/4434qX661bt84tOPfuep617qpz9tln7/ecqlXwqffd313dHQAAADhg0QUAAACwOQiIAAAAJBEQAQAAGAREAAAAkgiIAAAADAIiAAAASQREAACATamqnl1VN1bVTVV18iqv/0BVfaiqrqqqHVX14xs+p/sgAgAAbC5VtS3Jp5I8K8ktSS5L8tLuvn7ZPmckuaq7T6+qo5Jc0N1HbOS8OogAAACbzxOT3NTdn+nuu5Kcm+T5K/bpJN89Hh+W5IsbPel9NnoAAAAA9rkHJ/mrZc9vSfKkFfv8hyQXVdVrk9wvyTM3elIBEQAAmJRv/s1nNv11dAc96OGvSnLisk1ndPcZe3mYlyY5u7t/s6r++yR/WFVHd/fSvHUJiAAAAPeyEQbXCoRfSPLQZc8fMrYt97NJnj2O99GqOjjJ9yT58rx1uQYRAABg87ksyfaqelhVHZTkJUnOX7HP55M8I0mq6tFJDk7ylY2cVAcRAACYlqWdi65gw7r7W1X1miQXJtmW5Mzu/kRVnZrk8u4+P8nrk7y1qk7KbMGaV/YGb1PhNhcAAMCkfPPLn970IefA791ei65hNUZMAQAASCIgAgAAMLgGEQAAmJb57/Kw39NBBAAAIImACAAAwGDEFAAAmJYlI6bz0kEEAAAgiYAIAADAYMQUAACYlLaK6dx0EAEAAEgiIAIAADAYMQUAAKbFKqZz00EEAAAgiYAIAADAYMQUAACYFquYzk0HEQAAgCQCIgAAAIMRUwAAYFqWdi66gi1LBxEAAIAkAiIAAACDgAgAAEAS1yACAABT4zYXc9NBBAAAIImACAAAwGDEFAAAmJYlI6bz0kEEAAAgiYAIAADAYMQUAACYlLaK6dx0EAEAAEgiIAIAADAYMQUAAKbFKqZz00EEAAAgiYAIAADAYMQUAACYFquYzk0HEQAAgCQCIgAAAIOACAAAQBLXIAIAAFOztHPRFWxZOogAAAAkERABAAAYjJgCAADT4jYXc9NBBAAAIImACAAAwGDEFAAAmJYlI6bz0kEEAAAgiYAIAADAYMQUAACYFquYzk0HEQAAgCQCIgAAAIMRUwAAYFqsYjo3HUQAAACSCIgAAAAMAiIAAABJXIMIAABMTPfORZewZekgAgAAkERABAAAYDBiCgAATEu7zcW8dBABAABIIiACAAAwGDEFAACmZcmI6bx0EAEAAEgiIAIAADAYMQUAAKbFKqZz00EEAAAgiYAIAADAYMQUAACYlqWdi65gy9JBBAAAIImACAAAwGDEdCK++Tef6UXXsL/6rcefsugS9mu3lz/6i3Kgr36h/AvvYm1LLboEWIiTP3fO1vjDbxXTufnvCwAAAEkERAAAAAYBEQAAgCSuQQQAAKZmyTWI89JBBAAAIImACAAAwGDEFAAAmBa3uZibDiIAAABJBEQAAAAGI6YAAMC0WMV0bjqIAAAAJBEQAQAAGIyYAgAA02LEdG46iAAAACQREAEAABiMmAIAAJPSvXPRJWxZOogAAAAkERABAAAYBEQAAACSuAYRAACYGre5mJsOIgAAAEkERAAAAAYjpgAAwLS0EdN56SACAACQREAEAABgMGIKAABMi1VM56aDCAAAQBIBEQAAgMGIKQAAMC1WMZ2bDiIAAABJBEQAAAAGI6YAAMC0WMV0bjqIAAAAJBEQAQAAGIyYAgAA02IV07npIAIAAJBEQAQAAGAQEAEAAEjiGkQAAGBq3OZibjqIAAAAJBEQAQAAGIyYAgAA02LEdG6T6yBW1UOq6r9W1aer6i+r6v+uqoMWWM9PVtVRy56fWlXPXFQ9AAAAuzOpgFhVleSPk/yX7t6e5Mgkhyb53xdY1k8m+aeA2N2ndPd/W2A9AAAAq5pUQEzyo0nu7O6zkqS7dyY5Kcn/VFX3q6r/VFXXVdWOqnptklTVE6rqI1V1TVV9vKruX1WvrKrf2XXQqnp/VR0/Ht9eVb9VVZ+oqg9W1YPG9n9TVZeN47y3qu5bVU9O8rwkb6qqq6vq4VV1dlW9aLznGVV1VVVdW1VnVtV3je2frar/WFVXjtcede99hQAAsMX10ub/2aSmFhD/RZIrlm/o7n9I8vkkP5fkiCTHdvcxSd4xRk/fleR13f3YJM9McsceznG/JJd3979I8uEkvzq2/3F3P2Ec54YkP9vdH0lyfpJf6u5ju/svdx2kqg5OcnaSF3f3YzK7HvTVy87zN939uCSnJ3nD3n0NAAAAe29qAXEtxyf5ve7+VpJ0998meWSSL3X3ZWPbP+x6fQ1LmYXKJDknyVPG46Or6pKqujbJyzMLq2t5ZJKbu/tT4/nbkzxt2et/PH69IrNg+x2q6sSquryqLn/bH/zRHk4HAACwtqmtYnp9khct31BV353kB5J8di+O863cPTwfvMa+PX49O8lPdvc1VfXKzALpRnxj/Lozu/l96u4zkpyRJN/8m8/0avsAAMB+xyqmc5taB/GDSe5bVT+dJFW1LclvZhbeLkzyqqq6z3jtnyW5Mcn3VdUTxrb7j9c/m+TYqjqgqh6a5InLznFAvh1CX5bk/x2P75/kS1V1YGYdxF2+Nl5b6cYkR1TVI8bzV2Q2sgoAALAQkwqI3d1JXpDkX1fVp5N8KsmdSX4lydsyuxZxR1Vdk+Rl3X1Xkhcn+c9j2wcy6xZemuTmzDqSb05y5bLTfD3JE6vquswWxTl1bP/3ST423vvJZfufm+SXxmI0D19W651JTkhy3hhLXUryu/vquwAAANhbNctUrFdV3d7dhy66jpWMmC7Obz3+lEWXsF+7vfzRX5QDffULNal/4d2CtqUWXQIsxMmfO2dL/OG/47/+xqb/r9Qhz/9fNuV36b8vAAAAJBEQ99pm7B4CAADsCwIiAAAASaZ3mwsAAGB/5zYXc9NBBAAAIImACAAAwGDEFAAAmJY2YjovHUQAAACSCIgAAAAMAiIAADAtS0ub/2cdqurZVXVjVd1UVSfvZp//saqur6pPVNU7N/rVuQYRAABgk6mqbUnekuRZSW5JcllVnd/d1y/bZ3uSf5fkh7v776rqezd6Xh1EAACAzeeJSW7q7s90911Jzk3y/BX7/Jskb+nuv0uS7v7yRk+qgwgAAEzLOkc4F6mqTkxy4rJNZ3T3GcuePzjJXy17fkuSJ604zJHjWJcm2ZbkP3T3n22kLgERAADgXjbC4Bl73HFt90myPcnxSR6S5C+q6jHd/ffzHtCIKQAAwObzhSQPXfb8IWPbcrckOb+7v9ndNyf5VGaBcW4CIgAAMC3dm/9nzy5Lsr2qHlZVByV5SZLzV+zzXzLrHqaqviezkdPPbOSrExABAAA2me7+VpLXJLkwyQ1J3t3dn6iqU6vqeWO3C5N8taquT/KhJL/U3V/dyHldgwgAALAJdfcFSS5Yse2UZY87yb8dP/uEDiIAAABJdBABAICp2QK3udisdBABAABIIiACAAAwGDEFAACmxYjp3HQQAQAASCIgAgAAMBgxBQAApqWNmM5LBxEAAIAkAiIAAACDEVMAAGBarGI6Nx1EAAAAkgiIAAAADEZMAQCAaeledAVblg4iAAAASQREAAAABiOmAADAtFjFdG46iAAAACQREAEAABgERAAAAJK4BhEAAJga1yDOTQcRAACAJAIiAAAAgxFTAABgWtqI6bx0EAEAAEiigzgZv/X4UxZdwn7rpCtOXXQJ+7Xf9Gd/Ye5KL7qE/do3F13Afm6nP//ARAmIAADApPSSf8SZlxFTAAAAkgiIAAAADEZMAQCAaVmyium8dBABAABIIiACAAAwGDEFAACmpY2YzksHEQAAgCQCIgAAAIOACAAAQBLXIAIAAFOz1IuuYMvSQQQAACCJgAgAAMBgxBQAAJiWJbe5mJcOIgAAAEkERAAAAAYjpgAAwLQYMZ2bDiIAAABJBEQAAAAGI6YAAMC0dC+6gi1LBxEAAIAkAiIAAACDEVMAAGBarGI6Nx1EAAAAkgiIAAAADEZMAQCAaVmyium8dBABAABIIiACAAAwCIgAAAAkcQ0iAAAwNe02F/PSQQQAACCJgAgAAMBgxBQAAJgWt7mYmw4iAAAASQREAAAABiOmAADApPSSVUznpYMIAABAEgERAACAwYgpAAAwLVYxnZsOIgAAAEkERAAAAAYjpgAAwLS0VUznpYMIAABAkk0YEKuqq+qcZc/vU1Vfqar37+Vxvr+q3jMeH1tVP76O9xy/1nmq6vCqen9VXVNV11fVBWP7EVX1snUcf137AQAALMKmC4hJvp7k6Ko6ZDx/VpIv7M0Bquo+3f3F7n7R2HRskj0GxHU4NckHuvux3X1UkpPH9iOSrCf4rXc/AACAe91mDIhJckGSnxiPX5rkj3a9UFVPrKqPVtVVVfWRqnrk2P7Kqjq/qv48yQdHt+66qjoos2D34qq6uqpevLtjrMP3Jbll15Pu3jEe/lqSp47jnzTOfUlVXTl+nryb/V5ZVb+z7LO9f3Qxt1XV2aP+a6vqpL3/CgEAYD+11Jv/Z5ParIvUnJvklDHueUySM5M8dbz2ySRP7e5vVdUzk/wfSV44XntckmO6+2+r6ogk6e67quqUJMd192uSpKq+e41jrOUtSd5VVa9J8t+SnNXdX8ysk/iG7n7uOP59kzyru++squ2ZBdzjVtnvlbs5z7FJHtzdR4/9HrCO2gAAADZkUwbE7t4xAt5LM+smLndYkreP4NVJDlz22ge6+2/XcYq1jrFWXRdW1T9P8uwkz0lyVVUdvcquByb5nao6NsnOJEeu5/jLfCbJP6+q/5zkT5NctNpOVXVikhOT5F/9syfmSYdu38vTAAAAfNtmHTFNkvOT/KcsGy8d3pjkQ6O79i+THLzsta+v89hrHWNN3f233f3O7n5FksuSPG2V3U5KcmuSx2bWOTxoN4f7Vu7+e3DwOMffjfdenOTnk7xtN7Wc0d3HdfdxwiEAAAxLS5v/Z5PazAHxzCT/sbuvXbH9sHx70ZpXrvNYX0ty/w0eI1X1o2N8NFV1/yQPT/L53Rz/S929lOQVSbbtpo7PJjm2qg6oqocmeeI49vckOaC735vkf8tsdBYAAOAetWkDYnff0t1vXuWl30jyf1bVVVn/iOyHkhy1a5GaOY+RJI9PcnlV7Ujy0SRv6+7LkuxIsnPc/uKkJKcl+ZmquibJo/LtzubK/S5NcnOS65O8OcmVY78HJ7m4qq5Ock6Sf7cXNQIAAMylujfvCjqs32/84E/5jVyQk644ddEl7Nd+8/GnLLqE/dZd8dfOIm3e4aT9w6b9F3a4h53yuXfUomtYj6+f8pJN/x+p+5167qb8Lv39BgAAQJJNuorpolXVCUlet2Lzpd39Py+iHgAAgHuDgLiK7j4ryVmLrgMAAJhDG8SflxFTAAAAkgiIAAAADEZMAQCAaVna9IuYblo6iAAAACQREAEAABgERAAAAJK4BhEAAJiYXnKbi3npIAIAAJBEQAQAAGAwYgoAAEyL21zMTQcRAACAJAIiAAAAgxFTAABgWoyYzk0HEQAAgCQCIgAAAIMRUwAAYFp6adEVbFk6iAAAACQREAEAABiMmAIAANNiFdO56SACAACQREAEAABgMGIKAABMShsxnZsOIgAAAEkERAAAAAYBEQAAgCSuQQQAAKbGNYhz00EEAAAgiYAIAADAYMQUAACYlqWlRVewZekgAgAAkERABAAAYDBiCgAATItVTOemgwgAAEASAREAAIDBiCkAADAtRkznpoMIAABAEgERAACAwYgpAAAwKd1GTOelgwgAAEASHcTJuL38K8mi/ObjT1l0Cfu1119x6qJL2G/94nEnL7qE/dpbv3jpokvYr93voIMXXQIshP/rmT4dRAAAAJLoIAIAAFPjNhdz00EEAAAgiYAIAADAYMQUAACYFiOmc9NBBAAAIImACAAAwCAgAgAAk9JLvel/1qOqnl1VN1bVTVW12xsQV9ULq6qr6riNfncCIgAAwCZTVduSvCXJc5IcleSlVXXUKvvdP8nrknxsX5xXQAQAANh8npjkpu7+THffleTcJM9fZb83Jvn1JHfui5MKiAAAwLQs9eb/2bMHJ/mrZc9vGdv+SVU9LslDu/tP99VXJyACAADcy6rqxKq6fNnPiXv5/gOS/F9JXr8v63IfRAAAgHtZd5+R5Iw1dvlCkocue/6QsW2X+yc5OsnFVZUk/12S86vqed19+bx1CYgAAMC0LC26gH3isiTbq+phmQXDlyR52a4Xu/u2JN+z63lVXZzkDRsJh4kRUwAAgE2nu7+V5DVJLkxyQ5J3d/cnqurUqnrePXVeHUQAAIBNqLsvSHLBim2n7Gbf4/fFOQVEAABgUtZ7I3q+kxFTAAAAkgiIAAAADAIiAAAASVyDCAAATI1rEOemgwgAAEASAREAAIDBiCkAADAtS4suYOvSQQQAACCJgAgAAMBgxBQAAJiUtorp3HQQAQAASCIgAgAAMBgxBQAApsUqpnPTQQQAACCJgAgAAMBgxBQAAJgUq5jOTwcRAACAJAIiAAAAg4AIAABAEtcgAgAAU+M2F3PTQQQAACCJgAgAAMBgxBQAAJiUNmI6Nx1EAAAAkgiIAAAADEZMAQCAaTFiOjcdRAAAAJKsIyBW1c6qurqqrquq86rqvvOerKourqrj5njfA6rqF9ax35FVdUFVfbqqrqyqd1fV4Wvsf0RVvWxv67mnVdVrquqmquqq+p5F1wMAAOwf1tNBvKO7j+3uo5PcleTnl79YVffGmOoDkqwZEKvq4CR/muT07t7e3Y9LclqSB63xtiOS3OMBsaq27eVbLk3yzCSfuwfKAQCASeulzf+zWe3tiOklSR5RVcdX1SVVdX6S66vq4Ko6q6quraqrqurpSVJVh1TVuVV1Q1W9L8khuw5UVbcve/yiqjp7PD68qt5XVdeMnycn+bUkDx+dzDftpraXJflod//Jrg3dfXF3Xzc6hZeMruKV45gZx33qOO5JVbWtqt5UVZdV1Y6qetWo6YCqOq2qPllVHxhdyheN154xPvO1VXVmVX3X2P7Zqvr1qroyycnj112fd/vy5yt191Xd/dn1/ZYAAADsG+vu/o1O4XOS/NnY9LgkR3f3zVX1+iTd3Y+pqkcluaiqjkzy6iT/2N2Prqpjkuw2FC3z5iQf7u4XjM7boUlOHuc6do33HZ3kit289uUkz+ruO6tqe5I/SnLcOO4buvu54zOemOS27n7CCHqXVtVFSR6fWbfxqCTfm+SGJGeOruXZSZ7R3Z+qqj8Yn/m3x3m/OjqZqapnVtWx3X11khOSnLWO7wIAAOBes54O4iFVdXWSy5N8Psnvj+0f7+6bx+OnJDknSbr7k5mNRh6Z5GnLtu9IsmMd5/vRJKeP9+zs7tvW91HWdGCSt1bVtUnOyyzorebHkvz0+LwfS/LAJNsz+3zndfdSd/91kg+N/R+Z5Obu/tR4/vbMPvMu71r2+G1JThih98VJ3rnRD1VVJ1bV5VV1+ZVfu2mjhwMAgGlY2gI/m9R6Ooh3rOzcVVWSfH2D5+5ljw/e4LGS5BNJfmQ3r52U5NYkj80sFN+5m/0qyWu7+8K7baz68TlrWv4dvTfJryb58yRXdPdX5zzmP+nuM5KckSSnHPHy3sPuAAAAa9pXt7m4JMnLk9lKokl+IMmNSf4iYxGYqjo6yTHL3nNrVT26qg5I8oJl2z+Y2ZhmxjWBhyX5WpL776GGdyZ5clX9xK4NVfW0cd7Dknypu5eSvCLJrkVjVh73wiSvrqoDd32WqrpfZovGvHBci3h4kuPH/jcmOaKqHjGevyLJh1crrrvvHMc/PcZLAQCATWhfBcTTkhwwRjjfleSV3f2NzMLQoVV1Q5JTc/drBE9O8v4kH0nypWXbX5fk6eNYVyQ5anTbLh232lh1kZruviPJc5O8dtzm4vrMVj79yqjvZ6rqmiSPyrc7ezuS7ByL4ZyU2Rjo9UmurKrrkvxeZl3W9ya5Zbx2TmbXUt42Qt8JSc4b9S4l+d01vqd3jH0uWmOfVNUvVtUtSR6SZEdVvW2t/QEAAPaF6jaZuB5VdWh3315VD0zy8SQ/PK5H3JtjvCHJYd397/d1fUZMF+e+XYsuYb/2+itOXXQJ+61fPO7kRZewX3vrFy9ddAn7tfsdtC+ujoGt57bb/3JL/I/PV571I5v+/40f9IEPb8rv8t64h+FUvL+qHpDkoCRvnCMcvi/JwzNbhAcAAGDT2XIBsaoek+QPV2z+Rnc/6Z48b3cfv8H3v2DlthEaH7Zi8y+vXCQHAADg3rDlAmJ3X5tkrfshbhmrhUYAAGBjehPfRmKz21eL1AAAALDFCYgAAAAk2YIjpgAAAGsxYjo/HUQAAACSCIgAAAAMRkwBAIBp6U15D/otQQcRAACAJAIiAAAAgxFTAABgUqxiOj8dRAAAAJIIiAAAAAxGTAEAgEnpJauYzksHEQAAgCQCIgAAAIOACAAAQBLXIAIAABPjNhfz00EEAAAgiYAIAADAYMQUAACYlG63uZiXDiIAAABJBEQAAAAGI6YAAMCkWMV0fjqIAAAAJBEQAQAAGIyYAgAAk9JLVjGdlw4iAAAASQREAAAABiOmAADApHQvuoKtSwcRAACAJAIiAAAAg4AIAABAEtcgAgAAE+M2F/PTQQQAACCJgAgAAMBgxBQAAJgUI6bz00EEAAAgiYAIAADAYMQUAACYlO5FV7B16SACAACQRAdxMg70ryQLc1d8+Yv0i8edvOgS9ltvvvzXFl3Cfu2Hjzll0SXs1/5+26IrALhnCIgAAMCkWMV0fkZMAQAASCIgAgAAMBgxBQAAJqXbiOm8dBABAABIIiACAAAwGDEFAAAmpZcWXcHWpYMIAABAEgERAACAQUAEAAAgiWsQAQCAiVlym4u56SACAACQREAEAABgMGIKAABMShsxnZsOIgAAAEkERAAAAAYjpgAAwKT0khHTeekgAgAAkERABAAAYDBiCgAATEr3oivYunQQAQAASCIgAgAAMBgxBQAAJsUqpvPTQQQAACCJgAgAAMAgIAIAAJDENYgAAMDELLVrEOelgwgAAEASAREAAIDBiCkAADApbcR0bjqIAAAAJBEQAQAAGIyYAgAAk9K96Aq2Lh1EAAAAkgiIAAAADEZMAQCASVmyiuncdBABAABIIiACAAAwGDEFAAAmpY2Yzk0HEQAAgCQCIgAAAIOACAAAQBLXIAIAABPTvegKtq49dhCramdVXV1V11XVeVV133lPVlUXV9Vxc7zvAVX1C+vY78iquqCqPl1VV1bVu6vq8DX2P6KqXra39dzTquodVXXj+M7PrKoDF10TAAAwfesZMb2ju4/t7qOT3JXk55e/WFX3RhfyAUnWDIhVdXCSP01yendv7+7HJTktyYPWeNsRSe7xgFhV2/byLe9I8qgkj0lySJKf2+dFAQAArLC31yBekuQRVXV8VV1SVecnub6qDq6qs6rq2qq6qqqeniRVdUhVnVtVN1TV+zILOxmv3b7s8Yuq6uzx+PCqel9VXTN+npzk15I8fHQy37Sb2l6W5KPd/Se7NnT3xd193egUXjK6ileOY2Yc96njuCdV1baqelNVXVZVO6rqVaOmA6rqtKr6ZFV9YHQpXzRee8b4zNeObt93je2frapfr6ork5w8ft31ebcvf75Sd1/QQ5KPJ3nIun53AACALHVt+p/Nat3dv9EpfE6SPxubHpfk6O6+uapen6S7+zFV9agkF1XVkUleneQfu/vRVXVMkt2GomXenOTD3f2C0Xk7NMnJ41zHrvG+o5NcsZvXvpzkWd19Z1VtT/JHSY4bx31Ddz93fMYTk9zW3U8YQe/SqrooyeMz6zYeleR7k9yQ5MzRtTw7yTO6+1NV9QfjM//2OO9XRyczVfXMqjq2u69OckKSs/b0RYzR0lcked2e9gUAANio9XQQD6mqq5NcnuTzSX5/bP94d988Hj8lyTlJ0t2fTPK5JEcmedqy7TuS7FjH+X40yenjPTu7+7b1fZQ1HZjkrVV1bZLzMgt6q/mxJD89Pu/HkjwwyfbMPt953b3U3X+d5ENj/0cmubm7PzWevz2zz7zLu5Y9fluSE0bofXGSd66j7tOS/EV3X7Lai1V1YlVdXlWXX377Tes4HAAAwO6tp4N4x8rOXVUlydc3eO7lawsdvMFjJcknkvzIbl47KcmtSR6bWSi+czf7VZLXdveFd9tY9eNz1rT8O3pvkl9N8udJrujur671xqr61cyun3zV7vbp7jOSnJEkb/zBl1urCQAAkvQmHuHc7PbVfRAvSfLyZLaSaJIfSHJjkr/IWASmqo5Ocsyy99xaVY+uqgOSvGDZ9g9mNqaZcU3gYUm+luT+e6jhnUmeXFU/sWtDVT1tnPewJF/q7qXMRjZ3LRqz8rgXJnn1rlVDx6qo90tyaZIXjmsRD09y/Nj/xiRHVNUjxvNXJPnwasV1953j+KdnD+OlVfVzSf6HJC8dNQMAAPuZqnr2uLvBTVV18iqv/9uqun6sn/LBqvrBjZ5zXwXE05IcMEY435Xkld39jczC0KFVdUOSU3P3awRPTvL+JB9J8qVl21+X5OnjWFckOWp02y4dt31YdZGa7r4jyXOTvHbc5uL6zFY+/cqo72eq6prMVgfd1dnbkWTnWAznpMzGQK9PcmVVXZfk9zLrsr43yS3jtXMyu5bythH6Tkhy3qh3KcnvrvE9vWPsc9Ea+2Qc4/AkHx0L6Jyyh/0BAIAJGZemvSWzdWCOSvLSqlp5qdxVSY7r7mOSvCfJb2z0vHscMe3uQ1fZdnGSi5c93xWUVu53R5KX7Oa478nsQ2XNG7UAACAASURBVKzcfmuS56+yfY+3oxjXPz57lZduzd27l7889v9mZtc8Lvcr4+duquoN3X17VT0ws5VFrx3H+GCSH1qlliNWqeMpSc7q7p17+Bz3xq1DAABgkjbzKqF74YlJburuzyRJVZ2bWU66ftcO3f2hZfv/f0l+aqMnFUTW7/1V9YAkByV541isZt3GbT4enu8MpAAAwH5m3EHhxGWbzhhrjOzy4CR/tez5LUmetMYhfzbJ/7PRurZcQKyqxyT5wxWbv9Hda31ZG9bdx2/w/S9YuW2Exoet2PzLKxfJAQAApmX5gpMbVVU/ldlt/Ha3aOe6bbmA2N3XJlnrfohbxmqhEQAA2JiJLO//hSQPXfb8IWPb3VTVM5P8r0l+ZKwDsyH7apEaAAAA9p3LkmyvqodV1UGZre1y/vIdquqHMltY83nd/eV9cVIBEQAAYJPp7m8leU1mt8q7Icm7u/sTVXVqVT1v7PamJIdmdleFq6vq/N0cbt223IgpAADAWiayimm6+4IkF6zYdsqyx8/c1+fUQQQAACCJgAgAAMAgIAIAAJDENYgAAMDE9ESuQVwEHUQAAACSCIgAAAAMRkwBAIBJWVp0AVuYDiIAAABJBEQAAAAGI6YAAMCkdKxiOi8dRAAAAJIIiAAAAAxGTAEAgElZ6kVXsHXpIAIAAJBEQAQAAGAwYgoAAEzKklVM56aDCAAAQBIBEQAAgEFABAAAIIlrEAEAgIlp1yDOTQcRAACAJAIiAAAAgxFTAABgUpYWXcAWpoMIAABAEgERAACAwYgpAAAwKVYxnZ8OIgAAAEkERAAAAAYjpgAAwKRYxXR+OogAAAAkERABAAAYjJgCAACTYsR0fjqIAAAAJBEQAQAAGIyYAgAAk9KpRZewZekgAgAAkERABAAAYDBiOhGS/uJ8c9EF7Ofe+sVLF13CfuuHjzll0SXs116849RFl7Bf++KPnbjoEgDuEQIiAAAwKUsuQZybxhMAAABJBEQAAAAGI6YAAMCkLLnNxdx0EAEAAEgiIAIAADAYMQUAACalF13AFqaDCAAAQBIBEQAAgMGIKQAAMClLiy5gC9NBBAAAIImACAAAwGDEFAAAmJSlqkWXsGXpIAIAAJBEQAQAAGAQEAEAAEjiGkQAAGBietEFbGE6iAAAACQREAEAABiMmAIAAJOytOgCtjAdRAAAAJIIiAAAAAxGTAEAgElZqkVXsHXpIAIAAJBEQAQAAGAwYgoAAEzKUsyYzksHEQAAgCQCIgAAAIMRUwAAYFJ60QVsYTqIAAAAJBEQAQAAGAREAAAAkrgGEQAAmJgld7mYmw4iAAAASQREAAAABiOmAADApCwtuoAtTAcRAACAJAIiAAAAgxFTAABgUnrRBWxhOogAAAAkERABAAAYjJgCAACTslSLrmDr0kEEAAAgyToCYlXtrKqrq+q6qjqvqu4778mq6uKqOm6O9z2gqn5hHfsdWVUXVNWnq+rKqnp3VR2+xv5HVNXL9raee1pV/X5VXVNVO6rqPVV16KJrAgAApm89HcQ7uvvY7j46yV1Jfn75i1V1b4ypPiDJmgGxqg5O8qdJTu/u7d39uCSnJXnQGm87Isk9HhCrattevuWk7n5sdx+T5PNJXnMPlAUAAJO0tAV+Nqu9HTG9JMkjqur4qrqkqs5Pcn1VHVxVZ1XVtVV1VVU9PUmq6pCqOreqbqiq9yU5ZNeBqur2ZY9fVFVnj8eHV9X7Rgftmqp6cpJfS/Lw0cl8025qe1mSj3b3n+za0N0Xd/d1o1N4yegqXjmOmXHcp47jnlRV26rqTVV12ejevWrUdEBVnVZVn6yqD4wu5YvGa88Yn/naqjqzqr5rbP9sVf16VV2Z5OTx667Pu33585W6+x/GfjW+Myv1AgAA97h1d/9Gp/A5Sf5sbHpckqO7++aqen2S7u7HVNWjklxUVUcmeXWSf+zuR1fVMUl2G4qWeXOSD3f3C0bn7dAkJ49zHbvG+45OcsVuXvtykmd1951VtT3JHyU5bhz3Dd393PEZT0xyW3c/YQS9S6vqoiSPz6zbeFSS701yQ5IzR9fy7CTP6O5PVdUfjM/82+O8Xx2dzFTVM6vq2O6+OskJSc5a60uoqrOS/HiS65O8fq19AQAA9oX1dBAPqaqrk1ye2bjj74/tH+/um8fjpyQ5J0m6+5NJPpfkyCRPW7Z9R5Id6zjfjyY5fbxnZ3fftr6PsqYDk7y1qq5Ncl5mQW81P5bkp8fn/ViSBybZntnnO6+7l7r7r5N8aOz/yCQ3d/enxvO3Z/aZd3nXssdvS3LCCL0vTvLOtQru7hOSfH9mYfTFq+1TVSdW1eVVdfllt9+01uEAAGC/sejx0amPmO66BvHY7n5td981tn99g+dePjZ58AaPlSSfyKzTt5qTktya5LGZdQ4P2s1+leS1yz7vw7r7og3UtPw7em9mHdjnJrmiu7+6pzd3984k5yZ54W5eP6O7j+vu455w6CM2UCYAAMC+u83FJUlensxWEk3yA0luTPIXGYvAVNXRSY5Z9p5bq+rRVXVAkhcs2/7BzMY0M64JPCzJ15Lcfw81vDPJk6vqJ3ZtqKqnjfMeluRL3b2U5BVJdi0as/K4FyZ5dVUduOuzVNX9klya5IXjWsTDkxw/9r8xyRFVtSudvSLJh1crrrvvHMc/PWuMl9bMI3Y9TvK8JJ/cw2cHAADYsH0VEE9LcsAY4XxXkld29zcyC0OHVtUNSU7N3a8RPDnJ+5N8JMmXlm1/XZKnj2NdkeSo0W27dNxqY9VFarr7jsy6c68dt7m4PrOVT78y6vuZqromyaPy7c7ejiQ7x2I4J2U2Bnp9kiur6rokv5fZdZrvTXLLeO2czK6lvG2EvhOSnDfqXUryu2t8T+8Y+6zVlawkbx/HuzbJ92X23QEAANyj9rhITXd/xz34uvviJBcve74rKK3c744kL9nNcd+T5D2rbL81yfNX2b7H21GM6x+fvcpLt+bu3ctfHvt/M7NrHpf7lfFzN1X1hu6+vaoemOTjmYW3dPcHk/zQKrUcsUodT0ly1hgd3d1nWEryw7t7HQAAWFvXoivYuu6NexhOxfur6gGZXb/4xrFYzbqN23w8PN8ZSAEAADaFLRcQq+oxSf5wxeZvdPeT7snzdvfxG3z/C1ZuG6HxYSs2/3J3X7iRcwEAAMxjywXE7r42yVr3Q9wyVguNAADAxmzm20hsdvtqkRoAAAC2OAERAACAJFtwxBQAAGAtRkznp4MIAABAEgERAACAwYgpAAAwKb3oArYwHUQAAACSCIgAAAAMRkwBAIBJWapFV7B16SACAACQREAEAABgEBABAABI4hpEAABgYpYWXcAWpoMIAABAEgERAACAwYgpAAAwKUZM56eDCAAAQBIBEQAAgMGIKQAAMCm96AK2MB1EAAAAkgiIAAAADEZMAQCASVmqRVewdekgAgAAkERABAAAYDBiCgAATMrSogvYwnQQAQAASCIgAgAAMBgxBQAAJqUXXcAWpoMIAABAEgERAACAQUAEAAAgiWsQAQCAiVlyFeLcdBABAABIIiACAAAwGDEFAAAmZWnRBWxhOogAAACbUFU9u6purKqbqurkVV7/rqp613j9Y1V1xEbPKSACAABsMlW1LclbkjwnyVFJXlpVR63Y7WeT/F13PyLJbyX59Y2eV0AEAAAmpbfAzzo8MclN3f2Z7r4ryblJnr9in+cneft4/J4kz6iqWt/hVycgAgAAbD4PTvJXy57fMratuk93fyvJbUkeuJGTWqRmIrZlQ/9QwAbsdJ+dhbrfQQcvuoT91t9vW3QF+7cv/tiJiy5hv/b9F52x6BKALa6qTkyy/C/zM7p74X+5CIgAAMCkbIVVTEcYXCsQfiHJQ5c9f8jYtto+t1TVfZIcluSrG6nLiCkAAMDmc1mS7VX1sKo6KMlLkpy/Yp/zk/zMePyiJH/e3Rsab9NBBAAA2GS6+1tV9ZokFybZluTM7v5EVZ2a5PLuPj/J7yf5w6q6KcnfZhYiN0RABAAAJmVpIstzdPcFSS5Yse2UZY/vTPKv9+U5jZgCAACQREAEAABgEBABAABI4hpEAABgYpbcp3puOogAAAAkERABAAAYjJgCAACTYsB0fjqIAAAAJBEQAQAAGIyYAgAAk7K06AK2MB1EAAAAkgiIAAAADEZMAQCASVmyjuncdBABAABIIiACAAAwGDEFAAAmxYDp/HQQAQAASCIgAgAAMAiIAAAAJHENIgAAMDFLiy5gC9NBBAAAIImACAAAwGDEFAAAmJQlN7qYmw4iAAAASQREAAAABiOmAADApBgwnZ8OIgAAAEkERAAAAAYjpgAAwKQsLbqALUwHEQAAgCQCIgAAAIMRUwAAYFLaOqZz00EEAP7/9u49WrKquvf499fNo0Gal4gaRQVEUJBHgwTUEBUQH9HEqFFUBFGIoBHiDV6TaEAdI9Hk5hqikYhyETQ+EDUhmiCKDwiK2LxBRRREQBQVwQYE7D7z/lH7aHE83Y2HPrWqa38/Y9Q4e69dp3r27B51zqw119qSJAEWiJIkSZKkji2mkiRJkiaKu5jOnTOIkiRJkiTAAlGSJEmS1LFAlCRJkiQBrkGUJEmSNGGmvM3FnPV2BjHJiiSXJLkiyceTbLgGXvO4JH+xJuKTJEmSpFHrbYEI/KKqdq2qnYB7gFff129MsnD+wpIkSZKkNvpcIA47F3h0kqck+fT0YJJ3JzmkO/5eknckuQh4YZJnJLkoyaVJzh56rccl+VKSa5K8bui1/j3JhUmuTHJ4N7YwyQe6WczLk/x5N75tkjO755+bZIdRJEGSJEmaBLUWPMZV79cgJlkHeCZw5n14+k+rakmSBwEXAftU1bVJNh96zg7AU4HFwFVJTqiqXwKHVtUtSTYAvp7kE8CjgId1s5gk2bR7jROBV1fV1Ul+F3gP8LT7/7eVJEmSpJXrc4G4QZJLuuNzgZOAJ67mez7Wfd0LOKeqrgWoqluGnvOZqrobuDvJzcCDgRuA1yV5XvecrYDtgKuAbZK8C/gMcFaSjbo4Pp5k+jXXn+PfUZIkSZLusz4XiL+oql2HB5Is595tt4tmfM8d9+F17x46XgGsk+QpwH7A3lV1Z5IvAYuq6mdJdgEOYLAG8k+Ao4FbZ8Y2m65V9XCA522+J3tutN19CE+SJEmabO5iOneuQby36xisIVy/a/fcdyXPOx/YJ8nWADNaTGezCfCzrjjcgcEMJEm2ABZU1SeANwFLqurnwLVJXtg9J10R+Ruq6sSq2qOq9rA4lCRJknR/9XkG8TdU1fVJTgOuAK4FLl7J837czd59MskC4GZg/1W89JnAq5N8k0Fb6fnd+MOAk7vXAPjL7utLgROSvAlYF/gocOnc/2aSJEmStHq9LRCraqOVjL8BeMMs44+acf7fwH/PGDtuxvlOQ6fPXEkoS2b5s64FnrGS50uSJElahanWAazFbDGVJEmSJAEWiJIkSZKkTm9bTCVJkiRNpnIX0zlzBlGSJEmSBFggSpIkSZI6FoiSJEmSJMA1iJIkSZImjLe5mDtnECVJkiRJgAWiJEmSJKlji6kkSZKkieJtLubOGURJkiRJEmCBKEmSJEnq2GIqSZIkaaK4i+ncOYMoSZIkSQIsECVJkiRJHVtMJUmSJE2UqXIX07lyBlGSJEmSBFggSpIkSZI6tphKkiRJmig2mM6dM4iSJEmSJMACUZIkSZLUscVUkiRJ0kSZssl0zpxBlCRJkiQBFoiSJEmSpI4FoiRJkiQJcA2iJEmSpAlTrkGcM2cQJUmSJEmABaIkSZIkqWOLqSRJkqSJMtU6gLWYM4iSJEmSJMACUZIkSZLUscVUkiRJ0kSZchfTOXMGUZIkSZIEWCBKkiRJkjq2mEqSJEmaKGWL6Zw5gyhJkiRJAiwQJUmSJEkdW0wlSZIkTZSp1gGsxZxBlCRJkiQBFoiSJEmSpI4FoiRJkiQJcA2iJEmSpAlT5W0u5soZREmSJEkSYIEoSZIkSerYYipJkiRpokxhi+lcOYMoSZIkSQIsECVJkiRJHVtMJUmSJE2UqdYBrMWcQZQkSZIkARaIkiRJkqSOLaaSJEmSJkq5i+mcOYMoSZIkSQIsECVJkiRJHVtMJUmSJE2UKVtM58wZREmSJEkSYIEoSZIkSepYIEqSJEmSANcgSpIkSZowVa5BnCtnECVJkiRJgAWiJEmSJKlji6kkSZKkiTLVOoC1mDOIkiRJkiTAAlGSJEmS1LHFVJIkSdJEKdzFdK6cQZQkSZIkARaIkiRJkqSOLaaSJEmSJsqULaZz5gyiJEmSJAmwQJQkSZIkdWwxlSRJkjRRqmwxnStnECVJkiRJgAWiJEmSJKlji6kkSZKkieIupnPnDKIkSZIkCbBAlCRJkiR1LBAlSZIkSYBrECVJkiRNmHIN4pw5gyhJkiRJAiwQJUmSJEkdW0wlSZIkTZSpssV0rpxBlCRJkiQBFoiSJEmStFZJsnmSzyW5uvu62SzP2TXJV5NcmeSyJC+6L69tgShJkiRpotRa8Lif3gicXVXbAWd35zPdCby8qnYEngH8U5JNV/fCFoiSJEmStHb5Q+CU7vgU4I9mPqGqvl1VV3fHPwBuBh60uhe2QJQkSZKkEUtyeJKlQ4/Df4tvf3BV3dQd/xB48Gr+rD2B9YDvru6F3cVUkiRJ0kSZWhNNnPOsqk4ETlzZ9SSfBx4yy6W/nvE6lWSlf+EkDwU+CBxcVVOri8sCUZIkSZLGTFXtt7JrSX6U5KFVdVNXAN68kudtDHwG+OuqOv++/Lm2mP6WkjwwySXd44dJbhw6X28N/1mbJjlyTb6mJEmSpLXeGcDB3fHBwH/MfEJXm3wKOLWqTr+vL2yB+Fuqqp9W1a5VtSvwr8A7p8+r6p6VfV+SuczWbgpYIEqSJEm/hSlq7B/309uB/ZNcDezXnZNkjyTv757zJ8A+wCFDE1q7ru6FbTFdA5IcBhzOYOHnd4CDqurOJB8A7gJ2A85L8i/AvwEPYFDlH11VG3WvcQyDf8T1gU9V1bEM/qG3TXIJ8LmqOma0fzNJkiRJ46aqfgrsO8v4UuBV3fGHgA/9tq/tDOKa8cmqekJV7QJ8E3jl0LWHA0+sqtcDxwPHV9XjgRumn5Dk6cB2wJ7ArsDuSfZhcD+T73azkxaHkiRJkuaVBeKasVOSc5NcDrwU2HHo2serakV3vDfw8e74w0PPeXr3uBi4CNiBQcG4SsNb415w+9X39+8gSZIkqedsMV0zPgD8UVVdmuQQ4ClD1+64D98f4O+q6r33GkwetapvGt4a9+2PfNn47+UrSZIkjUCVvxrPlTOIa8Zi4KYk6zKYQVyZ84Hnd8cvHhr/LHBokun1iA9LsiWwrHttSZIkSZp3FohrxpuBrwHnAd9axfOOBl6f5DLg0cBtAFV1FoOW0692baqnA4u7xafnJbkiyT/M519AkiRJkmwxvR+q6rih0xNmuX7IjKEbgb2qqpK8GNh+6LnHM9jEZuZrvGSNBCtJkiT1xBq4jURvWSCO1u7Au5MEuBU4tHE8kiRJkvQrFogjVFXnAru0jkOSJEmSZmOBKEmSJGmilC2mc+YmNZIkSZIkwAJRkiRJktSxxVSSJEnSRKmyxXSunEGUJEmSJAEWiJIkSZKkji2mkiRJkibKlLuYzpkziJIkSZIkwAJRkiRJktSxxVSSJEnSRHEX07lzBlGSJEmSBFggSpIkSZI6FoiSJEmSJMA1iJIkSZImjLe5mDtnECVJkiRJgAWiJEmSJKlji6kkSZKkiVK2mM6ZM4iSJEmSJMACUZIkSZLUscVUkiRJ0kSZKltM58oZREmSJEkSYIEoSZIkSerYYipJkiRporiL6dw5gyhJkiRJAiwQJUmSJEkdW0wlSZIkTRR3MZ07ZxAlSZIkSYAFoiRJkiSpY4EoSZIkSQJcgyhJkiRpwnibi7lzBlGSJEmSBFggSpIkSZI6tphKkiRJmije5mLunEGUJEmSJAEWiJIkSZKkji2mkiRJkiaKu5jOnTOIkiRJkiTAAlGSJEmS1LHFVJIkSdJEcRfTuXMGUZIkSZIEWCBKkiRJkjopp181BpIcXlUnto6jj8x9W+a/LfPfjrlvy/y3Zf7n3zZb7Db2Rc41P7k4rWOYjTOIGheHtw6gx8x9W+a/LfPfjrlvy/y3Zf41tiwQJUmSJEmAu5hKkiRJmjBVU61DWGs5g6hxYR9+O+a+LfPflvlvx9y3Zf7bMv8aW25SI0mSJGmibP3AXca+yLn2p5e6SY0kSZIkaXy5BlGSJEnSRJli7CcQx5YziJIkSZIkwAJRkiRJmldJ3nFfxqRxYIGoJpI8Jsn7kpyV5AvTj9Zx9UmSo5JsnIGTklyU5Omt4+qrJO5oN8+SLEzyp0neluRJM669qVVcfZFkwyRvSHJMkkVJDklyRpK/T7JR6/j6oPvZe3aSK7rznf2/PzL7zzL2zJFH0SNVNfaPceUupmoiyaXAvwIXAiumx6vqwmZB9UySS6tqlyQHAH8KvBn4YFUtaRzaxEqy+couAZdW1cNHGU/fJHk/sCFwAXAQ8OWqen137SL/78+vJKcB1wMbANsD3wQ+BjwXeEhVHdQwvF5I8mXgGOC9VbVbN3ZFVe3UNrLJleQI4EhgG+C7Q5cWA+dV1cuaBNYDj9j88WNf5Hz/lsvHchdTN6lRK8ur6oTWQfTc9JvSsxgUhlcmGcs3qgnyY+A6fp17gOrOt2wSUb/sWVU7AyR5N/CeJJ8EDuTe/yaaH4+pqj/p3mduAvarqkryP8CljWPriw2r6oIZb/XLWwXTEx8G/hv4O+CNQ+PLquqWNiFJq2aBqFb+M8mRwKeAu6cHfbMcqQuTnAVsDfxlksXAVOOYJt01wL5V9f2ZF5Jc3yCevllv+qCqlgOHJ/kb4AuALY4j0hWF/1VdC1N3Pvaf9E+InyTZlsEHUyR5AYNiXfOkqm4DbgMOTPJkYLuqOjnJFkm2rqprG4c4sdzFdO4sENXKwd3XY4bGikELhkbjlcCuwDVVdWeSBwKvaBzTpPsnYDPgNwpE4O9HHEsfLU3yjKo6c3qgqt6a5AeAHQ3zb2mSjarq9qo6dHqwK1iWNYyrT14DnAjskORG4FrAFscRSHIssAeD9uqTGXxg9SHgSav6PqkF1yBKPZVkn9nGq+qcUceie0uyf1V9rnUcfWX+Ry9JpmcUzf/8S/IAYEFVWZiPSJJLgN2Ai4bWf1423fauNe/hm+809kXODbdcMZbLG5xBVBNJ1gWOAKaLlC8xWDT/y2ZB9c/w7O0iYE8GmwY9rU04GvIOwF+Q2zH/I1b3/rTa/M+TJEcxmL1aBrwvyRLgjVV1VtvIeuGe4XbqrkjXPHISbO68zYVaOQHYHXhP99gdW7xGqqqeM/TYH9gJ+FnruAS4YUpr5r8t8z9/Dq2qnwNPBx7IYDfft7cNqTdOS/JeYNMkhwGfB97XOCZpVs4gqpUnVNUuQ+df6G59oXZuAB7bOggBuLK+MfPflvmfP8O7V5/q7tWjU1X/J8n+wM8ZrEP8G1upNa4sENXKiiTbVtV3AZJsw9D9EDX/kryLX/8itoDBhjUXtYtIkjTP3L26oa4gtCgckSlbTOfMAlGtHAN8Mck1DD7RfCTuoDlqS4eOlwMfqarzWgWje/le6wB67nutA+i577UOYIK5e3UjSZbxm7PjtzH4Wfy/quqa0Uclzc5dTNVMkvUZtFkAXFVVd6/q+VqzkhxVVcevbkxrXpILgf8HfLiqXPc5Yua/LfPfVpLNgO0YbE4GuHv1KCR5G4OlHB9m8MH4i4FtGXTuHFFVT2kX3WR66KaPG/si56ZbvzGWLd5uUqORSvK07usfA88GHt09nt2NaXQOnmXskFEH0VMvAn4H+HqSjyY5wHVAI2X+2zL/jSR5FXAO8FngLd3X41rG1CPPrar3VtWyqvp5VZ0IHFBVH2Nwf1xpbDiDqJFK8paqOjbJybNcruGbJ2t+JDkQeAnwZODcoUuLgamq2rdJYD2UZAHwBwx28F3BYPv546vqlqaB9YT5b8v8j16Sy4EnAOdX1a5JdgD+tqr8gHaeJfkq8E7g9G7oBcDrq2qvJJdU1a7toptMD9n0sWNf5Pzw1m+O5YdjrkHUSFXVsd3hW6vq2uFrSbZuEFIffQW4CdgC+Meh8WXAZU0i6qEkOzNY+/Ms4BPAvzEo2r/AYI2Q5pH5b8v8N3NXVd2VhCTrV9W3kmy/+m/TGvBS4HgGt/Yq4HzgZUk2AF7bMjBpJgtEtfIJYMmMsdMZ3A9R86iqrgOuA/ZuHUtfdWuwbgVOYnCT6un1t19L8qR2kfWD+W/L/Dd1Q5JNgX8HPpfkZwx+HmgeJVkIHFlVz1nJU/5nlPFIq2OBqJHq2ll2BDaZseZwY4YWzGv+JdkLeBeDex+uBywE7qiqjZsGNuG6trpPVNXfznbdVq/5Zf7bMv9tVdXzusPjknwR2AQ4s2FIvVBVK5I8uXUcfeMyurlzkxqN2vYM1pxsCjxn6LEEOKxhXH30buBA4GpgA+BVwL80jagHqmoK8JfgRsx/W+a/vSQLk/wOcC1wCfCQxiH1xcVJzkhyUJI/nn60DkqajZvUqIkke1fVV1vH0WdJllbVHkkuq6qdu7GLq2q31rFNuiRvB34CfAy4Y3rczTlGw/y3Zf7bSfJnwLHAj4CpbrimfwZo/rg53+g9eJMdxr7I+dFt3xrLTWosENVEkkUMbti7I/e+F5NvlCOS5BxgP+D9wA8ZbFxzSFXt0jSwHkhy7SzDVVXbjDyYHjL/bZn/dpJ8B/jdqvpp61ik+fagTbYf+yLnx7ddNZYFomsQ1coHgW8BBwBvZbC71zebRtQ/BzFoM38t8OfAVtj6NRJV5Y69DZn/tsx/U9cDt7UOoo/8YFxrEwtEtfLoqnphkj+sqlOSfJh735NP8++PRsNFpAAADwNJREFUqup44C4GN0wmyVEMtuHWPEqyLnAEsE839CXgvVX1y2ZB9Yj5b8v8j16S13eH1wBfSvIZYHr3WKrq/zYJrF/8YFxrDVtM1USSC6pqz67N8UgGLY4X2GI0OkkuqqolM8ZcgzgCSd4PrAuc0g0dBKyoqle1i6o/zH9b5n/0khy7qutV9ZZRxdI3SdapquXTP1+n1/13H5ScW1V7tY5xUm2x8WPGvsj5yc+/bYupNOTEJJsBbwbOADbqjjXPkhwIvATYOskZQ5c2BtwkYjSeMGOt5xeSXNosmv4x/22Z/xGzAGzqAgY7tU/PkN+aZCcGH4xv2SwqaRUsENVEVb2/O/wy4KzhaH2FwYY0WwD/ODS+DLisSUT9syLJtlX1XYAk2wArGsfUJ+a/LfPfSJLPAS+sqlu7882Aj1bVAW0j64XpD8bfhB+Ma8xZIKqJJA8EjgOeBBSD9Ydvc2e1+VdV1wHXAXvDr/4t9gFur6rlLWPrkWOALya5BgjwSOAVbUPqFfPflvlv50HTxSFAVf0sibNY82vLoTWg0//Pp+85/IAG8fTGlMvo5swCUa18FDgHeH53/lIG98Tar1lEPZHk08Abq+qKJA8FLgKWAtsmObGq/qlthJOvqs5Osh2wfTd0VVXdvarv0Zpj/tsy/02tSPKIqvo+QJJHMviQVvNnIYPZwtnWmpl7jSU3qVETSa6oqp1mjF1eVY9vFVNfJLmyqnbsjv8K2KGqXp5kMXCeN0yef0lmu53IbcDlVXXzqOPpG/PflvlvJ8kBwPsYLO8I8HvA4VX12aaBTbDZNoTTaGy+eLuxL3JuWXa1m9RIQ85K8mLgtO78BYA/oEZjeCv5fRn8skBVLUsy1Sak3nklgxbfL3bnTwEuZLBx0Fur6oOtAusJ89+W+W8gyQJgEwYbpkzvnHl0Vf2kXVS9MJYFgLQqFohq5TDgaOBD3fkC4I4kfwpUVW3cLLLJd32SPwNuYPCLwpkASTZgsPW85t86wGOr6kcASR4MnAr8LoPWa39Bnl/mvy3z30BVTSV5Q1WdBny6dTw9sm/rAPrKLsm5W9A6APVTVS2uqgVVtU73WNCNLbY4nHevBHYEDgFeNLRhwV7Aya2C6pmtpn857tzcjd3CvWd4NT/Mf1vmv53PJ/mLJFsl2Xz60TqoSdb9v5bWKs4gqokk+8w2XlXnjDqWvunW+Lx6lvEv8uuWL5K8q6r+bJSx9ciXus2CPt6dv6AbewBw68q/TWuI+W/L/Lfzou7ra4bGCm83JWmIm9SoiST/OXS6CNgTuLCqntYoJM3gwvr5kyTAHwNP7obOAz5RviGPhPlvy/xLGoVNNtp27N9Tbrv9u2O5RtUZRDVRVc8ZPk+yFeDtFdQLVVVJlgK3VdXnk2zIYBv0ZY1D6wXz35b5byfJy2cbr6pTRx2LpPHlGkSNixuAx7YOQhqFJIcBpwPv7YYeBvx7u4j6xfy3Zf6besLQ4/eA44DntgxI0vhxBlFNJHkXv75B7AJgVwY3bNf4GMu2hwnxGgZt1V8DqKqrk2zZNqReMf9tmf9GZq4rT7Ip8NFG4Ujzyq71ubNAVCtLh46XAx+pqvNaBaNZHd86gAl2d1XdM1iKBUnW4dcfmGj+mf+2zP/4uAPYunUQksaLBaJaOR24q6pWACRZmGTDqrqzcVwTr9sgaKW/jFXVc7uvHxhVTD305SR/BWyQZH/gSOA/V/M9WnPMf1vmv5EZ7/8LgMcBp7WLSNI4chdTNZHkfGC/qrq9O98IOKuqntg2ssmX5PdXdb2qvjyqWPoqyQIG96N8OoNW3s9W1fvaRtUf5r8t89/OjPf/5cB1VXVDq3ik+bTRhluPfZFz+53XjuVyHgtENZHkkqradXVj0iRKclRVHb+6Mc0P89+W+R+9JIsY3P/20cDlwElVtbxtVNL8skCcO3cxVSt3JPnVPfaS7A78omE8vZNkuySnJ/lGkmumH63j6omDZxk7ZNRB9Jj5b8v8j94pwB4MisNnAv/YNhxJ48w1iGrlaODjSX7AoMXoIcCL2obUOycDxwLvBJ4KvAI/NJpXSQ4EXgJsneSMoUuLgVvaRNUf5r8t89/U46rq8QBJTgIuaByPNO/Kva/mzAJRTVTV15PsAGzfDV1VVb9sGVMPbVBVZydJVV0HHJfkQuBvWgc2wb4C3ARswb0/wV8GXNYkon4x/22Z/3Z+9fO1qpZP7yArSbNxDaKaSPIa4N+q6tbufDPgwKp6T9vI+iPJV4AnM9hR9gvAjcDbq2r7VX6jJGmtkmQFg1tawKBrZwPgzu64qmrjVrFJ8+UBGz5q7IucO+783lh+WmM7mVo5bLo4BKiqnwGHNYynj44CNgReB+wOvAx4edOIeiLJXkm+nuT2JPckWZHk563j6gvz35b5H72qWlhVG3ePxVW1ztCxxaEm0lTV2D/GlQWiWlmYoR6XJAuB9RrG00ePqqrbq+qGqnpFVT0feETroHri3cCBwNUMPsl/FfAvTSPqF/PflvmXpDFmgahWPgt8LMm+SfYFPgqc2TimvvnL+zimeVBV3wEWVtWKqjoZeEbrmPrE/Ldl/iVpfLlJjVp5M4OW0iO7888CJ7ULpz+SPBN4FvCwJP88dGljBjdO1vy7M8l6wCVJ/p7Bxh1+YDc65r8t8y9JY8w3ZI1UknW6Xwi+z+C+V1sDvw9sh/8fR+UHwFLgLuDCoccZwAEN4+qTgxj8f38tg40jtgKe3zSifjH/bZl/SfOuqsb+Ma7cxVQjleSdDO559edVtawbW8xgy/NfVNVRLePrkyTrVJUzhiOU5EHAg6rqGzPGdwRurqoft4msH8x/W+Zf0igtWvSIsS9y7rrr++5iKgF/wGAH02XTA93xEQzaHjXPkpzWHV6c5LKZj6bBTb53MbgH3EybA8ePOJY+Mv9tmX9JWgs4g6iRSvLtqnrMb3tNa06Sh1bVTUkeOdv1qrpu1DH1RZKlVbXHSq5dUVU7jTqmPjH/bZl/SaO0/qKtxr7Iufuu68dyBtFNajRq30jy8qo6dXgwycuAbzWKqVe64nAh8IGqemrreHpm8SqurTuyKPrL/Ldl/iVpLWCBqFF7DfDJJIcy2BgFYA8G98J6XrOoeqaqViSZSrJJVd3WOp4e+U6SZ1XVfw0PdjvLXtMopj4x/22Zf0laC9hiqiaSPA3YsTv9RlWd3TKePkryH8BuwOcY7CQIQFW9rllQEy7JdsBngK9w7w9I9gb+oKq+3Sq2PjD/bZl/SaO03voPH/si5567bxjLFlMLRKmnkhw823hVnTLqWPokyfrAS4Dp9VZXAh+uqrvaRdUf5r8t8y9pVCwQ584CUZLGTJKvVtXerePoK/PflvmXtCZYIM6daxClnuravf4OeBywaHq8qrZpFpSmLVr9UzSPzH9b5l/S/eYk2Nx5H0Spv04GTgCWA08FTgU+1DQiTfOnWlvmvy3zL0kNWSBK/bVBtzlQquq6qjoOeHbjmCRJktSQLaZSf92dZAFwdZLXAjcCGzWOSQNjuSahR8x/W+Zf0v1mK8LcOYMo9ddRwIbA64DdgYOAWXc21ZqV5B2rGTtohOH0jvlvy/xL0nhzF1NJGrEkF1XVkhljl1XVzq1i6hPz35b5lzQK66z3sLEvcpbfc+NYdkzYYir1TJIzVnW9qp47qlj6JskRwJHAtkkuG7q0mMHNwzWPzH9b5l/SKI1r8bU2cAZR6pkkPwauBz4CfI0Z632q6sst4uqDJJsAmzG4vcgbhy4tq6pb2kTVH+a/LfMvSWsHC0SpZ5IsBPYHDgR2Bj4DfKSqrmwaWI8k2Qu4sqqWdecbA4+tqq+1jawfzH9b5l+SxpsFotRjSdZnUCj+A/CWqnp345B6IcnFwJLq3oC73WSXzlyXpflh/tsy/5I03lyDKPVQVxg+m0Fx+Cjgn4FPtYypZ1JDn85V1VQS349Hx/y3Zf4laYx5mwupZ5KcCnwVWMJg1vAJVfW2qrqxcWh9ck2S1yVZt3scBVzTOqgeMf9tmX9JGmO2mEo9k2QKuKM7HX4DCFBVtfHoo+qXJFsymLV9GoN/g7OBo6vq5qaB9YT5b8v8S9J4s0CUJEmSJAGuQZSkkUtyMveevQWgqg5tEE7vmP+2zL8kjTcLREkavU8PHS8Cngf8oFEsfWT+2zL/kjTGbDGVpMa6bf7/p6qe2DqWPjL/bZl/SRov7mIqSe1tB2zZOogeM/9tmX9JGiO2mErSiCVZxmANVrqvPwT+d9OgesT8t2X+JWm82WIqSZIkSQKcQZSkkUmyZFXXq+qiUcXSR+a/LfMvSWsHZxAlaUSSfLE7XATsAVzKoM1uZ2BpVe3dKrY+MP9tmX9JWju4SY0kjUhVPbWqngrcBCypqj2qandgN+DGttFNPvPflvmXpLWDBaIkjd72VXX59ElVXQE8tmE8fWP+2zL/kjTGXIMoSaN3WZL3Ax/qzl8KXNYwnr4x/22Zf0kaY65BlKQRS7IIOALYpxs6Bzihqu5qF1V/mP+2zL8kjTcLRElqIMl6wPYM7gN3VVX9snFIvWL+2zL/kjS+LBAlacSSPAU4Bfgeg10ctwIOrqpzGobVG+a/LfMvSePNAlGSRizJhcBLquqq7vwxwEe6HR01z8x/W+Zfksabu5hK0uitO/3LMUBVfRtYt2E8fWP+2zL/kjTG3MVUkkbvwll2cVzaMJ6+Mf9tmX9JGmO2mErSiCVZH3gN8ORu6FzgPVV1d7uo+sP8t2X+JWm8WSBK0gglWQhcWVU7tI6lj8x/W+ZfksafaxAlaYSqagVwVZJHtI6lj8x/W+ZfksafaxAlafQ2A65McgFwx/RgVT23XUi9Yv7bMv+SNMYsECVp9N7cOoCeM/9tmX9JGmMWiJI0IkkWAa8GHg1cDpxUVcvbRtUf5r8t8y9Jawc3qZGkEUnyMeCXDHZtfCZwXVUd1Taq/jD/bZl/SVo7WCBK0ogkubyqHt8drwNcUFVLGofVG+a/LfMvSWsHdzGVpNH55fSBrXVNmP+2zL8krQWcQZSkEUmygl/v2hhgA+DO7riqauNWsfWB+W/L/EvS2sECUZIkSZIE2GIqSZIkSepYIEqSJEmSAAtESZIkSVLHAlGSJEmSBFggSpIkSZI6/x/n94EkyeSymQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def oneHotEncode(df,colNames):\n",
        "    for col in colNames:\n",
        "        if( df[col].dtype == np.dtype('object')):\n",
        "            dummies = pd.get_dummies(df[col],prefix=col)\n",
        "            df = pd.concat([df,dummies],axis=1)\n",
        "\n",
        "            #drop the encoded column\n",
        "            df.drop([col],axis = 1 , inplace=True)\n",
        "    return df\n",
        "\n",
        "print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\n",
        "combined = oneHotEncode(combined, cat_cols)\n",
        "print('There are {} columns after encoding categorical features'.format(combined.shape[1]))"
      ],
      "metadata": {
        "id": "MlOzjdcWfZJP",
        "outputId": "8caf8ae2-703f-435f-838a-0f86469234f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There were 12 columns before encoding categorical features\n",
            "There are 9545 columns after encoding categorical features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_combined():\n",
        "    global combined\n",
        "    train = combined[:1460]\n",
        "    test = combined[1460:]\n",
        "\n",
        "    return train , test \n",
        "\n",
        "train, test = split_combined()"
      ],
      "metadata": {
        "id": "Hy-BsLA5gEJy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model = Sequential()"
      ],
      "metadata": {
        "id": "mzcZ9-YsgEHJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
      ],
      "metadata": {
        "id": "U-Z-CVhbgEEv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "metadata": {
        "id": "wkO08Vi0gEB3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "metadata": {
        "id": "AT3ChN3ygD7o",
        "outputId": "bef2fdf0-92c9-445f-9477-7ae3ce8ed5a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               1221888   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,386,753\n",
            "Trainable params: 1,386,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "ddXBVw5ggDzh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "hSkPdb1FhZzd",
        "outputId": "722decb9-d5e8-4243-f7cb-c8eb2e81d2a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 4748.3574 - mean_absolute_error: 4748.3574\n",
            "Epoch 1: val_loss improved from inf to 711.27234, saving model to Weights-001--711.27234.hdf5\n",
            "37/37 [==============================] - 1s 17ms/step - loss: 4249.9082 - mean_absolute_error: 4249.9082 - val_loss: 711.2723 - val_mean_absolute_error: 711.2723\n",
            "Epoch 2/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 525.9915 - mean_absolute_error: 525.9915\n",
            "Epoch 2: val_loss improved from 711.27234 to 162.85550, saving model to Weights-002--162.85550.hdf5\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 504.0348 - mean_absolute_error: 504.0348 - val_loss: 162.8555 - val_mean_absolute_error: 162.8555\n",
            "Epoch 3/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 153.1749 - mean_absolute_error: 153.1749\n",
            "Epoch 3: val_loss improved from 162.85550 to 60.42187, saving model to Weights-003--60.42187.hdf5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 147.1440 - mean_absolute_error: 147.1440 - val_loss: 60.4219 - val_mean_absolute_error: 60.4219\n",
            "Epoch 4/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 165.3888 - mean_absolute_error: 165.3888\n",
            "Epoch 4: val_loss did not improve from 60.42187\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 153.6022 - mean_absolute_error: 153.6022 - val_loss: 199.0402 - val_mean_absolute_error: 199.0402\n",
            "Epoch 5/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 96.0257 - mean_absolute_error: 96.0257\n",
            "Epoch 5: val_loss improved from 60.42187 to 8.20431, saving model to Weights-005--8.20431.hdf5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 94.5739 - mean_absolute_error: 94.5739 - val_loss: 8.2043 - val_mean_absolute_error: 8.2043\n",
            "Epoch 6/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 139.2188 - mean_absolute_error: 139.2188\n",
            "Epoch 6: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 138.5198 - mean_absolute_error: 138.5198 - val_loss: 77.3941 - val_mean_absolute_error: 77.3941\n",
            "Epoch 7/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 229.9937 - mean_absolute_error: 229.9937\n",
            "Epoch 7: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 228.9255 - mean_absolute_error: 228.9255 - val_loss: 303.5217 - val_mean_absolute_error: 303.5217\n",
            "Epoch 8/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 419.2462 - mean_absolute_error: 419.2462\n",
            "Epoch 8: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 389.4083 - mean_absolute_error: 389.4083 - val_loss: 300.2936 - val_mean_absolute_error: 300.2936\n",
            "Epoch 9/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 179.1416 - mean_absolute_error: 179.1416\n",
            "Epoch 9: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 158.9950 - mean_absolute_error: 158.9950 - val_loss: 33.5847 - val_mean_absolute_error: 33.5847\n",
            "Epoch 10/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 148.2506 - mean_absolute_error: 148.2506\n",
            "Epoch 10: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 161.6083 - mean_absolute_error: 161.6083 - val_loss: 452.4500 - val_mean_absolute_error: 452.4500\n",
            "Epoch 11/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 231.5001 - mean_absolute_error: 231.5001\n",
            "Epoch 11: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 221.0825 - mean_absolute_error: 221.0825 - val_loss: 122.6430 - val_mean_absolute_error: 122.6430\n",
            "Epoch 12/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 251.6268 - mean_absolute_error: 251.6268\n",
            "Epoch 12: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 244.5807 - mean_absolute_error: 244.5807 - val_loss: 193.5576 - val_mean_absolute_error: 193.5576\n",
            "Epoch 13/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 228.7396 - mean_absolute_error: 228.7396\n",
            "Epoch 13: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 239.3923 - mean_absolute_error: 239.3923 - val_loss: 398.6598 - val_mean_absolute_error: 398.6598\n",
            "Epoch 14/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 312.9252 - mean_absolute_error: 312.9252\n",
            "Epoch 14: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 303.7406 - mean_absolute_error: 303.7406 - val_loss: 138.1003 - val_mean_absolute_error: 138.1003\n",
            "Epoch 15/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 172.9450 - mean_absolute_error: 172.9450\n",
            "Epoch 15: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 182.2519 - mean_absolute_error: 182.2519 - val_loss: 174.0813 - val_mean_absolute_error: 174.0813\n",
            "Epoch 16/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 285.8106 - mean_absolute_error: 285.8106\n",
            "Epoch 16: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 263.4178 - mean_absolute_error: 263.4178 - val_loss: 270.9626 - val_mean_absolute_error: 270.9626\n",
            "Epoch 17/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 141.6216 - mean_absolute_error: 141.6216\n",
            "Epoch 17: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 133.2919 - mean_absolute_error: 133.2919 - val_loss: 57.9091 - val_mean_absolute_error: 57.9091\n",
            "Epoch 18/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 189.7805 - mean_absolute_error: 189.7805\n",
            "Epoch 18: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 189.1817 - mean_absolute_error: 189.1817 - val_loss: 186.8096 - val_mean_absolute_error: 186.8096\n",
            "Epoch 19/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 112.5122 - mean_absolute_error: 112.5122\n",
            "Epoch 19: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 116.3569 - mean_absolute_error: 116.3569 - val_loss: 219.8069 - val_mean_absolute_error: 219.8069\n",
            "Epoch 20/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 207.1767 - mean_absolute_error: 207.1767\n",
            "Epoch 20: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 216.9232 - mean_absolute_error: 216.9232 - val_loss: 394.6540 - val_mean_absolute_error: 394.6540\n",
            "Epoch 21/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 210.8807 - mean_absolute_error: 210.8807\n",
            "Epoch 21: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 228.8555 - mean_absolute_error: 228.8555 - val_loss: 206.4332 - val_mean_absolute_error: 206.4332\n",
            "Epoch 22/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 192.7548 - mean_absolute_error: 192.7548\n",
            "Epoch 22: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 191.4843 - mean_absolute_error: 191.4843 - val_loss: 133.2083 - val_mean_absolute_error: 133.2083\n",
            "Epoch 23/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 118.0786 - mean_absolute_error: 118.0786\n",
            "Epoch 23: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 118.5172 - mean_absolute_error: 118.5172 - val_loss: 141.2832 - val_mean_absolute_error: 141.2832\n",
            "Epoch 24/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 99.8791 - mean_absolute_error: 99.8791\n",
            "Epoch 24: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 103.7973 - mean_absolute_error: 103.7973 - val_loss: 203.9492 - val_mean_absolute_error: 203.9492\n",
            "Epoch 25/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 357.3979 - mean_absolute_error: 357.3979\n",
            "Epoch 25: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 357.0428 - mean_absolute_error: 357.0428 - val_loss: 405.4925 - val_mean_absolute_error: 405.4925\n",
            "Epoch 26/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 228.7901 - mean_absolute_error: 228.7901\n",
            "Epoch 26: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 228.0155 - mean_absolute_error: 228.0155 - val_loss: 82.9079 - val_mean_absolute_error: 82.9079\n",
            "Epoch 27/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 234.5958 - mean_absolute_error: 234.5958\n",
            "Epoch 27: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 233.1583 - mean_absolute_error: 233.1583 - val_loss: 350.6819 - val_mean_absolute_error: 350.6819\n",
            "Epoch 28/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 207.3401 - mean_absolute_error: 207.3401\n",
            "Epoch 28: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 211.8713 - mean_absolute_error: 211.8713 - val_loss: 307.6036 - val_mean_absolute_error: 307.6036\n",
            "Epoch 29/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 266.5633 - mean_absolute_error: 266.5633\n",
            "Epoch 29: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 257.9958 - mean_absolute_error: 257.9958 - val_loss: 20.8555 - val_mean_absolute_error: 20.8555\n",
            "Epoch 30/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 52.8459 - mean_absolute_error: 52.8459\n",
            "Epoch 30: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 59.2193 - mean_absolute_error: 59.2193 - val_loss: 375.2552 - val_mean_absolute_error: 375.2552\n",
            "Epoch 31/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 252.2020 - mean_absolute_error: 252.2020\n",
            "Epoch 31: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 244.8155 - mean_absolute_error: 244.8155 - val_loss: 168.9856 - val_mean_absolute_error: 168.9856\n",
            "Epoch 32/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 177.7830 - mean_absolute_error: 177.7830\n",
            "Epoch 32: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 182.9594 - mean_absolute_error: 182.9594 - val_loss: 403.1683 - val_mean_absolute_error: 403.1683\n",
            "Epoch 33/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 292.1292 - mean_absolute_error: 292.1292\n",
            "Epoch 33: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 275.8979 - mean_absolute_error: 275.8979 - val_loss: 103.2499 - val_mean_absolute_error: 103.2499\n",
            "Epoch 34/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 179.0580 - mean_absolute_error: 179.0580\n",
            "Epoch 34: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 174.3758 - mean_absolute_error: 174.3758 - val_loss: 130.8459 - val_mean_absolute_error: 130.8459\n",
            "Epoch 35/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 147.6803 - mean_absolute_error: 147.6803\n",
            "Epoch 35: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 145.7781 - mean_absolute_error: 145.7781 - val_loss: 227.3797 - val_mean_absolute_error: 227.3797\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 166.8231 - mean_absolute_error: 166.8231\n",
            "Epoch 36: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 166.8231 - mean_absolute_error: 166.8231 - val_loss: 76.1090 - val_mean_absolute_error: 76.1090\n",
            "Epoch 37/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 139.4493 - mean_absolute_error: 139.4493\n",
            "Epoch 37: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 181.7180 - mean_absolute_error: 181.7180 - val_loss: 350.9443 - val_mean_absolute_error: 350.9443\n",
            "Epoch 38/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 165.4269 - mean_absolute_error: 165.4269\n",
            "Epoch 38: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 174.8217 - mean_absolute_error: 174.8217 - val_loss: 236.7651 - val_mean_absolute_error: 236.7651\n",
            "Epoch 39/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 134.1101 - mean_absolute_error: 134.1101\n",
            "Epoch 39: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 149.7346 - mean_absolute_error: 149.7346 - val_loss: 44.7307 - val_mean_absolute_error: 44.7307\n",
            "Epoch 40/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 222.2236 - mean_absolute_error: 222.2236\n",
            "Epoch 40: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 209.0095 - mean_absolute_error: 209.0095 - val_loss: 10.5533 - val_mean_absolute_error: 10.5533\n",
            "Epoch 41/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 166.5782 - mean_absolute_error: 166.5782\n",
            "Epoch 41: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 163.9595 - mean_absolute_error: 163.9595 - val_loss: 304.8762 - val_mean_absolute_error: 304.8762\n",
            "Epoch 42/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 205.9844 - mean_absolute_error: 205.9844\n",
            "Epoch 42: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 234.8898 - mean_absolute_error: 234.8898 - val_loss: 201.4207 - val_mean_absolute_error: 201.4207\n",
            "Epoch 43/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 258.6786 - mean_absolute_error: 258.6786\n",
            "Epoch 43: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 258.6136 - mean_absolute_error: 258.6136 - val_loss: 296.7621 - val_mean_absolute_error: 296.7621\n",
            "Epoch 44/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 130.4669 - mean_absolute_error: 130.4669\n",
            "Epoch 44: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 127.5060 - mean_absolute_error: 127.5060 - val_loss: 45.3034 - val_mean_absolute_error: 45.3034\n",
            "Epoch 45/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 103.4786 - mean_absolute_error: 103.4786\n",
            "Epoch 45: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 95.6963 - mean_absolute_error: 95.6963 - val_loss: 67.7516 - val_mean_absolute_error: 67.7516\n",
            "Epoch 46/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 158.6985 - mean_absolute_error: 158.6985\n",
            "Epoch 46: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 152.9832 - mean_absolute_error: 152.9832 - val_loss: 55.7459 - val_mean_absolute_error: 55.7459\n",
            "Epoch 47/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 128.2238 - mean_absolute_error: 128.2238\n",
            "Epoch 47: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 137.0707 - mean_absolute_error: 137.0707 - val_loss: 378.3552 - val_mean_absolute_error: 378.3552\n",
            "Epoch 48/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 183.8975 - mean_absolute_error: 183.8975\n",
            "Epoch 48: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 190.8528 - mean_absolute_error: 190.8528 - val_loss: 234.5152 - val_mean_absolute_error: 234.5152\n",
            "Epoch 49/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 294.5117 - mean_absolute_error: 294.5117\n",
            "Epoch 49: val_loss did not improve from 8.20431\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 295.2933 - mean_absolute_error: 295.2933 - val_loss: 340.5641 - val_mean_absolute_error: 340.5641\n",
            "Epoch 50/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 133.3670 - mean_absolute_error: 133.3670\n",
            "Epoch 50: val_loss improved from 8.20431 to 8.15184, saving model to Weights-050--8.15184.hdf5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 131.5582 - mean_absolute_error: 131.5582 - val_loss: 8.1518 - val_mean_absolute_error: 8.1518\n",
            "Epoch 51/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 162.8353 - mean_absolute_error: 162.8353\n",
            "Epoch 51: val_loss did not improve from 8.15184\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 156.7671 - mean_absolute_error: 156.7671 - val_loss: 128.9889 - val_mean_absolute_error: 128.9889\n",
            "Epoch 52/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 106.3277 - mean_absolute_error: 106.3277\n",
            "Epoch 52: val_loss improved from 8.15184 to 1.16261, saving model to Weights-052--1.16261.hdf5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 99.9798 - mean_absolute_error: 99.9798 - val_loss: 1.1626 - val_mean_absolute_error: 1.1626\n",
            "Epoch 53/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 69.8184 - mean_absolute_error: 69.8184\n",
            "Epoch 53: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 70.2736 - mean_absolute_error: 70.2736 - val_loss: 122.1154 - val_mean_absolute_error: 122.1154\n",
            "Epoch 54/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 88.0438 - mean_absolute_error: 88.0438\n",
            "Epoch 54: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 101.9070 - mean_absolute_error: 101.9070 - val_loss: 195.8502 - val_mean_absolute_error: 195.8502\n",
            "Epoch 55/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 143.0391 - mean_absolute_error: 143.0391\n",
            "Epoch 55: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 151.2294 - mean_absolute_error: 151.2294 - val_loss: 53.8290 - val_mean_absolute_error: 53.8290\n",
            "Epoch 56/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 387.1890 - mean_absolute_error: 387.1890\n",
            "Epoch 56: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 389.8051 - mean_absolute_error: 389.8051 - val_loss: 474.7017 - val_mean_absolute_error: 474.7017\n",
            "Epoch 57/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 310.2519 - mean_absolute_error: 310.2519\n",
            "Epoch 57: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 292.1765 - mean_absolute_error: 292.1765 - val_loss: 54.1438 - val_mean_absolute_error: 54.1438\n",
            "Epoch 58/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 125.4622 - mean_absolute_error: 125.4622\n",
            "Epoch 58: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 123.9831 - mean_absolute_error: 123.9831 - val_loss: 46.2243 - val_mean_absolute_error: 46.2243\n",
            "Epoch 59/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 149.8056 - mean_absolute_error: 149.8056\n",
            "Epoch 59: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 162.4493 - mean_absolute_error: 162.4493 - val_loss: 282.5461 - val_mean_absolute_error: 282.5461\n",
            "Epoch 60/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 448.8555 - mean_absolute_error: 448.8555\n",
            "Epoch 60: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 446.2111 - mean_absolute_error: 446.2111 - val_loss: 69.1486 - val_mean_absolute_error: 69.1486\n",
            "Epoch 61/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 201.4460 - mean_absolute_error: 201.4460\n",
            "Epoch 61: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 200.6597 - mean_absolute_error: 200.6597 - val_loss: 45.4644 - val_mean_absolute_error: 45.4644\n",
            "Epoch 62/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 138.8824 - mean_absolute_error: 138.8824\n",
            "Epoch 62: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 144.0487 - mean_absolute_error: 144.0487 - val_loss: 318.1102 - val_mean_absolute_error: 318.1102\n",
            "Epoch 63/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 175.0185 - mean_absolute_error: 175.0185\n",
            "Epoch 63: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 195.0104 - mean_absolute_error: 195.0104 - val_loss: 265.9017 - val_mean_absolute_error: 265.9017\n",
            "Epoch 64/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 171.8800 - mean_absolute_error: 171.8800\n",
            "Epoch 64: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 170.1443 - mean_absolute_error: 170.1443 - val_loss: 170.2713 - val_mean_absolute_error: 170.2713\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 98.2976 - mean_absolute_error: 98.2976  \n",
            "Epoch 65: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 98.2976 - mean_absolute_error: 98.2976 - val_loss: 124.8282 - val_mean_absolute_error: 124.8282\n",
            "Epoch 66/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 124.0755 - mean_absolute_error: 124.0755\n",
            "Epoch 66: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 123.6162 - mean_absolute_error: 123.6162 - val_loss: 13.8150 - val_mean_absolute_error: 13.8150\n",
            "Epoch 67/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 86.3909 - mean_absolute_error: 86.3909\n",
            "Epoch 67: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 87.7199 - mean_absolute_error: 87.7199 - val_loss: 404.8035 - val_mean_absolute_error: 404.8035\n",
            "Epoch 68/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 242.1658 - mean_absolute_error: 242.1658\n",
            "Epoch 68: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 234.7113 - mean_absolute_error: 234.7113 - val_loss: 108.6111 - val_mean_absolute_error: 108.6111\n",
            "Epoch 69/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 262.3148 - mean_absolute_error: 262.3148\n",
            "Epoch 69: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 294.6192 - mean_absolute_error: 294.6192 - val_loss: 598.7289 - val_mean_absolute_error: 598.7289\n",
            "Epoch 70/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 257.5681 - mean_absolute_error: 257.5681\n",
            "Epoch 70: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 251.9549 - mean_absolute_error: 251.9549 - val_loss: 411.8443 - val_mean_absolute_error: 411.8443\n",
            "Epoch 71/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 311.3224 - mean_absolute_error: 311.3224\n",
            "Epoch 71: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 291.4207 - mean_absolute_error: 291.4207 - val_loss: 151.6550 - val_mean_absolute_error: 151.6550\n",
            "Epoch 72/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 237.3848 - mean_absolute_error: 237.3848\n",
            "Epoch 72: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 233.4170 - mean_absolute_error: 233.4170 - val_loss: 42.8198 - val_mean_absolute_error: 42.8198\n",
            "Epoch 73/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 121.0722 - mean_absolute_error: 121.0722\n",
            "Epoch 73: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 114.8888 - mean_absolute_error: 114.8888 - val_loss: 1.5995 - val_mean_absolute_error: 1.5995\n",
            "Epoch 74/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 257.6322 - mean_absolute_error: 257.6322\n",
            "Epoch 74: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 261.7766 - mean_absolute_error: 261.7766 - val_loss: 691.6481 - val_mean_absolute_error: 691.6481\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 428.6468 - mean_absolute_error: 428.6468\n",
            "Epoch 75: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 428.6468 - mean_absolute_error: 428.6468 - val_loss: 96.2750 - val_mean_absolute_error: 96.2750\n",
            "Epoch 76/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 259.3798 - mean_absolute_error: 259.3798\n",
            "Epoch 76: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 238.7745 - mean_absolute_error: 238.7745 - val_loss: 158.0773 - val_mean_absolute_error: 158.0773\n",
            "Epoch 77/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 174.9958 - mean_absolute_error: 174.9958\n",
            "Epoch 77: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 175.5949 - mean_absolute_error: 175.5949 - val_loss: 454.9550 - val_mean_absolute_error: 454.9550\n",
            "Epoch 78/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 152.4909 - mean_absolute_error: 152.4909\n",
            "Epoch 78: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 153.3220 - mean_absolute_error: 153.3220 - val_loss: 557.8925 - val_mean_absolute_error: 557.8925\n",
            "Epoch 79/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 212.6599 - mean_absolute_error: 212.6599\n",
            "Epoch 79: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 214.0327 - mean_absolute_error: 214.0327 - val_loss: 175.8754 - val_mean_absolute_error: 175.8754\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 188.4680 - mean_absolute_error: 188.4680\n",
            "Epoch 80: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 188.4680 - mean_absolute_error: 188.4680 - val_loss: 165.1028 - val_mean_absolute_error: 165.1028\n",
            "Epoch 81/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 189.1027 - mean_absolute_error: 189.1027\n",
            "Epoch 81: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 193.8025 - mean_absolute_error: 193.8025 - val_loss: 79.7895 - val_mean_absolute_error: 79.7895\n",
            "Epoch 82/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 77.0322 - mean_absolute_error: 77.0322\n",
            "Epoch 82: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.1124 - mean_absolute_error: 75.1124 - val_loss: 100.4233 - val_mean_absolute_error: 100.4233\n",
            "Epoch 83/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 201.7659 - mean_absolute_error: 201.7659\n",
            "Epoch 83: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 205.5343 - mean_absolute_error: 205.5343 - val_loss: 210.5698 - val_mean_absolute_error: 210.5698\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 197.6096 - mean_absolute_error: 197.6096\n",
            "Epoch 84: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 197.6096 - mean_absolute_error: 197.6096 - val_loss: 290.3068 - val_mean_absolute_error: 290.3068\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 141.7961 - mean_absolute_error: 141.7961\n",
            "Epoch 85: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 141.7961 - mean_absolute_error: 141.7961 - val_loss: 85.3029 - val_mean_absolute_error: 85.3029\n",
            "Epoch 86/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 206.6212 - mean_absolute_error: 206.6212\n",
            "Epoch 86: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 201.8210 - mean_absolute_error: 201.8210 - val_loss: 4.5251 - val_mean_absolute_error: 4.5251\n",
            "Epoch 87/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 151.5757 - mean_absolute_error: 151.5757\n",
            "Epoch 87: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 142.2024 - mean_absolute_error: 142.2024 - val_loss: 258.1795 - val_mean_absolute_error: 258.1795\n",
            "Epoch 88/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 167.5394 - mean_absolute_error: 167.5394\n",
            "Epoch 88: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 161.0816 - mean_absolute_error: 161.0816 - val_loss: 104.6025 - val_mean_absolute_error: 104.6025\n",
            "Epoch 89/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 130.3201 - mean_absolute_error: 130.3201\n",
            "Epoch 89: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 129.2283 - mean_absolute_error: 129.2283 - val_loss: 23.2041 - val_mean_absolute_error: 23.2041\n",
            "Epoch 90/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 129.2083 - mean_absolute_error: 129.2083\n",
            "Epoch 90: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 156.0417 - mean_absolute_error: 156.0417 - val_loss: 136.0447 - val_mean_absolute_error: 136.0447\n",
            "Epoch 91/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 156.1766 - mean_absolute_error: 156.1766\n",
            "Epoch 91: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 158.5117 - mean_absolute_error: 158.5117 - val_loss: 49.4114 - val_mean_absolute_error: 49.4114\n",
            "Epoch 92/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 114.1481 - mean_absolute_error: 114.1481\n",
            "Epoch 92: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 111.5667 - mean_absolute_error: 111.5667 - val_loss: 81.2825 - val_mean_absolute_error: 81.2825\n",
            "Epoch 93/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 131.9568 - mean_absolute_error: 131.9568\n",
            "Epoch 93: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 123.5125 - mean_absolute_error: 123.5125 - val_loss: 7.3367 - val_mean_absolute_error: 7.3367\n",
            "Epoch 94/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 89.2936 - mean_absolute_error: 89.2936\n",
            "Epoch 94: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 98.7598 - mean_absolute_error: 98.7598 - val_loss: 251.3127 - val_mean_absolute_error: 251.3127\n",
            "Epoch 95/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 148.0499 - mean_absolute_error: 148.0499\n",
            "Epoch 95: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 139.1253 - mean_absolute_error: 139.1253 - val_loss: 61.0912 - val_mean_absolute_error: 61.0912\n",
            "Epoch 96/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 90.0970 - mean_absolute_error: 90.0970\n",
            "Epoch 96: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 93.5111 - mean_absolute_error: 93.5111 - val_loss: 58.6554 - val_mean_absolute_error: 58.6554\n",
            "Epoch 97/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 244.4336 - mean_absolute_error: 244.4336\n",
            "Epoch 97: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 245.3207 - mean_absolute_error: 245.3207 - val_loss: 311.1970 - val_mean_absolute_error: 311.1970\n",
            "Epoch 98/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 212.8546 - mean_absolute_error: 212.8546\n",
            "Epoch 98: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 213.0474 - mean_absolute_error: 213.0474 - val_loss: 98.5359 - val_mean_absolute_error: 98.5359\n",
            "Epoch 99/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 135.2089 - mean_absolute_error: 135.2089\n",
            "Epoch 99: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 133.7154 - mean_absolute_error: 133.7154 - val_loss: 46.9092 - val_mean_absolute_error: 46.9092\n",
            "Epoch 100/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 234.1942 - mean_absolute_error: 234.1942\n",
            "Epoch 100: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 219.7989 - mean_absolute_error: 219.7989 - val_loss: 272.9922 - val_mean_absolute_error: 272.9922\n",
            "Epoch 101/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 235.2428 - mean_absolute_error: 235.2428\n",
            "Epoch 101: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 205.0805 - mean_absolute_error: 205.0805 - val_loss: 69.8631 - val_mean_absolute_error: 69.8631\n",
            "Epoch 102/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 192.2733 - mean_absolute_error: 192.2733\n",
            "Epoch 102: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 218.2753 - mean_absolute_error: 218.2753 - val_loss: 244.1967 - val_mean_absolute_error: 244.1967\n",
            "Epoch 103/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 242.5310 - mean_absolute_error: 242.5310\n",
            "Epoch 103: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 235.4405 - mean_absolute_error: 235.4405 - val_loss: 270.8321 - val_mean_absolute_error: 270.8321\n",
            "Epoch 104/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 181.1153 - mean_absolute_error: 181.1153\n",
            "Epoch 104: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 173.9506 - mean_absolute_error: 173.9506 - val_loss: 113.6727 - val_mean_absolute_error: 113.6727\n",
            "Epoch 105/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 126.9999 - mean_absolute_error: 126.9999\n",
            "Epoch 105: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 126.6340 - mean_absolute_error: 126.6340 - val_loss: 148.4837 - val_mean_absolute_error: 148.4837\n",
            "Epoch 106/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 197.3942 - mean_absolute_error: 197.3942\n",
            "Epoch 106: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 196.5499 - mean_absolute_error: 196.5499 - val_loss: 205.4029 - val_mean_absolute_error: 205.4029\n",
            "Epoch 107/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 257.0063 - mean_absolute_error: 257.0063\n",
            "Epoch 107: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 238.4073 - mean_absolute_error: 238.4073 - val_loss: 4.8097 - val_mean_absolute_error: 4.8097\n",
            "Epoch 108/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 117.2927 - mean_absolute_error: 117.2927\n",
            "Epoch 108: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 121.8162 - mean_absolute_error: 121.8162 - val_loss: 385.7874 - val_mean_absolute_error: 385.7874\n",
            "Epoch 109/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 213.1437 - mean_absolute_error: 213.1437\n",
            "Epoch 109: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 206.1466 - mean_absolute_error: 206.1466 - val_loss: 186.6623 - val_mean_absolute_error: 186.6623\n",
            "Epoch 110/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 136.7276 - mean_absolute_error: 136.7276\n",
            "Epoch 110: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 134.9844 - mean_absolute_error: 134.9844 - val_loss: 56.8934 - val_mean_absolute_error: 56.8934\n",
            "Epoch 111/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 241.2715 - mean_absolute_error: 241.2715\n",
            "Epoch 111: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 243.8608 - mean_absolute_error: 243.8608 - val_loss: 176.0732 - val_mean_absolute_error: 176.0732\n",
            "Epoch 112/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 117.2217 - mean_absolute_error: 117.2217\n",
            "Epoch 112: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 118.9797 - mean_absolute_error: 118.9797 - val_loss: 63.8210 - val_mean_absolute_error: 63.8210\n",
            "Epoch 113/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 67.4889 - mean_absolute_error: 67.4889\n",
            "Epoch 113: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 67.5903 - mean_absolute_error: 67.5903 - val_loss: 336.1570 - val_mean_absolute_error: 336.1570\n",
            "Epoch 114/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 126.4861 - mean_absolute_error: 126.4861\n",
            "Epoch 114: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 127.7021 - mean_absolute_error: 127.7021 - val_loss: 5.5249 - val_mean_absolute_error: 5.5249\n",
            "Epoch 115/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 115.7744 - mean_absolute_error: 115.7744\n",
            "Epoch 115: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 124.5495 - mean_absolute_error: 124.5495 - val_loss: 268.1989 - val_mean_absolute_error: 268.1989\n",
            "Epoch 116/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 109.8982 - mean_absolute_error: 109.8982\n",
            "Epoch 116: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 103.9221 - mean_absolute_error: 103.9221 - val_loss: 65.1312 - val_mean_absolute_error: 65.1312\n",
            "Epoch 117/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 88.1286 - mean_absolute_error: 88.1286\n",
            "Epoch 117: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 86.3413 - mean_absolute_error: 86.3413 - val_loss: 83.9061 - val_mean_absolute_error: 83.9061\n",
            "Epoch 118/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 100.9438 - mean_absolute_error: 100.9438\n",
            "Epoch 118: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 98.9146 - mean_absolute_error: 98.9146 - val_loss: 56.7570 - val_mean_absolute_error: 56.7570\n",
            "Epoch 119/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 115.2868 - mean_absolute_error: 115.2868\n",
            "Epoch 119: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 113.0941 - mean_absolute_error: 113.0941 - val_loss: 305.0956 - val_mean_absolute_error: 305.0956\n",
            "Epoch 120/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 257.0239 - mean_absolute_error: 257.0239\n",
            "Epoch 120: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 254.3591 - mean_absolute_error: 254.3591 - val_loss: 157.1100 - val_mean_absolute_error: 157.1100\n",
            "Epoch 121/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 105.2906 - mean_absolute_error: 105.2906\n",
            "Epoch 121: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 107.4492 - mean_absolute_error: 107.4492 - val_loss: 251.6647 - val_mean_absolute_error: 251.6647\n",
            "Epoch 122/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 108.1138 - mean_absolute_error: 108.1138\n",
            "Epoch 122: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 101.6968 - mean_absolute_error: 101.6968 - val_loss: 127.7551 - val_mean_absolute_error: 127.7551\n",
            "Epoch 123/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 146.6265 - mean_absolute_error: 146.6265\n",
            "Epoch 123: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 159.3609 - mean_absolute_error: 159.3609 - val_loss: 156.8519 - val_mean_absolute_error: 156.8519\n",
            "Epoch 124/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 153.3344 - mean_absolute_error: 153.3344\n",
            "Epoch 124: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 137.8572 - mean_absolute_error: 137.8572 - val_loss: 98.4178 - val_mean_absolute_error: 98.4178\n",
            "Epoch 125/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 64.1117 - mean_absolute_error: 64.1117\n",
            "Epoch 125: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 73.8630 - mean_absolute_error: 73.8630 - val_loss: 177.7251 - val_mean_absolute_error: 177.7251\n",
            "Epoch 126/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 193.5933 - mean_absolute_error: 193.5933\n",
            "Epoch 126: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 197.0515 - mean_absolute_error: 197.0515 - val_loss: 101.1977 - val_mean_absolute_error: 101.1977\n",
            "Epoch 127/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 120.8598 - mean_absolute_error: 120.8598\n",
            "Epoch 127: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 117.3038 - mean_absolute_error: 117.3038 - val_loss: 32.1806 - val_mean_absolute_error: 32.1806\n",
            "Epoch 128/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 121.2961 - mean_absolute_error: 121.2961\n",
            "Epoch 128: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 112.8530 - mean_absolute_error: 112.8530 - val_loss: 33.1436 - val_mean_absolute_error: 33.1436\n",
            "Epoch 129/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 93.3030 - mean_absolute_error: 93.3030\n",
            "Epoch 129: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 89.3738 - mean_absolute_error: 89.3738 - val_loss: 145.8915 - val_mean_absolute_error: 145.8915\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 93.3398 - mean_absolute_error: 93.3398  \n",
            "Epoch 130: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 93.3398 - mean_absolute_error: 93.3398 - val_loss: 21.9849 - val_mean_absolute_error: 21.9849\n",
            "Epoch 131/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 52.7317 - mean_absolute_error: 52.7317\n",
            "Epoch 131: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 53.8513 - mean_absolute_error: 53.8513 - val_loss: 45.5345 - val_mean_absolute_error: 45.5345\n",
            "Epoch 132/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 126.7073 - mean_absolute_error: 126.7073\n",
            "Epoch 132: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 121.3662 - mean_absolute_error: 121.3662 - val_loss: 195.9402 - val_mean_absolute_error: 195.9402\n",
            "Epoch 133/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 86.9777 - mean_absolute_error: 86.9777\n",
            "Epoch 133: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 91.7297 - mean_absolute_error: 91.7297 - val_loss: 160.7683 - val_mean_absolute_error: 160.7683\n",
            "Epoch 134/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 233.6817 - mean_absolute_error: 233.6817\n",
            "Epoch 134: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 235.7256 - mean_absolute_error: 235.7256 - val_loss: 117.1587 - val_mean_absolute_error: 117.1587\n",
            "Epoch 135/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 204.4308 - mean_absolute_error: 204.4308\n",
            "Epoch 135: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 205.4884 - mean_absolute_error: 205.4884 - val_loss: 95.1970 - val_mean_absolute_error: 95.1970\n",
            "Epoch 136/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 160.6509 - mean_absolute_error: 160.6509\n",
            "Epoch 136: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 159.6372 - mean_absolute_error: 159.6372 - val_loss: 36.0525 - val_mean_absolute_error: 36.0525\n",
            "Epoch 137/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 143.1066 - mean_absolute_error: 143.1066\n",
            "Epoch 137: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 144.7789 - mean_absolute_error: 144.7789 - val_loss: 29.5378 - val_mean_absolute_error: 29.5378\n",
            "Epoch 138/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 158.6508 - mean_absolute_error: 158.6508\n",
            "Epoch 138: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 156.9392 - mean_absolute_error: 156.9392 - val_loss: 34.9365 - val_mean_absolute_error: 34.9365\n",
            "Epoch 139/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 127.5570 - mean_absolute_error: 127.5570\n",
            "Epoch 139: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 128.2771 - mean_absolute_error: 128.2771 - val_loss: 139.8969 - val_mean_absolute_error: 139.8969\n",
            "Epoch 140/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 167.0589 - mean_absolute_error: 167.0589\n",
            "Epoch 140: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 174.7467 - mean_absolute_error: 174.7467 - val_loss: 214.1993 - val_mean_absolute_error: 214.1993\n",
            "Epoch 141/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 151.3243 - mean_absolute_error: 151.3243\n",
            "Epoch 141: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 140.4225 - mean_absolute_error: 140.4225 - val_loss: 66.2822 - val_mean_absolute_error: 66.2822\n",
            "Epoch 142/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 135.9493 - mean_absolute_error: 135.9493\n",
            "Epoch 142: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 134.9004 - mean_absolute_error: 134.9004 - val_loss: 77.5171 - val_mean_absolute_error: 77.5171\n",
            "Epoch 143/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 120.7292 - mean_absolute_error: 120.7292\n",
            "Epoch 143: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 125.4375 - mean_absolute_error: 125.4375 - val_loss: 405.5262 - val_mean_absolute_error: 405.5262\n",
            "Epoch 144/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 212.0856 - mean_absolute_error: 212.0856\n",
            "Epoch 144: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 205.0654 - mean_absolute_error: 205.0654 - val_loss: 169.2641 - val_mean_absolute_error: 169.2641\n",
            "Epoch 145/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 147.0173 - mean_absolute_error: 147.0173\n",
            "Epoch 145: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 152.8237 - mean_absolute_error: 152.8237 - val_loss: 152.3702 - val_mean_absolute_error: 152.3702\n",
            "Epoch 146/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 108.5957 - mean_absolute_error: 108.5957\n",
            "Epoch 146: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 115.4795 - mean_absolute_error: 115.4795 - val_loss: 479.4178 - val_mean_absolute_error: 479.4178\n",
            "Epoch 147/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 125.6885 - mean_absolute_error: 125.6885\n",
            "Epoch 147: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 129.3996 - mean_absolute_error: 129.3996 - val_loss: 70.8705 - val_mean_absolute_error: 70.8705\n",
            "Epoch 148/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 111.2799 - mean_absolute_error: 111.2799\n",
            "Epoch 148: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 107.6647 - mean_absolute_error: 107.6647 - val_loss: 97.6233 - val_mean_absolute_error: 97.6233\n",
            "Epoch 149/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 80.2583 - mean_absolute_error: 80.2583\n",
            "Epoch 149: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 80.7516 - mean_absolute_error: 80.7516 - val_loss: 154.0452 - val_mean_absolute_error: 154.0452\n",
            "Epoch 150/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 94.4952 - mean_absolute_error: 94.4952\n",
            "Epoch 150: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 93.3932 - mean_absolute_error: 93.3932 - val_loss: 30.5537 - val_mean_absolute_error: 30.5537\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 224.1115 - mean_absolute_error: 224.1115\n",
            "Epoch 151: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 224.1115 - mean_absolute_error: 224.1115 - val_loss: 66.7500 - val_mean_absolute_error: 66.7500\n",
            "Epoch 152/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 147.7723 - mean_absolute_error: 147.7723\n",
            "Epoch 152: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 151.0084 - mean_absolute_error: 151.0084 - val_loss: 365.8478 - val_mean_absolute_error: 365.8478\n",
            "Epoch 153/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 200.9102 - mean_absolute_error: 200.9102\n",
            "Epoch 153: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 213.4748 - mean_absolute_error: 213.4748 - val_loss: 263.0586 - val_mean_absolute_error: 263.0586\n",
            "Epoch 154/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 120.6742 - mean_absolute_error: 120.6742\n",
            "Epoch 154: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 120.7718 - mean_absolute_error: 120.7718 - val_loss: 88.0392 - val_mean_absolute_error: 88.0392\n",
            "Epoch 155/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 84.2868 - mean_absolute_error: 84.2868\n",
            "Epoch 155: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 90.6492 - mean_absolute_error: 90.6492 - val_loss: 3.1749 - val_mean_absolute_error: 3.1749\n",
            "Epoch 156/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 62.7784 - mean_absolute_error: 62.7784\n",
            "Epoch 156: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 64.2257 - mean_absolute_error: 64.2257 - val_loss: 22.5497 - val_mean_absolute_error: 22.5497\n",
            "Epoch 157/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 78.0491 - mean_absolute_error: 78.0491\n",
            "Epoch 157: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 80.5501 - mean_absolute_error: 80.5501 - val_loss: 109.8181 - val_mean_absolute_error: 109.8181\n",
            "Epoch 158/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 92.8504 - mean_absolute_error: 92.8504  \n",
            "Epoch 158: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 94.3812 - mean_absolute_error: 94.3812 - val_loss: 44.5875 - val_mean_absolute_error: 44.5875\n",
            "Epoch 159/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 209.1308 - mean_absolute_error: 209.1308\n",
            "Epoch 159: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 209.3070 - mean_absolute_error: 209.3070 - val_loss: 104.9335 - val_mean_absolute_error: 104.9335\n",
            "Epoch 160/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 185.0347 - mean_absolute_error: 185.0347\n",
            "Epoch 160: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 185.4356 - mean_absolute_error: 185.4356 - val_loss: 86.3094 - val_mean_absolute_error: 86.3094\n",
            "Epoch 161/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 137.7636 - mean_absolute_error: 137.7636\n",
            "Epoch 161: val_loss did not improve from 1.16261\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 143.7733 - mean_absolute_error: 143.7733 - val_loss: 126.1436 - val_mean_absolute_error: 126.1436\n",
            "Epoch 162/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 94.6038 - mean_absolute_error: 94.6038  \n",
            "Epoch 162: val_loss improved from 1.16261 to 0.94090, saving model to Weights-162--0.94090.hdf5\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 92.2630 - mean_absolute_error: 92.2630 - val_loss: 0.9409 - val_mean_absolute_error: 0.9409\n",
            "Epoch 163/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 133.5806 - mean_absolute_error: 133.5806\n",
            "Epoch 163: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 128.4187 - mean_absolute_error: 128.4187 - val_loss: 116.4538 - val_mean_absolute_error: 116.4538\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 92.1322 - mean_absolute_error: 92.1322\n",
            "Epoch 164: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 92.1322 - mean_absolute_error: 92.1322 - val_loss: 32.4790 - val_mean_absolute_error: 32.4790\n",
            "Epoch 165/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 55.8477 - mean_absolute_error: 55.8477\n",
            "Epoch 165: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 57.8246 - mean_absolute_error: 57.8246 - val_loss: 167.3923 - val_mean_absolute_error: 167.3923\n",
            "Epoch 166/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 163.4926 - mean_absolute_error: 163.4926\n",
            "Epoch 166: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 158.8588 - mean_absolute_error: 158.8588 - val_loss: 162.3833 - val_mean_absolute_error: 162.3833\n",
            "Epoch 167/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 152.3087 - mean_absolute_error: 152.3087\n",
            "Epoch 167: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 152.2897 - mean_absolute_error: 152.2897 - val_loss: 247.1164 - val_mean_absolute_error: 247.1164\n",
            "Epoch 168/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 188.9915 - mean_absolute_error: 188.9915\n",
            "Epoch 168: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 190.3056 - mean_absolute_error: 190.3056 - val_loss: 93.5136 - val_mean_absolute_error: 93.5136\n",
            "Epoch 169/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 91.4983 - mean_absolute_error: 91.4983\n",
            "Epoch 169: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 85.9529 - mean_absolute_error: 85.9529 - val_loss: 37.7440 - val_mean_absolute_error: 37.7440\n",
            "Epoch 170/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 114.6806 - mean_absolute_error: 114.6806\n",
            "Epoch 170: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 121.4552 - mean_absolute_error: 121.4552 - val_loss: 170.2297 - val_mean_absolute_error: 170.2297\n",
            "Epoch 171/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 186.2126 - mean_absolute_error: 186.2126\n",
            "Epoch 171: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 188.8014 - mean_absolute_error: 188.8014 - val_loss: 74.0567 - val_mean_absolute_error: 74.0567\n",
            "Epoch 172/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 152.9455 - mean_absolute_error: 152.9455\n",
            "Epoch 172: val_loss did not improve from 0.94090\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 147.2212 - mean_absolute_error: 147.2212 - val_loss: 44.9404 - val_mean_absolute_error: 44.9404\n",
            "Epoch 173/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 55.6862 - mean_absolute_error: 55.6862\n",
            "Epoch 173: val_loss improved from 0.94090 to 0.92161, saving model to Weights-173--0.92161.hdf5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 53.6724 - mean_absolute_error: 53.6724 - val_loss: 0.9216 - val_mean_absolute_error: 0.9216\n",
            "Epoch 174/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 98.3939 - mean_absolute_error: 98.3939  \n",
            "Epoch 174: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 104.7946 - mean_absolute_error: 104.7946 - val_loss: 184.6341 - val_mean_absolute_error: 184.6341\n",
            "Epoch 175/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 206.8472 - mean_absolute_error: 206.8472\n",
            "Epoch 175: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 203.0106 - mean_absolute_error: 203.0106 - val_loss: 127.3299 - val_mean_absolute_error: 127.3299\n",
            "Epoch 176/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 180.2477 - mean_absolute_error: 180.2477\n",
            "Epoch 176: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 173.8099 - mean_absolute_error: 173.8099 - val_loss: 177.9105 - val_mean_absolute_error: 177.9105\n",
            "Epoch 177/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 202.6366 - mean_absolute_error: 202.6366\n",
            "Epoch 177: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 199.6400 - mean_absolute_error: 199.6400 - val_loss: 248.1004 - val_mean_absolute_error: 248.1004\n",
            "Epoch 178/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 233.3289 - mean_absolute_error: 233.3289\n",
            "Epoch 178: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 218.7766 - mean_absolute_error: 218.7766 - val_loss: 38.0709 - val_mean_absolute_error: 38.0709\n",
            "Epoch 179/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 145.2894 - mean_absolute_error: 145.2894\n",
            "Epoch 179: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 171.5294 - mean_absolute_error: 171.5294 - val_loss: 264.6771 - val_mean_absolute_error: 264.6771\n",
            "Epoch 180/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 115.0663 - mean_absolute_error: 115.0663\n",
            "Epoch 180: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 111.0564 - mean_absolute_error: 111.0564 - val_loss: 90.0135 - val_mean_absolute_error: 90.0135\n",
            "Epoch 181/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 120.7325 - mean_absolute_error: 120.7325\n",
            "Epoch 181: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 141.4731 - mean_absolute_error: 141.4731 - val_loss: 157.7730 - val_mean_absolute_error: 157.7730\n",
            "Epoch 182/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 112.1217 - mean_absolute_error: 112.1217\n",
            "Epoch 182: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 120.2705 - mean_absolute_error: 120.2705 - val_loss: 129.0067 - val_mean_absolute_error: 129.0067\n",
            "Epoch 183/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 141.9383 - mean_absolute_error: 141.9383\n",
            "Epoch 183: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 147.5684 - mean_absolute_error: 147.5684 - val_loss: 98.8403 - val_mean_absolute_error: 98.8403\n",
            "Epoch 184/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 76.5326 - mean_absolute_error: 76.5326\n",
            "Epoch 184: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.6312 - mean_absolute_error: 75.6312 - val_loss: 5.7026 - val_mean_absolute_error: 5.7026\n",
            "Epoch 185/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 105.7380 - mean_absolute_error: 105.7380\n",
            "Epoch 185: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 103.1047 - mean_absolute_error: 103.1047 - val_loss: 4.4621 - val_mean_absolute_error: 4.4621\n",
            "Epoch 186/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 76.6008 - mean_absolute_error: 76.6008\n",
            "Epoch 186: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 75.4144 - mean_absolute_error: 75.4144 - val_loss: 211.7869 - val_mean_absolute_error: 211.7869\n",
            "Epoch 187/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 203.1291 - mean_absolute_error: 203.1291\n",
            "Epoch 187: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 213.3931 - mean_absolute_error: 213.3931 - val_loss: 324.6824 - val_mean_absolute_error: 324.6824\n",
            "Epoch 188/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 289.7015 - mean_absolute_error: 289.7015\n",
            "Epoch 188: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 279.5042 - mean_absolute_error: 279.5042 - val_loss: 102.6762 - val_mean_absolute_error: 102.6762\n",
            "Epoch 189/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 64.6873 - mean_absolute_error: 64.6873\n",
            "Epoch 189: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 66.2990 - mean_absolute_error: 66.2990 - val_loss: 274.2626 - val_mean_absolute_error: 274.2626\n",
            "Epoch 190/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 124.0181 - mean_absolute_error: 124.0181\n",
            "Epoch 190: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 121.6603 - mean_absolute_error: 121.6603 - val_loss: 175.3832 - val_mean_absolute_error: 175.3832\n",
            "Epoch 191/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 112.5704 - mean_absolute_error: 112.5704\n",
            "Epoch 191: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 104.3457 - mean_absolute_error: 104.3457 - val_loss: 82.8316 - val_mean_absolute_error: 82.8316\n",
            "Epoch 192/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 97.0652 - mean_absolute_error: 97.0652\n",
            "Epoch 192: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 93.0202 - mean_absolute_error: 93.0202 - val_loss: 95.3564 - val_mean_absolute_error: 95.3564\n",
            "Epoch 193/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 83.2447 - mean_absolute_error: 83.2447\n",
            "Epoch 193: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 83.0398 - mean_absolute_error: 83.0398 - val_loss: 222.7928 - val_mean_absolute_error: 222.7928\n",
            "Epoch 194/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 123.4611 - mean_absolute_error: 123.4611\n",
            "Epoch 194: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 116.6801 - mean_absolute_error: 116.6801 - val_loss: 60.2629 - val_mean_absolute_error: 60.2629\n",
            "Epoch 195/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 57.0815 - mean_absolute_error: 57.0815\n",
            "Epoch 195: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 55.3666 - mean_absolute_error: 55.3666 - val_loss: 70.9718 - val_mean_absolute_error: 70.9718\n",
            "Epoch 196/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 85.1174 - mean_absolute_error: 85.1174\n",
            "Epoch 196: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 82.1636 - mean_absolute_error: 82.1636 - val_loss: 120.1956 - val_mean_absolute_error: 120.1956\n",
            "Epoch 197/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 139.3820 - mean_absolute_error: 139.3820\n",
            "Epoch 197: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 138.1983 - mean_absolute_error: 138.1983 - val_loss: 92.2465 - val_mean_absolute_error: 92.2465\n",
            "Epoch 198/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 61.0401 - mean_absolute_error: 61.0401\n",
            "Epoch 198: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 65.3078 - mean_absolute_error: 65.3078 - val_loss: 49.3104 - val_mean_absolute_error: 49.3104\n",
            "Epoch 199/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 83.6581 - mean_absolute_error: 83.6581\n",
            "Epoch 199: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 82.3981 - mean_absolute_error: 82.3981 - val_loss: 151.8813 - val_mean_absolute_error: 151.8813\n",
            "Epoch 200/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 129.7315 - mean_absolute_error: 129.7315\n",
            "Epoch 200: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 133.2197 - mean_absolute_error: 133.2197 - val_loss: 312.4716 - val_mean_absolute_error: 312.4716\n",
            "Epoch 201/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 154.9795 - mean_absolute_error: 154.9795\n",
            "Epoch 201: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 146.7728 - mean_absolute_error: 146.7728 - val_loss: 135.2514 - val_mean_absolute_error: 135.2514\n",
            "Epoch 202/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 96.2623 - mean_absolute_error: 96.2623\n",
            "Epoch 202: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 93.3469 - mean_absolute_error: 93.3469 - val_loss: 234.4767 - val_mean_absolute_error: 234.4767\n",
            "Epoch 203/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 279.6536 - mean_absolute_error: 279.6536\n",
            "Epoch 203: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 283.6128 - mean_absolute_error: 283.6128 - val_loss: 184.7779 - val_mean_absolute_error: 184.7779\n",
            "Epoch 204/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 191.7037 - mean_absolute_error: 191.7037\n",
            "Epoch 204: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 189.5183 - mean_absolute_error: 189.5183 - val_loss: 224.9052 - val_mean_absolute_error: 224.9052\n",
            "Epoch 205/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 257.6143 - mean_absolute_error: 257.6143\n",
            "Epoch 205: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 256.8528 - mean_absolute_error: 256.8528 - val_loss: 302.0983 - val_mean_absolute_error: 302.0983\n",
            "Epoch 206/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 180.3889 - mean_absolute_error: 180.3889\n",
            "Epoch 206: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 181.9889 - mean_absolute_error: 181.9889 - val_loss: 84.5163 - val_mean_absolute_error: 84.5163\n",
            "Epoch 207/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 135.1855 - mean_absolute_error: 135.1855\n",
            "Epoch 207: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 142.5453 - mean_absolute_error: 142.5453 - val_loss: 412.2409 - val_mean_absolute_error: 412.2409\n",
            "Epoch 208/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 132.9608 - mean_absolute_error: 132.9608\n",
            "Epoch 208: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 131.7417 - mean_absolute_error: 131.7417 - val_loss: 17.2729 - val_mean_absolute_error: 17.2729\n",
            "Epoch 209/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 102.9531 - mean_absolute_error: 102.9531\n",
            "Epoch 209: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 102.3829 - mean_absolute_error: 102.3829 - val_loss: 36.8325 - val_mean_absolute_error: 36.8325\n",
            "Epoch 210/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 56.3662 - mean_absolute_error: 56.3662\n",
            "Epoch 210: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 54.7387 - mean_absolute_error: 54.7387 - val_loss: 69.6241 - val_mean_absolute_error: 69.6241\n",
            "Epoch 211/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 98.7365 - mean_absolute_error: 98.7365\n",
            "Epoch 211: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 100.5161 - mean_absolute_error: 100.5161 - val_loss: 33.9096 - val_mean_absolute_error: 33.9096\n",
            "Epoch 212/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 64.7629 - mean_absolute_error: 64.7629\n",
            "Epoch 212: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 66.1225 - mean_absolute_error: 66.1225 - val_loss: 194.8921 - val_mean_absolute_error: 194.8921\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 66.0995 - mean_absolute_error: 66.0995\n",
            "Epoch 213: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 66.0995 - mean_absolute_error: 66.0995 - val_loss: 5.9125 - val_mean_absolute_error: 5.9125\n",
            "Epoch 214/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 94.7418 - mean_absolute_error: 94.7418\n",
            "Epoch 214: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 93.1915 - mean_absolute_error: 93.1915 - val_loss: 44.5258 - val_mean_absolute_error: 44.5258\n",
            "Epoch 215/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 75.2601 - mean_absolute_error: 75.2601\n",
            "Epoch 215: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 76.5321 - mean_absolute_error: 76.5321 - val_loss: 98.9395 - val_mean_absolute_error: 98.9395\n",
            "Epoch 216/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 200.1551 - mean_absolute_error: 200.1551\n",
            "Epoch 216: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 197.7385 - mean_absolute_error: 197.7385 - val_loss: 4.3677 - val_mean_absolute_error: 4.3677\n",
            "Epoch 217/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 72.9788 - mean_absolute_error: 72.9788\n",
            "Epoch 217: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 80.5419 - mean_absolute_error: 80.5419 - val_loss: 1.4275 - val_mean_absolute_error: 1.4275\n",
            "Epoch 218/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 94.7474 - mean_absolute_error: 94.7474  \n",
            "Epoch 218: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 96.6530 - mean_absolute_error: 96.6530 - val_loss: 207.1338 - val_mean_absolute_error: 207.1338\n",
            "Epoch 219/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 116.5969 - mean_absolute_error: 116.5969\n",
            "Epoch 219: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 110.5788 - mean_absolute_error: 110.5788 - val_loss: 14.2099 - val_mean_absolute_error: 14.2099\n",
            "Epoch 220/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 59.4965 - mean_absolute_error: 59.4965\n",
            "Epoch 220: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 57.1100 - mean_absolute_error: 57.1100 - val_loss: 4.3450 - val_mean_absolute_error: 4.3450\n",
            "Epoch 221/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 87.9160 - mean_absolute_error: 87.9160\n",
            "Epoch 221: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 91.6596 - mean_absolute_error: 91.6596 - val_loss: 164.8055 - val_mean_absolute_error: 164.8055\n",
            "Epoch 222/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 149.3783 - mean_absolute_error: 149.3783\n",
            "Epoch 222: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 157.7045 - mean_absolute_error: 157.7045 - val_loss: 172.7398 - val_mean_absolute_error: 172.7398\n",
            "Epoch 223/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 92.2426 - mean_absolute_error: 92.2426  \n",
            "Epoch 223: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 89.6563 - mean_absolute_error: 89.6563 - val_loss: 40.2742 - val_mean_absolute_error: 40.2742\n",
            "Epoch 224/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 188.0701 - mean_absolute_error: 188.0701\n",
            "Epoch 224: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 181.9293 - mean_absolute_error: 181.9293 - val_loss: 66.0829 - val_mean_absolute_error: 66.0829\n",
            "Epoch 225/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 82.6004 - mean_absolute_error: 82.6004\n",
            "Epoch 225: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 77.7194 - mean_absolute_error: 77.7194 - val_loss: 21.8475 - val_mean_absolute_error: 21.8475\n",
            "Epoch 226/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 165.0967 - mean_absolute_error: 165.0967\n",
            "Epoch 226: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 175.9739 - mean_absolute_error: 175.9739 - val_loss: 400.9458 - val_mean_absolute_error: 400.9458\n",
            "Epoch 227/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 207.7170 - mean_absolute_error: 207.7170\n",
            "Epoch 227: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 194.9725 - mean_absolute_error: 194.9725 - val_loss: 98.6873 - val_mean_absolute_error: 98.6873\n",
            "Epoch 228/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 107.2720 - mean_absolute_error: 107.2720\n",
            "Epoch 228: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 98.7652 - mean_absolute_error: 98.7652 - val_loss: 138.2783 - val_mean_absolute_error: 138.2783\n",
            "Epoch 229/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 45.0951 - mean_absolute_error: 45.0951\n",
            "Epoch 229: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 44.8905 - mean_absolute_error: 44.8905 - val_loss: 38.6962 - val_mean_absolute_error: 38.6962\n",
            "Epoch 230/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 213.2032 - mean_absolute_error: 213.2032\n",
            "Epoch 230: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 218.3558 - mean_absolute_error: 218.3558 - val_loss: 43.3697 - val_mean_absolute_error: 43.3697\n",
            "Epoch 231/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 130.5303 - mean_absolute_error: 130.5303\n",
            "Epoch 231: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 130.4257 - mean_absolute_error: 130.4257 - val_loss: 41.2205 - val_mean_absolute_error: 41.2205\n",
            "Epoch 232/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 64.7070 - mean_absolute_error: 64.7070\n",
            "Epoch 232: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 62.4904 - mean_absolute_error: 62.4904 - val_loss: 43.5429 - val_mean_absolute_error: 43.5429\n",
            "Epoch 233/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 107.6054 - mean_absolute_error: 107.6054\n",
            "Epoch 233: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 105.9738 - mean_absolute_error: 105.9738 - val_loss: 97.2884 - val_mean_absolute_error: 97.2884\n",
            "Epoch 234/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 196.0593 - mean_absolute_error: 196.0593\n",
            "Epoch 234: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 195.5803 - mean_absolute_error: 195.5803 - val_loss: 216.6617 - val_mean_absolute_error: 216.6617\n",
            "Epoch 235/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 135.9881 - mean_absolute_error: 135.9881\n",
            "Epoch 235: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 151.9384 - mean_absolute_error: 151.9384 - val_loss: 372.5178 - val_mean_absolute_error: 372.5178\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 159.5067 - mean_absolute_error: 159.5067\n",
            "Epoch 236: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 159.5067 - mean_absolute_error: 159.5067 - val_loss: 33.8765 - val_mean_absolute_error: 33.8765\n",
            "Epoch 237/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 113.1217 - mean_absolute_error: 113.1217\n",
            "Epoch 237: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 112.1364 - mean_absolute_error: 112.1364 - val_loss: 83.6955 - val_mean_absolute_error: 83.6955\n",
            "Epoch 238/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 50.1233 - mean_absolute_error: 50.1233\n",
            "Epoch 238: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 53.7449 - mean_absolute_error: 53.7449 - val_loss: 69.4250 - val_mean_absolute_error: 69.4250\n",
            "Epoch 239/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 132.4744 - mean_absolute_error: 132.4744\n",
            "Epoch 239: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 145.9290 - mean_absolute_error: 145.9290 - val_loss: 274.5393 - val_mean_absolute_error: 274.5393\n",
            "Epoch 240/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 160.8355 - mean_absolute_error: 160.8355\n",
            "Epoch 240: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 157.6770 - mean_absolute_error: 157.6770 - val_loss: 172.9377 - val_mean_absolute_error: 172.9377\n",
            "Epoch 241/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 135.0634 - mean_absolute_error: 135.0634\n",
            "Epoch 241: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 132.1250 - mean_absolute_error: 132.1250 - val_loss: 81.3719 - val_mean_absolute_error: 81.3719\n",
            "Epoch 242/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 144.4021 - mean_absolute_error: 144.4021\n",
            "Epoch 242: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 144.8471 - mean_absolute_error: 144.8471 - val_loss: 40.7852 - val_mean_absolute_error: 40.7852\n",
            "Epoch 243/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 191.3480 - mean_absolute_error: 191.3480\n",
            "Epoch 243: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 189.6290 - mean_absolute_error: 189.6290 - val_loss: 256.8434 - val_mean_absolute_error: 256.8434\n",
            "Epoch 244/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 243.4006 - mean_absolute_error: 243.4006\n",
            "Epoch 244: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 229.5361 - mean_absolute_error: 229.5361 - val_loss: 67.0850 - val_mean_absolute_error: 67.0850\n",
            "Epoch 245/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 62.5407 - mean_absolute_error: 62.5407\n",
            "Epoch 245: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 62.3182 - mean_absolute_error: 62.3182 - val_loss: 79.6414 - val_mean_absolute_error: 79.6414\n",
            "Epoch 246/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 129.4681 - mean_absolute_error: 129.4681\n",
            "Epoch 246: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 136.8285 - mean_absolute_error: 136.8285 - val_loss: 248.4821 - val_mean_absolute_error: 248.4821\n",
            "Epoch 247/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 141.8109 - mean_absolute_error: 141.8109\n",
            "Epoch 247: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 139.4437 - mean_absolute_error: 139.4437 - val_loss: 77.3252 - val_mean_absolute_error: 77.3252\n",
            "Epoch 248/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 156.7791 - mean_absolute_error: 156.7791\n",
            "Epoch 248: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 155.1934 - mean_absolute_error: 155.1934 - val_loss: 160.2870 - val_mean_absolute_error: 160.2870\n",
            "Epoch 249/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 73.7062 - mean_absolute_error: 73.7062\n",
            "Epoch 249: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 72.3097 - mean_absolute_error: 72.3097 - val_loss: 3.6307 - val_mean_absolute_error: 3.6307\n",
            "Epoch 250/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 108.6620 - mean_absolute_error: 108.6620\n",
            "Epoch 250: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 112.1263 - mean_absolute_error: 112.1263 - val_loss: 27.9935 - val_mean_absolute_error: 27.9935\n",
            "Epoch 251/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 174.9493 - mean_absolute_error: 174.9493\n",
            "Epoch 251: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 172.7627 - mean_absolute_error: 172.7627 - val_loss: 162.0696 - val_mean_absolute_error: 162.0696\n",
            "Epoch 252/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 144.1636 - mean_absolute_error: 144.1636\n",
            "Epoch 252: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 142.1366 - mean_absolute_error: 142.1366 - val_loss: 217.6556 - val_mean_absolute_error: 217.6556\n",
            "Epoch 253/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 186.1508 - mean_absolute_error: 186.1508\n",
            "Epoch 253: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 186.3631 - mean_absolute_error: 186.3631 - val_loss: 45.6617 - val_mean_absolute_error: 45.6617\n",
            "Epoch 254/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 150.6367 - mean_absolute_error: 150.6367\n",
            "Epoch 254: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 137.9574 - mean_absolute_error: 137.9574 - val_loss: 11.9576 - val_mean_absolute_error: 11.9576\n",
            "Epoch 255/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 48.5513 - mean_absolute_error: 48.5513\n",
            "Epoch 255: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 47.1977 - mean_absolute_error: 47.1977 - val_loss: 27.2683 - val_mean_absolute_error: 27.2683\n",
            "Epoch 256/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 173.9796 - mean_absolute_error: 173.9796\n",
            "Epoch 256: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 170.0408 - mean_absolute_error: 170.0408 - val_loss: 171.3914 - val_mean_absolute_error: 171.3914\n",
            "Epoch 257/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 139.6414 - mean_absolute_error: 139.6414\n",
            "Epoch 257: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 137.3578 - mean_absolute_error: 137.3578 - val_loss: 236.0315 - val_mean_absolute_error: 236.0315\n",
            "Epoch 258/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 173.3801 - mean_absolute_error: 173.3801\n",
            "Epoch 258: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 167.0046 - mean_absolute_error: 167.0046 - val_loss: 33.6000 - val_mean_absolute_error: 33.6000\n",
            "Epoch 259/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 97.0611 - mean_absolute_error: 97.0611\n",
            "Epoch 259: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 96.2728 - mean_absolute_error: 96.2728 - val_loss: 13.5530 - val_mean_absolute_error: 13.5530\n",
            "Epoch 260/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 80.0184 - mean_absolute_error: 80.0184\n",
            "Epoch 260: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.7836 - mean_absolute_error: 75.7836 - val_loss: 98.7845 - val_mean_absolute_error: 98.7845\n",
            "Epoch 261/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 206.8351 - mean_absolute_error: 206.8351\n",
            "Epoch 261: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 203.6633 - mean_absolute_error: 203.6633 - val_loss: 242.0532 - val_mean_absolute_error: 242.0532\n",
            "Epoch 262/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 121.5266 - mean_absolute_error: 121.5266\n",
            "Epoch 262: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 119.5826 - mean_absolute_error: 119.5826 - val_loss: 103.1008 - val_mean_absolute_error: 103.1008\n",
            "Epoch 263/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 143.6746 - mean_absolute_error: 143.6746\n",
            "Epoch 263: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 151.6122 - mean_absolute_error: 151.6122 - val_loss: 205.2022 - val_mean_absolute_error: 205.2022\n",
            "Epoch 264/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 72.9861 - mean_absolute_error: 72.9861\n",
            "Epoch 264: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 73.9480 - mean_absolute_error: 73.9480 - val_loss: 80.6819 - val_mean_absolute_error: 80.6819\n",
            "Epoch 265/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 77.6908 - mean_absolute_error: 77.6908\n",
            "Epoch 265: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 80.3648 - mean_absolute_error: 80.3648 - val_loss: 117.7023 - val_mean_absolute_error: 117.7023\n",
            "Epoch 266/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 109.2129 - mean_absolute_error: 109.2129\n",
            "Epoch 266: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 104.1156 - mean_absolute_error: 104.1156 - val_loss: 50.6154 - val_mean_absolute_error: 50.6154\n",
            "Epoch 267/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 59.2958 - mean_absolute_error: 59.2958\n",
            "Epoch 267: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 64.1969 - mean_absolute_error: 64.1969 - val_loss: 167.2387 - val_mean_absolute_error: 167.2387\n",
            "Epoch 268/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 90.2324 - mean_absolute_error: 90.2324\n",
            "Epoch 268: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 98.1300 - mean_absolute_error: 98.1300 - val_loss: 188.5790 - val_mean_absolute_error: 188.5790\n",
            "Epoch 269/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 96.9509 - mean_absolute_error: 96.9509  \n",
            "Epoch 269: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 104.9928 - mean_absolute_error: 104.9928 - val_loss: 62.2945 - val_mean_absolute_error: 62.2945\n",
            "Epoch 270/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 121.6332 - mean_absolute_error: 121.6332\n",
            "Epoch 270: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 127.3622 - mean_absolute_error: 127.3622 - val_loss: 366.5907 - val_mean_absolute_error: 366.5907\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 119.1960 - mean_absolute_error: 119.1960\n",
            "Epoch 271: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 119.1960 - mean_absolute_error: 119.1960 - val_loss: 132.6073 - val_mean_absolute_error: 132.6073\n",
            "Epoch 272/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 164.8016 - mean_absolute_error: 164.8016\n",
            "Epoch 272: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 171.2397 - mean_absolute_error: 171.2397 - val_loss: 43.9558 - val_mean_absolute_error: 43.9558\n",
            "Epoch 273/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 123.4213 - mean_absolute_error: 123.4213\n",
            "Epoch 273: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 125.0479 - mean_absolute_error: 125.0479 - val_loss: 64.2965 - val_mean_absolute_error: 64.2965\n",
            "Epoch 274/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 149.9185 - mean_absolute_error: 149.9185\n",
            "Epoch 274: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 16ms/step - loss: 150.3677 - mean_absolute_error: 150.3677 - val_loss: 10.8481 - val_mean_absolute_error: 10.8481\n",
            "Epoch 275/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 104.9583 - mean_absolute_error: 104.9583\n",
            "Epoch 275: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 108.7973 - mean_absolute_error: 108.7973 - val_loss: 47.5846 - val_mean_absolute_error: 47.5846\n",
            "Epoch 276/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 133.7719 - mean_absolute_error: 133.7719\n",
            "Epoch 276: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 132.9470 - mean_absolute_error: 132.9470 - val_loss: 120.0597 - val_mean_absolute_error: 120.0597\n",
            "Epoch 277/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 65.7525 - mean_absolute_error: 65.7525\n",
            "Epoch 277: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 65.4567 - mean_absolute_error: 65.4567 - val_loss: 101.4887 - val_mean_absolute_error: 101.4887\n",
            "Epoch 278/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 77.9965 - mean_absolute_error: 77.9965\n",
            "Epoch 278: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 80.2699 - mean_absolute_error: 80.2699 - val_loss: 213.8110 - val_mean_absolute_error: 213.8110\n",
            "Epoch 279/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 182.9554 - mean_absolute_error: 182.9554\n",
            "Epoch 279: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 17ms/step - loss: 185.6620 - mean_absolute_error: 185.6620 - val_loss: 360.2778 - val_mean_absolute_error: 360.2778\n",
            "Epoch 280/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 151.3133 - mean_absolute_error: 151.3133\n",
            "Epoch 280: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 151.2589 - mean_absolute_error: 151.2589 - val_loss: 131.0466 - val_mean_absolute_error: 131.0466\n",
            "Epoch 281/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 84.9144 - mean_absolute_error: 84.9144\n",
            "Epoch 281: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 22ms/step - loss: 84.8704 - mean_absolute_error: 84.8704 - val_loss: 69.4236 - val_mean_absolute_error: 69.4236\n",
            "Epoch 282/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 155.1288 - mean_absolute_error: 155.1288\n",
            "Epoch 282: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 156.1496 - mean_absolute_error: 156.1496 - val_loss: 52.1220 - val_mean_absolute_error: 52.1220\n",
            "Epoch 283/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 89.8200 - mean_absolute_error: 89.8200\n",
            "Epoch 283: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 90.3129 - mean_absolute_error: 90.3129 - val_loss: 123.2525 - val_mean_absolute_error: 123.2525\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 102.8560 - mean_absolute_error: 102.8560\n",
            "Epoch 284: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 17ms/step - loss: 102.8560 - mean_absolute_error: 102.8560 - val_loss: 166.4283 - val_mean_absolute_error: 166.4283\n",
            "Epoch 285/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 84.3022 - mean_absolute_error: 84.3022\n",
            "Epoch 285: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 21ms/step - loss: 83.5887 - mean_absolute_error: 83.5887 - val_loss: 19.6304 - val_mean_absolute_error: 19.6304\n",
            "Epoch 286/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 73.7626 - mean_absolute_error: 73.7626\n",
            "Epoch 286: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 18ms/step - loss: 73.1341 - mean_absolute_error: 73.1341 - val_loss: 6.7669 - val_mean_absolute_error: 6.7669\n",
            "Epoch 287/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 64.0166 - mean_absolute_error: 64.0166\n",
            "Epoch 287: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 64.7684 - mean_absolute_error: 64.7684 - val_loss: 42.7697 - val_mean_absolute_error: 42.7697\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 102.5514 - mean_absolute_error: 102.5514\n",
            "Epoch 288: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 102.5514 - mean_absolute_error: 102.5514 - val_loss: 136.4100 - val_mean_absolute_error: 136.4100\n",
            "Epoch 289/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 104.9959 - mean_absolute_error: 104.9959\n",
            "Epoch 289: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 1s 16ms/step - loss: 104.4219 - mean_absolute_error: 104.4219 - val_loss: 145.0836 - val_mean_absolute_error: 145.0836\n",
            "Epoch 290/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 70.5380 - mean_absolute_error: 70.5380\n",
            "Epoch 290: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 72.8657 - mean_absolute_error: 72.8657 - val_loss: 134.7526 - val_mean_absolute_error: 134.7526\n",
            "Epoch 291/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 94.5351 - mean_absolute_error: 94.5351\n",
            "Epoch 291: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 100.4622 - mean_absolute_error: 100.4622 - val_loss: 71.7816 - val_mean_absolute_error: 71.7816\n",
            "Epoch 292/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 69.3750 - mean_absolute_error: 69.3750\n",
            "Epoch 292: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 70.0796 - mean_absolute_error: 70.0796 - val_loss: 10.8291 - val_mean_absolute_error: 10.8291\n",
            "Epoch 293/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 87.2866 - mean_absolute_error: 87.2866\n",
            "Epoch 293: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 85.6493 - mean_absolute_error: 85.6493 - val_loss: 52.9278 - val_mean_absolute_error: 52.9278\n",
            "Epoch 294/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 98.2624 - mean_absolute_error: 98.2624\n",
            "Epoch 294: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 107.2304 - mean_absolute_error: 107.2304 - val_loss: 181.9802 - val_mean_absolute_error: 181.9802\n",
            "Epoch 295/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 142.6996 - mean_absolute_error: 142.6996\n",
            "Epoch 295: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 132.5485 - mean_absolute_error: 132.5485 - val_loss: 93.6710 - val_mean_absolute_error: 93.6710\n",
            "Epoch 296/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 85.0905 - mean_absolute_error: 85.0905\n",
            "Epoch 296: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 90.6455 - mean_absolute_error: 90.6455 - val_loss: 114.2869 - val_mean_absolute_error: 114.2869\n",
            "Epoch 297/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 104.9986 - mean_absolute_error: 104.9986\n",
            "Epoch 297: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 112.6913 - mean_absolute_error: 112.6913 - val_loss: 77.7831 - val_mean_absolute_error: 77.7831\n",
            "Epoch 298/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 102.3674 - mean_absolute_error: 102.3674\n",
            "Epoch 298: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 101.9828 - mean_absolute_error: 101.9828 - val_loss: 317.8275 - val_mean_absolute_error: 317.8275\n",
            "Epoch 299/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 190.6526 - mean_absolute_error: 190.6526\n",
            "Epoch 299: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 184.2820 - mean_absolute_error: 184.2820 - val_loss: 23.3456 - val_mean_absolute_error: 23.3456\n",
            "Epoch 300/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 175.4392 - mean_absolute_error: 175.4392\n",
            "Epoch 300: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 164.9867 - mean_absolute_error: 164.9867 - val_loss: 155.1438 - val_mean_absolute_error: 155.1438\n",
            "Epoch 301/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 146.9041 - mean_absolute_error: 146.9041\n",
            "Epoch 301: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 144.1571 - mean_absolute_error: 144.1571 - val_loss: 230.6368 - val_mean_absolute_error: 230.6368\n",
            "Epoch 302/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 133.4765 - mean_absolute_error: 133.4765\n",
            "Epoch 302: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 127.0779 - mean_absolute_error: 127.0779 - val_loss: 119.3591 - val_mean_absolute_error: 119.3591\n",
            "Epoch 303/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 144.3165 - mean_absolute_error: 144.3165\n",
            "Epoch 303: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 138.6886 - mean_absolute_error: 138.6886 - val_loss: 41.3953 - val_mean_absolute_error: 41.3953\n",
            "Epoch 304/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 100.7553 - mean_absolute_error: 100.7553\n",
            "Epoch 304: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 97.2112 - mean_absolute_error: 97.2112 - val_loss: 170.1222 - val_mean_absolute_error: 170.1222\n",
            "Epoch 305/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 109.4578 - mean_absolute_error: 109.4578\n",
            "Epoch 305: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 102.1653 - mean_absolute_error: 102.1653 - val_loss: 41.0289 - val_mean_absolute_error: 41.0289\n",
            "Epoch 306/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 98.0658 - mean_absolute_error: 98.0658\n",
            "Epoch 306: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 100.0033 - mean_absolute_error: 100.0033 - val_loss: 173.9995 - val_mean_absolute_error: 173.9995\n",
            "Epoch 307/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 121.0791 - mean_absolute_error: 121.0791\n",
            "Epoch 307: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 116.9694 - mean_absolute_error: 116.9694 - val_loss: 169.7734 - val_mean_absolute_error: 169.7734\n",
            "Epoch 308/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 102.9082 - mean_absolute_error: 102.9082\n",
            "Epoch 308: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 102.7091 - mean_absolute_error: 102.7091 - val_loss: 27.5908 - val_mean_absolute_error: 27.5908\n",
            "Epoch 309/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 92.8837 - mean_absolute_error: 92.8837\n",
            "Epoch 309: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 92.8753 - mean_absolute_error: 92.8753 - val_loss: 190.2923 - val_mean_absolute_error: 190.2923\n",
            "Epoch 310/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 189.4705 - mean_absolute_error: 189.4705\n",
            "Epoch 310: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 188.6706 - mean_absolute_error: 188.6706 - val_loss: 186.2944 - val_mean_absolute_error: 186.2944\n",
            "Epoch 311/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 90.5749 - mean_absolute_error: 90.5749  \n",
            "Epoch 311: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 87.3258 - mean_absolute_error: 87.3258 - val_loss: 50.1714 - val_mean_absolute_error: 50.1714\n",
            "Epoch 312/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 190.8918 - mean_absolute_error: 190.8918\n",
            "Epoch 312: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 194.7085 - mean_absolute_error: 194.7085 - val_loss: 160.9687 - val_mean_absolute_error: 160.9687\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 79.2219 - mean_absolute_error: 79.2219\n",
            "Epoch 313: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 79.2219 - mean_absolute_error: 79.2219 - val_loss: 20.6772 - val_mean_absolute_error: 20.6772\n",
            "Epoch 314/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 80.1040 - mean_absolute_error: 80.1040\n",
            "Epoch 314: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.3488 - mean_absolute_error: 75.3488 - val_loss: 4.1195 - val_mean_absolute_error: 4.1195\n",
            "Epoch 315/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 89.6927 - mean_absolute_error: 89.6927  \n",
            "Epoch 315: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 84.9842 - mean_absolute_error: 84.9842 - val_loss: 63.1040 - val_mean_absolute_error: 63.1040\n",
            "Epoch 316/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 145.8558 - mean_absolute_error: 145.8558\n",
            "Epoch 316: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 140.5425 - mean_absolute_error: 140.5425 - val_loss: 9.9312 - val_mean_absolute_error: 9.9312\n",
            "Epoch 317/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 105.8827 - mean_absolute_error: 105.8827\n",
            "Epoch 317: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 105.3992 - mean_absolute_error: 105.3992 - val_loss: 144.8134 - val_mean_absolute_error: 144.8134\n",
            "Epoch 318/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 94.0975 - mean_absolute_error: 94.0975\n",
            "Epoch 318: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 93.3584 - mean_absolute_error: 93.3584 - val_loss: 355.1269 - val_mean_absolute_error: 355.1269\n",
            "Epoch 319/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 114.8911 - mean_absolute_error: 114.8911\n",
            "Epoch 319: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 113.3206 - mean_absolute_error: 113.3206 - val_loss: 86.7946 - val_mean_absolute_error: 86.7946\n",
            "Epoch 320/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 162.0085 - mean_absolute_error: 162.0085\n",
            "Epoch 320: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 153.4117 - mean_absolute_error: 153.4117 - val_loss: 2.2024 - val_mean_absolute_error: 2.2024\n",
            "Epoch 321/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 139.1474 - mean_absolute_error: 139.1474\n",
            "Epoch 321: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 139.6876 - mean_absolute_error: 139.6876 - val_loss: 134.2767 - val_mean_absolute_error: 134.2767\n",
            "Epoch 322/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 85.5970 - mean_absolute_error: 85.5970\n",
            "Epoch 322: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 86.4359 - mean_absolute_error: 86.4359 - val_loss: 35.6809 - val_mean_absolute_error: 35.6809\n",
            "Epoch 323/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 64.7446 - mean_absolute_error: 64.7446\n",
            "Epoch 323: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 63.4264 - mean_absolute_error: 63.4264 - val_loss: 19.2100 - val_mean_absolute_error: 19.2100\n",
            "Epoch 324/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 105.9644 - mean_absolute_error: 105.9644\n",
            "Epoch 324: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 104.7442 - mean_absolute_error: 104.7442 - val_loss: 142.5641 - val_mean_absolute_error: 142.5641\n",
            "Epoch 325/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 99.6195 - mean_absolute_error: 99.6195  \n",
            "Epoch 325: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 94.0767 - mean_absolute_error: 94.0767 - val_loss: 6.3819 - val_mean_absolute_error: 6.3819\n",
            "Epoch 326/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 125.3011 - mean_absolute_error: 125.3011\n",
            "Epoch 326: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 133.4986 - mean_absolute_error: 133.4986 - val_loss: 227.3691 - val_mean_absolute_error: 227.3691\n",
            "Epoch 327/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 243.4457 - mean_absolute_error: 243.4457\n",
            "Epoch 327: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 236.5742 - mean_absolute_error: 236.5742 - val_loss: 105.2845 - val_mean_absolute_error: 105.2845\n",
            "Epoch 328/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 150.1962 - mean_absolute_error: 150.1962\n",
            "Epoch 328: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 152.7055 - mean_absolute_error: 152.7055 - val_loss: 368.5242 - val_mean_absolute_error: 368.5242\n",
            "Epoch 329/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 137.5682 - mean_absolute_error: 137.5682\n",
            "Epoch 329: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 149.3034 - mean_absolute_error: 149.3034 - val_loss: 58.4269 - val_mean_absolute_error: 58.4269\n",
            "Epoch 330/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 122.4046 - mean_absolute_error: 122.4046\n",
            "Epoch 330: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 123.0114 - mean_absolute_error: 123.0114 - val_loss: 37.5631 - val_mean_absolute_error: 37.5631\n",
            "Epoch 331/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 141.2220 - mean_absolute_error: 141.2220\n",
            "Epoch 331: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 132.2891 - mean_absolute_error: 132.2891 - val_loss: 34.7648 - val_mean_absolute_error: 34.7648\n",
            "Epoch 332/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 51.1428 - mean_absolute_error: 51.1428\n",
            "Epoch 332: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 50.5686 - mean_absolute_error: 50.5686 - val_loss: 82.7728 - val_mean_absolute_error: 82.7728\n",
            "Epoch 333/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 66.8292 - mean_absolute_error: 66.8292\n",
            "Epoch 333: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 71.9335 - mean_absolute_error: 71.9335 - val_loss: 270.8759 - val_mean_absolute_error: 270.8759\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 80.6576 - mean_absolute_error: 80.6576\n",
            "Epoch 334: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 80.6576 - mean_absolute_error: 80.6576 - val_loss: 187.7827 - val_mean_absolute_error: 187.7827\n",
            "Epoch 335/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 138.4550 - mean_absolute_error: 138.4550\n",
            "Epoch 335: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 139.6599 - mean_absolute_error: 139.6599 - val_loss: 306.3421 - val_mean_absolute_error: 306.3421\n",
            "Epoch 336/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 84.1580 - mean_absolute_error: 84.1580\n",
            "Epoch 336: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 86.1286 - mean_absolute_error: 86.1286 - val_loss: 2.9282 - val_mean_absolute_error: 2.9282\n",
            "Epoch 337/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 173.5356 - mean_absolute_error: 173.5356\n",
            "Epoch 337: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 172.1978 - mean_absolute_error: 172.1978 - val_loss: 228.2019 - val_mean_absolute_error: 228.2019\n",
            "Epoch 338/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 101.7046 - mean_absolute_error: 101.7046\n",
            "Epoch 338: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 101.5885 - mean_absolute_error: 101.5885 - val_loss: 75.8038 - val_mean_absolute_error: 75.8038\n",
            "Epoch 339/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 121.2121 - mean_absolute_error: 121.2121\n",
            "Epoch 339: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 123.0468 - mean_absolute_error: 123.0468 - val_loss: 14.6768 - val_mean_absolute_error: 14.6768\n",
            "Epoch 340/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 112.0459 - mean_absolute_error: 112.0459\n",
            "Epoch 340: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 111.3356 - mean_absolute_error: 111.3356 - val_loss: 125.4970 - val_mean_absolute_error: 125.4970\n",
            "Epoch 341/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 101.9459 - mean_absolute_error: 101.9459\n",
            "Epoch 341: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 114.2195 - mean_absolute_error: 114.2195 - val_loss: 195.9300 - val_mean_absolute_error: 195.9300\n",
            "Epoch 342/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 281.4324 - mean_absolute_error: 281.4324\n",
            "Epoch 342: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 270.2139 - mean_absolute_error: 270.2139 - val_loss: 282.3417 - val_mean_absolute_error: 282.3417\n",
            "Epoch 343/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 98.1342 - mean_absolute_error: 98.1342  \n",
            "Epoch 343: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 96.1206 - mean_absolute_error: 96.1206 - val_loss: 95.4858 - val_mean_absolute_error: 95.4858\n",
            "Epoch 344/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 88.8943 - mean_absolute_error: 88.8943\n",
            "Epoch 344: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 88.0615 - mean_absolute_error: 88.0615 - val_loss: 77.9541 - val_mean_absolute_error: 77.9541\n",
            "Epoch 345/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 103.1497 - mean_absolute_error: 103.1497\n",
            "Epoch 345: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 104.2387 - mean_absolute_error: 104.2387 - val_loss: 87.5619 - val_mean_absolute_error: 87.5619\n",
            "Epoch 346/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 126.2837 - mean_absolute_error: 126.2837\n",
            "Epoch 346: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 121.2857 - mean_absolute_error: 121.2857 - val_loss: 23.4213 - val_mean_absolute_error: 23.4213\n",
            "Epoch 347/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 151.8641 - mean_absolute_error: 151.8641\n",
            "Epoch 347: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 153.4448 - mean_absolute_error: 153.4448 - val_loss: 320.4184 - val_mean_absolute_error: 320.4184\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 144.4039 - mean_absolute_error: 144.4039\n",
            "Epoch 348: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 144.4039 - mean_absolute_error: 144.4039 - val_loss: 34.0384 - val_mean_absolute_error: 34.0384\n",
            "Epoch 349/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 53.2400 - mean_absolute_error: 53.2400\n",
            "Epoch 349: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 68.9217 - mean_absolute_error: 68.9217 - val_loss: 56.0701 - val_mean_absolute_error: 56.0701\n",
            "Epoch 350/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 134.3629 - mean_absolute_error: 134.3629\n",
            "Epoch 350: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 131.5977 - mean_absolute_error: 131.5977 - val_loss: 14.7967 - val_mean_absolute_error: 14.7967\n",
            "Epoch 351/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 108.5792 - mean_absolute_error: 108.5792\n",
            "Epoch 351: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 109.8300 - mean_absolute_error: 109.8300 - val_loss: 313.7770 - val_mean_absolute_error: 313.7770\n",
            "Epoch 352/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 99.3090 - mean_absolute_error: 99.3090  \n",
            "Epoch 352: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 95.0364 - mean_absolute_error: 95.0364 - val_loss: 186.7020 - val_mean_absolute_error: 186.7020\n",
            "Epoch 353/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 111.1781 - mean_absolute_error: 111.1781\n",
            "Epoch 353: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 107.1657 - mean_absolute_error: 107.1657 - val_loss: 52.6953 - val_mean_absolute_error: 52.6953\n",
            "Epoch 354/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 68.2067 - mean_absolute_error: 68.2067\n",
            "Epoch 354: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 69.2729 - mean_absolute_error: 69.2729 - val_loss: 41.4930 - val_mean_absolute_error: 41.4930\n",
            "Epoch 355/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 72.5220 - mean_absolute_error: 72.5220\n",
            "Epoch 355: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 72.7075 - mean_absolute_error: 72.7075 - val_loss: 30.8337 - val_mean_absolute_error: 30.8337\n",
            "Epoch 356/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 137.5933 - mean_absolute_error: 137.5933\n",
            "Epoch 356: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 135.7738 - mean_absolute_error: 135.7738 - val_loss: 141.6096 - val_mean_absolute_error: 141.6096\n",
            "Epoch 357/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 95.3460 - mean_absolute_error: 95.3460  \n",
            "Epoch 357: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 92.0115 - mean_absolute_error: 92.0115 - val_loss: 3.9443 - val_mean_absolute_error: 3.9443\n",
            "Epoch 358/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 113.1606 - mean_absolute_error: 113.1606\n",
            "Epoch 358: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 111.1991 - mean_absolute_error: 111.1991 - val_loss: 81.7341 - val_mean_absolute_error: 81.7341\n",
            "Epoch 359/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 130.2789 - mean_absolute_error: 130.2789\n",
            "Epoch 359: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 137.4620 - mean_absolute_error: 137.4620 - val_loss: 378.1130 - val_mean_absolute_error: 378.1130\n",
            "Epoch 360/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 150.4779 - mean_absolute_error: 150.4779\n",
            "Epoch 360: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 146.2570 - mean_absolute_error: 146.2570 - val_loss: 107.5777 - val_mean_absolute_error: 107.5777\n",
            "Epoch 361/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 154.6078 - mean_absolute_error: 154.6078\n",
            "Epoch 361: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 145.5980 - mean_absolute_error: 145.5980 - val_loss: 116.7096 - val_mean_absolute_error: 116.7096\n",
            "Epoch 362/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 56.7159 - mean_absolute_error: 56.7159\n",
            "Epoch 362: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 66.5488 - mean_absolute_error: 66.5488 - val_loss: 101.4807 - val_mean_absolute_error: 101.4807\n",
            "Epoch 363/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 74.5941 - mean_absolute_error: 74.5941\n",
            "Epoch 363: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 72.9746 - mean_absolute_error: 72.9746 - val_loss: 15.3443 - val_mean_absolute_error: 15.3443\n",
            "Epoch 364/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 60.3437 - mean_absolute_error: 60.3437\n",
            "Epoch 364: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 59.9366 - mean_absolute_error: 59.9366 - val_loss: 150.3145 - val_mean_absolute_error: 150.3145\n",
            "Epoch 365/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 94.3051 - mean_absolute_error: 94.3051\n",
            "Epoch 365: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 94.6150 - mean_absolute_error: 94.6150 - val_loss: 119.7218 - val_mean_absolute_error: 119.7218\n",
            "Epoch 366/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 86.4278 - mean_absolute_error: 86.4278\n",
            "Epoch 366: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 88.3065 - mean_absolute_error: 88.3065 - val_loss: 10.3797 - val_mean_absolute_error: 10.3797\n",
            "Epoch 367/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 86.7225 - mean_absolute_error: 86.7225\n",
            "Epoch 367: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 92.6021 - mean_absolute_error: 92.6021 - val_loss: 113.0719 - val_mean_absolute_error: 113.0719\n",
            "Epoch 368/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 101.5378 - mean_absolute_error: 101.5378\n",
            "Epoch 368: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 99.4916 - mean_absolute_error: 99.4916 - val_loss: 118.0151 - val_mean_absolute_error: 118.0151\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 83.7555 - mean_absolute_error: 83.7555\n",
            "Epoch 369: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 83.7555 - mean_absolute_error: 83.7555 - val_loss: 373.7811 - val_mean_absolute_error: 373.7811\n",
            "Epoch 370/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 205.0542 - mean_absolute_error: 205.0542\n",
            "Epoch 370: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 188.7124 - mean_absolute_error: 188.7124 - val_loss: 104.6731 - val_mean_absolute_error: 104.6731\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 78.4128 - mean_absolute_error: 78.4128\n",
            "Epoch 371: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 78.4128 - mean_absolute_error: 78.4128 - val_loss: 125.6196 - val_mean_absolute_error: 125.6196\n",
            "Epoch 372/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 96.0208 - mean_absolute_error: 96.0208  \n",
            "Epoch 372: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 89.0180 - mean_absolute_error: 89.0180 - val_loss: 40.6828 - val_mean_absolute_error: 40.6828\n",
            "Epoch 373/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 40.3638 - mean_absolute_error: 40.3638\n",
            "Epoch 373: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 42.2798 - mean_absolute_error: 42.2798 - val_loss: 131.7657 - val_mean_absolute_error: 131.7657\n",
            "Epoch 374/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 118.6218 - mean_absolute_error: 118.6218\n",
            "Epoch 374: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 119.3000 - mean_absolute_error: 119.3000 - val_loss: 97.8642 - val_mean_absolute_error: 97.8642\n",
            "Epoch 375/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 67.4834 - mean_absolute_error: 67.4834\n",
            "Epoch 375: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 62.2361 - mean_absolute_error: 62.2361 - val_loss: 20.3331 - val_mean_absolute_error: 20.3331\n",
            "Epoch 376/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 99.9201 - mean_absolute_error: 99.9201\n",
            "Epoch 376: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 98.3258 - mean_absolute_error: 98.3258 - val_loss: 13.1630 - val_mean_absolute_error: 13.1630\n",
            "Epoch 377/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 108.9196 - mean_absolute_error: 108.9196\n",
            "Epoch 377: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 105.0676 - mean_absolute_error: 105.0676 - val_loss: 144.9054 - val_mean_absolute_error: 144.9054\n",
            "Epoch 378/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 80.9102 - mean_absolute_error: 80.9102\n",
            "Epoch 378: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 83.5076 - mean_absolute_error: 83.5076 - val_loss: 66.5295 - val_mean_absolute_error: 66.5295\n",
            "Epoch 379/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 134.9912 - mean_absolute_error: 134.9912\n",
            "Epoch 379: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 143.4568 - mean_absolute_error: 143.4568 - val_loss: 421.6461 - val_mean_absolute_error: 421.6461\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 216.3148 - mean_absolute_error: 216.3148\n",
            "Epoch 380: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 216.3148 - mean_absolute_error: 216.3148 - val_loss: 29.4730 - val_mean_absolute_error: 29.4730\n",
            "Epoch 381/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 205.7014 - mean_absolute_error: 205.7014\n",
            "Epoch 381: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 203.9354 - mean_absolute_error: 203.9354 - val_loss: 299.0452 - val_mean_absolute_error: 299.0452\n",
            "Epoch 382/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 150.9861 - mean_absolute_error: 150.9861\n",
            "Epoch 382: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 148.0327 - mean_absolute_error: 148.0327 - val_loss: 73.6041 - val_mean_absolute_error: 73.6041\n",
            "Epoch 383/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 185.6512 - mean_absolute_error: 185.6512\n",
            "Epoch 383: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 181.1875 - mean_absolute_error: 181.1875 - val_loss: 261.6295 - val_mean_absolute_error: 261.6295\n",
            "Epoch 384/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 93.9712 - mean_absolute_error: 93.9712\n",
            "Epoch 384: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 90.4064 - mean_absolute_error: 90.4064 - val_loss: 209.6129 - val_mean_absolute_error: 209.6129\n",
            "Epoch 385/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 78.7430 - mean_absolute_error: 78.7430\n",
            "Epoch 385: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 74.0284 - mean_absolute_error: 74.0284 - val_loss: 24.2535 - val_mean_absolute_error: 24.2535\n",
            "Epoch 386/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 78.0853 - mean_absolute_error: 78.0853\n",
            "Epoch 386: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.5232 - mean_absolute_error: 75.5232 - val_loss: 40.3179 - val_mean_absolute_error: 40.3179\n",
            "Epoch 387/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 112.1950 - mean_absolute_error: 112.1950\n",
            "Epoch 387: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 117.2783 - mean_absolute_error: 117.2783 - val_loss: 261.3818 - val_mean_absolute_error: 261.3818\n",
            "Epoch 388/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 176.3647 - mean_absolute_error: 176.3647\n",
            "Epoch 388: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 167.2992 - mean_absolute_error: 167.2992 - val_loss: 33.9290 - val_mean_absolute_error: 33.9290\n",
            "Epoch 389/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 141.4200 - mean_absolute_error: 141.4200\n",
            "Epoch 389: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 138.3748 - mean_absolute_error: 138.3748 - val_loss: 80.0928 - val_mean_absolute_error: 80.0928\n",
            "Epoch 390/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 102.7895 - mean_absolute_error: 102.7895\n",
            "Epoch 390: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 98.1576 - mean_absolute_error: 98.1576 - val_loss: 145.6951 - val_mean_absolute_error: 145.6951\n",
            "Epoch 391/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 79.4451 - mean_absolute_error: 79.4451\n",
            "Epoch 391: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.8501 - mean_absolute_error: 75.8501 - val_loss: 145.2628 - val_mean_absolute_error: 145.2628\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 138.2924 - mean_absolute_error: 138.2924\n",
            "Epoch 392: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 138.2924 - mean_absolute_error: 138.2924 - val_loss: 39.5394 - val_mean_absolute_error: 39.5394\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 64.9894 - mean_absolute_error: 64.9894\n",
            "Epoch 393: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 64.9894 - mean_absolute_error: 64.9894 - val_loss: 70.3678 - val_mean_absolute_error: 70.3678\n",
            "Epoch 394/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 86.1079 - mean_absolute_error: 86.1079\n",
            "Epoch 394: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 85.2875 - mean_absolute_error: 85.2875 - val_loss: 73.5076 - val_mean_absolute_error: 73.5076\n",
            "Epoch 395/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 87.9394 - mean_absolute_error: 87.9394\n",
            "Epoch 395: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 97.6849 - mean_absolute_error: 97.6849 - val_loss: 80.3813 - val_mean_absolute_error: 80.3813\n",
            "Epoch 396/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 111.6257 - mean_absolute_error: 111.6257\n",
            "Epoch 396: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 112.7041 - mean_absolute_error: 112.7041 - val_loss: 124.2754 - val_mean_absolute_error: 124.2754\n",
            "Epoch 397/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 54.4665 - mean_absolute_error: 54.4665\n",
            "Epoch 397: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 70.1955 - mean_absolute_error: 70.1955 - val_loss: 72.2235 - val_mean_absolute_error: 72.2235\n",
            "Epoch 398/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 78.9382 - mean_absolute_error: 78.9382\n",
            "Epoch 398: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.3219 - mean_absolute_error: 75.3219 - val_loss: 133.3314 - val_mean_absolute_error: 133.3314\n",
            "Epoch 399/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 90.2902 - mean_absolute_error: 90.2902  \n",
            "Epoch 399: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 87.1569 - mean_absolute_error: 87.1569 - val_loss: 157.1016 - val_mean_absolute_error: 157.1016\n",
            "Epoch 400/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 97.9955 - mean_absolute_error: 97.9955  \n",
            "Epoch 400: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 96.2681 - mean_absolute_error: 96.2681 - val_loss: 238.9607 - val_mean_absolute_error: 238.9607\n",
            "Epoch 401/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 91.7346 - mean_absolute_error: 91.7346  \n",
            "Epoch 401: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 89.5260 - mean_absolute_error: 89.5260 - val_loss: 16.2225 - val_mean_absolute_error: 16.2225\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 65.5041 - mean_absolute_error: 65.5041\n",
            "Epoch 402: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 65.5041 - mean_absolute_error: 65.5041 - val_loss: 22.0768 - val_mean_absolute_error: 22.0768\n",
            "Epoch 403/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 77.5576 - mean_absolute_error: 77.5576\n",
            "Epoch 403: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 74.2653 - mean_absolute_error: 74.2653 - val_loss: 14.8243 - val_mean_absolute_error: 14.8243\n",
            "Epoch 404/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 81.2461 - mean_absolute_error: 81.2461\n",
            "Epoch 404: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 74.7032 - mean_absolute_error: 74.7032 - val_loss: 4.9064 - val_mean_absolute_error: 4.9064\n",
            "Epoch 405/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 67.2164 - mean_absolute_error: 67.2164\n",
            "Epoch 405: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 70.5187 - mean_absolute_error: 70.5187 - val_loss: 203.3017 - val_mean_absolute_error: 203.3017\n",
            "Epoch 406/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 157.6388 - mean_absolute_error: 157.6388\n",
            "Epoch 406: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 142.5836 - mean_absolute_error: 142.5836 - val_loss: 94.3916 - val_mean_absolute_error: 94.3916\n",
            "Epoch 407/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 151.6178 - mean_absolute_error: 151.6178\n",
            "Epoch 407: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 154.0280 - mean_absolute_error: 154.0280 - val_loss: 29.7838 - val_mean_absolute_error: 29.7838\n",
            "Epoch 408/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 122.5085 - mean_absolute_error: 122.5085\n",
            "Epoch 408: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 121.6504 - mean_absolute_error: 121.6504 - val_loss: 28.7134 - val_mean_absolute_error: 28.7134\n",
            "Epoch 409/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 60.0023 - mean_absolute_error: 60.0023\n",
            "Epoch 409: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 61.7473 - mean_absolute_error: 61.7473 - val_loss: 82.8667 - val_mean_absolute_error: 82.8667\n",
            "Epoch 410/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 106.8071 - mean_absolute_error: 106.8071\n",
            "Epoch 410: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 111.6911 - mean_absolute_error: 111.6911 - val_loss: 181.2839 - val_mean_absolute_error: 181.2839\n",
            "Epoch 411/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 71.6345 - mean_absolute_error: 71.6345\n",
            "Epoch 411: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 69.3337 - mean_absolute_error: 69.3337 - val_loss: 2.8965 - val_mean_absolute_error: 2.8965\n",
            "Epoch 412/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 130.7664 - mean_absolute_error: 130.7664\n",
            "Epoch 412: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 128.1832 - mean_absolute_error: 128.1832 - val_loss: 97.4381 - val_mean_absolute_error: 97.4381\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 107.4602 - mean_absolute_error: 107.4602\n",
            "Epoch 413: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 107.4602 - mean_absolute_error: 107.4602 - val_loss: 102.1517 - val_mean_absolute_error: 102.1517\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 149.5312 - mean_absolute_error: 149.5312\n",
            "Epoch 414: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 149.5312 - mean_absolute_error: 149.5312 - val_loss: 30.9221 - val_mean_absolute_error: 30.9221\n",
            "Epoch 415/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 134.2104 - mean_absolute_error: 134.2104\n",
            "Epoch 415: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 126.8302 - mean_absolute_error: 126.8302 - val_loss: 4.8590 - val_mean_absolute_error: 4.8590\n",
            "Epoch 416/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 72.1257 - mean_absolute_error: 72.1257\n",
            "Epoch 416: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 71.4180 - mean_absolute_error: 71.4180 - val_loss: 115.5420 - val_mean_absolute_error: 115.5420\n",
            "Epoch 417/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 57.6601 - mean_absolute_error: 57.6601\n",
            "Epoch 417: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 60.0471 - mean_absolute_error: 60.0471 - val_loss: 128.7611 - val_mean_absolute_error: 128.7611\n",
            "Epoch 418/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 133.6825 - mean_absolute_error: 133.6825\n",
            "Epoch 418: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 136.0624 - mean_absolute_error: 136.0624 - val_loss: 250.1247 - val_mean_absolute_error: 250.1247\n",
            "Epoch 419/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 128.3734 - mean_absolute_error: 128.3734\n",
            "Epoch 419: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 128.3661 - mean_absolute_error: 128.3661 - val_loss: 168.0914 - val_mean_absolute_error: 168.0914\n",
            "Epoch 420/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 147.4418 - mean_absolute_error: 147.4418\n",
            "Epoch 420: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 139.7733 - mean_absolute_error: 139.7733 - val_loss: 55.4320 - val_mean_absolute_error: 55.4320\n",
            "Epoch 421/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 83.8575 - mean_absolute_error: 83.8575\n",
            "Epoch 421: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 78.6322 - mean_absolute_error: 78.6322 - val_loss: 83.7895 - val_mean_absolute_error: 83.7895\n",
            "Epoch 422/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 71.9953 - mean_absolute_error: 71.9953\n",
            "Epoch 422: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 65.7788 - mean_absolute_error: 65.7788 - val_loss: 19.0396 - val_mean_absolute_error: 19.0396\n",
            "Epoch 423/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 92.6292 - mean_absolute_error: 92.6292\n",
            "Epoch 423: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 92.2035 - mean_absolute_error: 92.2035 - val_loss: 199.3043 - val_mean_absolute_error: 199.3043\n",
            "Epoch 424/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 116.2531 - mean_absolute_error: 116.2531\n",
            "Epoch 424: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 113.5308 - mean_absolute_error: 113.5308 - val_loss: 7.6983 - val_mean_absolute_error: 7.6983\n",
            "Epoch 425/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 52.4384 - mean_absolute_error: 52.4384\n",
            "Epoch 425: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 53.8768 - mean_absolute_error: 53.8768 - val_loss: 164.7985 - val_mean_absolute_error: 164.7985\n",
            "Epoch 426/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 107.6268 - mean_absolute_error: 107.6268\n",
            "Epoch 426: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 101.1524 - mean_absolute_error: 101.1524 - val_loss: 42.1838 - val_mean_absolute_error: 42.1838\n",
            "Epoch 427/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 122.1964 - mean_absolute_error: 122.1964\n",
            "Epoch 427: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 121.1320 - mean_absolute_error: 121.1320 - val_loss: 142.1429 - val_mean_absolute_error: 142.1429\n",
            "Epoch 428/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 102.1633 - mean_absolute_error: 102.1633\n",
            "Epoch 428: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 98.1472 - mean_absolute_error: 98.1472 - val_loss: 120.8400 - val_mean_absolute_error: 120.8400\n",
            "Epoch 429/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 84.8500 - mean_absolute_error: 84.8500\n",
            "Epoch 429: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 78.9691 - mean_absolute_error: 78.9691 - val_loss: 20.0589 - val_mean_absolute_error: 20.0589\n",
            "Epoch 430/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 105.2677 - mean_absolute_error: 105.2677\n",
            "Epoch 430: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 101.6339 - mean_absolute_error: 101.6339 - val_loss: 2.4590 - val_mean_absolute_error: 2.4590\n",
            "Epoch 431/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 106.4577 - mean_absolute_error: 106.4577\n",
            "Epoch 431: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 105.1267 - mean_absolute_error: 105.1267 - val_loss: 90.7309 - val_mean_absolute_error: 90.7309\n",
            "Epoch 432/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 67.7673 - mean_absolute_error: 67.7673\n",
            "Epoch 432: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 71.3176 - mean_absolute_error: 71.3176 - val_loss: 112.2795 - val_mean_absolute_error: 112.2795\n",
            "Epoch 433/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 118.8992 - mean_absolute_error: 118.8992\n",
            "Epoch 433: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 114.2591 - mean_absolute_error: 114.2591 - val_loss: 34.8843 - val_mean_absolute_error: 34.8843\n",
            "Epoch 434/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 83.9152 - mean_absolute_error: 83.9152\n",
            "Epoch 434: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 90.3948 - mean_absolute_error: 90.3948 - val_loss: 103.7645 - val_mean_absolute_error: 103.7645\n",
            "Epoch 435/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 73.7640 - mean_absolute_error: 73.7640\n",
            "Epoch 435: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 74.0839 - mean_absolute_error: 74.0839 - val_loss: 103.7415 - val_mean_absolute_error: 103.7415\n",
            "Epoch 436/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 65.4882 - mean_absolute_error: 65.4882\n",
            "Epoch 436: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 65.6139 - mean_absolute_error: 65.6139 - val_loss: 2.3707 - val_mean_absolute_error: 2.3707\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 136.6546 - mean_absolute_error: 136.6546\n",
            "Epoch 437: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 136.6546 - mean_absolute_error: 136.6546 - val_loss: 78.9417 - val_mean_absolute_error: 78.9417\n",
            "Epoch 438/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 65.1901 - mean_absolute_error: 65.1901\n",
            "Epoch 438: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 72.4402 - mean_absolute_error: 72.4402 - val_loss: 113.6644 - val_mean_absolute_error: 113.6644\n",
            "Epoch 439/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 89.5343 - mean_absolute_error: 89.5343\n",
            "Epoch 439: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 95.2614 - mean_absolute_error: 95.2614 - val_loss: 143.7741 - val_mean_absolute_error: 143.7741\n",
            "Epoch 440/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 62.3096 - mean_absolute_error: 62.3096\n",
            "Epoch 440: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 66.1271 - mean_absolute_error: 66.1271 - val_loss: 178.6656 - val_mean_absolute_error: 178.6656\n",
            "Epoch 441/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 127.3625 - mean_absolute_error: 127.3625\n",
            "Epoch 441: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 128.5167 - mean_absolute_error: 128.5167 - val_loss: 316.8900 - val_mean_absolute_error: 316.8900\n",
            "Epoch 442/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 144.2027 - mean_absolute_error: 144.2027\n",
            "Epoch 442: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 138.9773 - mean_absolute_error: 138.9773 - val_loss: 184.8062 - val_mean_absolute_error: 184.8062\n",
            "Epoch 443/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 75.6979 - mean_absolute_error: 75.6979\n",
            "Epoch 443: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 89.0505 - mean_absolute_error: 89.0505 - val_loss: 39.9751 - val_mean_absolute_error: 39.9751\n",
            "Epoch 444/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 145.1820 - mean_absolute_error: 145.1820\n",
            "Epoch 444: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 137.5945 - mean_absolute_error: 137.5945 - val_loss: 1.5370 - val_mean_absolute_error: 1.5370\n",
            "Epoch 445/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 65.1350 - mean_absolute_error: 65.1350\n",
            "Epoch 445: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 72.4638 - mean_absolute_error: 72.4638 - val_loss: 75.3634 - val_mean_absolute_error: 75.3634\n",
            "Epoch 446/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 54.5491 - mean_absolute_error: 54.5491\n",
            "Epoch 446: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 53.1732 - mean_absolute_error: 53.1732 - val_loss: 116.4425 - val_mean_absolute_error: 116.4425\n",
            "Epoch 447/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 77.0487 - mean_absolute_error: 77.0487\n",
            "Epoch 447: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 74.0441 - mean_absolute_error: 74.0441 - val_loss: 42.1327 - val_mean_absolute_error: 42.1327\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 78.9392 - mean_absolute_error: 78.9392\n",
            "Epoch 448: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 78.9392 - mean_absolute_error: 78.9392 - val_loss: 89.0798 - val_mean_absolute_error: 89.0798\n",
            "Epoch 449/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 102.8320 - mean_absolute_error: 102.8320\n",
            "Epoch 449: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 107.4560 - mean_absolute_error: 107.4560 - val_loss: 68.0908 - val_mean_absolute_error: 68.0908\n",
            "Epoch 450/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 185.9202 - mean_absolute_error: 185.9202\n",
            "Epoch 450: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 191.3686 - mean_absolute_error: 191.3686 - val_loss: 222.3433 - val_mean_absolute_error: 222.3433\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 131.6514 - mean_absolute_error: 131.6514\n",
            "Epoch 451: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 131.6514 - mean_absolute_error: 131.6514 - val_loss: 185.8668 - val_mean_absolute_error: 185.8668\n",
            "Epoch 452/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 89.7751 - mean_absolute_error: 89.7751  \n",
            "Epoch 452: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 79.1409 - mean_absolute_error: 79.1409 - val_loss: 10.0637 - val_mean_absolute_error: 10.0637\n",
            "Epoch 453/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 88.1367 - mean_absolute_error: 88.1367\n",
            "Epoch 453: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 86.8852 - mean_absolute_error: 86.8852 - val_loss: 33.9432 - val_mean_absolute_error: 33.9432\n",
            "Epoch 454/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 87.2670 - mean_absolute_error: 87.2670\n",
            "Epoch 454: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 112.8148 - mean_absolute_error: 112.8148 - val_loss: 133.4852 - val_mean_absolute_error: 133.4852\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 95.9494 - mean_absolute_error: 95.9494  \n",
            "Epoch 455: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 95.9494 - mean_absolute_error: 95.9494 - val_loss: 103.3094 - val_mean_absolute_error: 103.3094\n",
            "Epoch 456/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 44.8489 - mean_absolute_error: 44.8489\n",
            "Epoch 456: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 47.0094 - mean_absolute_error: 47.0094 - val_loss: 114.9810 - val_mean_absolute_error: 114.9810\n",
            "Epoch 457/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 88.0475 - mean_absolute_error: 88.0475\n",
            "Epoch 457: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 87.8316 - mean_absolute_error: 87.8316 - val_loss: 36.1895 - val_mean_absolute_error: 36.1895\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 76.6266 - mean_absolute_error: 76.6266\n",
            "Epoch 458: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 76.6266 - mean_absolute_error: 76.6266 - val_loss: 191.6268 - val_mean_absolute_error: 191.6268\n",
            "Epoch 459/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 100.3374 - mean_absolute_error: 100.3374\n",
            "Epoch 459: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 91.0234 - mean_absolute_error: 91.0234 - val_loss: 1.2415 - val_mean_absolute_error: 1.2415\n",
            "Epoch 460/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 76.6724 - mean_absolute_error: 76.6724\n",
            "Epoch 460: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 76.2239 - mean_absolute_error: 76.2239 - val_loss: 101.4826 - val_mean_absolute_error: 101.4826\n",
            "Epoch 461/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 70.8005 - mean_absolute_error: 70.8005\n",
            "Epoch 461: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 68.7772 - mean_absolute_error: 68.7772 - val_loss: 42.1794 - val_mean_absolute_error: 42.1794\n",
            "Epoch 462/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 98.0553 - mean_absolute_error: 98.0553\n",
            "Epoch 462: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 101.0711 - mean_absolute_error: 101.0711 - val_loss: 50.0505 - val_mean_absolute_error: 50.0505\n",
            "Epoch 463/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 85.9650 - mean_absolute_error: 85.9650\n",
            "Epoch 463: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 90.1397 - mean_absolute_error: 90.1397 - val_loss: 110.9075 - val_mean_absolute_error: 110.9075\n",
            "Epoch 464/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 55.9034 - mean_absolute_error: 55.9034\n",
            "Epoch 464: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 59.2465 - mean_absolute_error: 59.2465 - val_loss: 30.2461 - val_mean_absolute_error: 30.2461\n",
            "Epoch 465/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 145.0404 - mean_absolute_error: 145.0404\n",
            "Epoch 465: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 161.6586 - mean_absolute_error: 161.6586 - val_loss: 175.6123 - val_mean_absolute_error: 175.6123\n",
            "Epoch 466/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 52.7889 - mean_absolute_error: 52.7889\n",
            "Epoch 466: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 60.5163 - mean_absolute_error: 60.5163 - val_loss: 208.4124 - val_mean_absolute_error: 208.4124\n",
            "Epoch 467/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 93.7898 - mean_absolute_error: 93.7898\n",
            "Epoch 467: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 95.4832 - mean_absolute_error: 95.4832 - val_loss: 171.5269 - val_mean_absolute_error: 171.5269\n",
            "Epoch 468/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 179.0182 - mean_absolute_error: 179.0182\n",
            "Epoch 468: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 200.2636 - mean_absolute_error: 200.2636 - val_loss: 87.5467 - val_mean_absolute_error: 87.5467\n",
            "Epoch 469/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 104.5688 - mean_absolute_error: 104.5688\n",
            "Epoch 469: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 107.6280 - mean_absolute_error: 107.6280 - val_loss: 98.9672 - val_mean_absolute_error: 98.9672\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 71.9269 - mean_absolute_error: 71.9269\n",
            "Epoch 470: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 71.9269 - mean_absolute_error: 71.9269 - val_loss: 160.9046 - val_mean_absolute_error: 160.9046\n",
            "Epoch 471/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 72.2816 - mean_absolute_error: 72.2816\n",
            "Epoch 471: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.6224 - mean_absolute_error: 75.6224 - val_loss: 66.8034 - val_mean_absolute_error: 66.8034\n",
            "Epoch 472/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 90.6794 - mean_absolute_error: 90.6794  \n",
            "Epoch 472: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 88.7160 - mean_absolute_error: 88.7160 - val_loss: 128.5900 - val_mean_absolute_error: 128.5900\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 75.0085 - mean_absolute_error: 75.0085\n",
            "Epoch 473: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 75.0085 - mean_absolute_error: 75.0085 - val_loss: 42.3539 - val_mean_absolute_error: 42.3539\n",
            "Epoch 474/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 37.6494 - mean_absolute_error: 37.6494\n",
            "Epoch 474: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 42.2814 - mean_absolute_error: 42.2814 - val_loss: 3.5722 - val_mean_absolute_error: 3.5722\n",
            "Epoch 475/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 92.2432 - mean_absolute_error: 92.2432\n",
            "Epoch 475: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 88.9272 - mean_absolute_error: 88.9272 - val_loss: 102.3206 - val_mean_absolute_error: 102.3206\n",
            "Epoch 476/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 73.6572 - mean_absolute_error: 73.6572\n",
            "Epoch 476: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 72.1219 - mean_absolute_error: 72.1219 - val_loss: 126.5866 - val_mean_absolute_error: 126.5866\n",
            "Epoch 477/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 147.4494 - mean_absolute_error: 147.4494\n",
            "Epoch 477: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 150.3751 - mean_absolute_error: 150.3751 - val_loss: 64.3128 - val_mean_absolute_error: 64.3128\n",
            "Epoch 478/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 64.7678 - mean_absolute_error: 64.7678\n",
            "Epoch 478: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 70.8761 - mean_absolute_error: 70.8761 - val_loss: 75.4707 - val_mean_absolute_error: 75.4707\n",
            "Epoch 479/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 101.5139 - mean_absolute_error: 101.5139\n",
            "Epoch 479: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 99.3368 - mean_absolute_error: 99.3368 - val_loss: 66.6703 - val_mean_absolute_error: 66.6703\n",
            "Epoch 480/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 59.5764 - mean_absolute_error: 59.5764\n",
            "Epoch 480: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 59.1888 - mean_absolute_error: 59.1888 - val_loss: 40.1573 - val_mean_absolute_error: 40.1573\n",
            "Epoch 481/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 91.0164 - mean_absolute_error: 91.0164\n",
            "Epoch 481: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 88.5366 - mean_absolute_error: 88.5366 - val_loss: 93.2936 - val_mean_absolute_error: 93.2936\n",
            "Epoch 482/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 76.0443 - mean_absolute_error: 76.0443\n",
            "Epoch 482: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 73.7844 - mean_absolute_error: 73.7844 - val_loss: 36.4927 - val_mean_absolute_error: 36.4927\n",
            "Epoch 483/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 101.7981 - mean_absolute_error: 101.7981\n",
            "Epoch 483: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 94.1229 - mean_absolute_error: 94.1229 - val_loss: 77.2859 - val_mean_absolute_error: 77.2859\n",
            "Epoch 484/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 110.8870 - mean_absolute_error: 110.8870\n",
            "Epoch 484: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 99.8631 - mean_absolute_error: 99.8631 - val_loss: 48.7540 - val_mean_absolute_error: 48.7540\n",
            "Epoch 485/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 94.7707 - mean_absolute_error: 94.7707\n",
            "Epoch 485: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 108.7430 - mean_absolute_error: 108.7430 - val_loss: 117.1092 - val_mean_absolute_error: 117.1092\n",
            "Epoch 486/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 50.9470 - mean_absolute_error: 50.9470\n",
            "Epoch 486: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 50.8125 - mean_absolute_error: 50.8125 - val_loss: 149.8681 - val_mean_absolute_error: 149.8681\n",
            "Epoch 487/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 95.0151 - mean_absolute_error: 95.0151  \n",
            "Epoch 487: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 90.2427 - mean_absolute_error: 90.2427 - val_loss: 83.0507 - val_mean_absolute_error: 83.0507\n",
            "Epoch 488/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 73.8976 - mean_absolute_error: 73.8976\n",
            "Epoch 488: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 69.6020 - mean_absolute_error: 69.6020 - val_loss: 93.3941 - val_mean_absolute_error: 93.3941\n",
            "Epoch 489/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 97.4786 - mean_absolute_error: 97.4786  \n",
            "Epoch 489: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 97.4611 - mean_absolute_error: 97.4611 - val_loss: 122.8958 - val_mean_absolute_error: 122.8958\n",
            "Epoch 490/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 114.5434 - mean_absolute_error: 114.5434\n",
            "Epoch 490: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 112.7332 - mean_absolute_error: 112.7332 - val_loss: 241.1986 - val_mean_absolute_error: 241.1986\n",
            "Epoch 491/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 124.9225 - mean_absolute_error: 124.9225\n",
            "Epoch 491: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 115.0604 - mean_absolute_error: 115.0604 - val_loss: 68.2759 - val_mean_absolute_error: 68.2759\n",
            "Epoch 492/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 106.9212 - mean_absolute_error: 106.9212\n",
            "Epoch 492: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 96.0062 - mean_absolute_error: 96.0062 - val_loss: 5.3283 - val_mean_absolute_error: 5.3283\n",
            "Epoch 493/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 70.1636 - mean_absolute_error: 70.1636\n",
            "Epoch 493: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 68.1242 - mean_absolute_error: 68.1242 - val_loss: 25.3574 - val_mean_absolute_error: 25.3574\n",
            "Epoch 494/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 52.7642 - mean_absolute_error: 52.7642\n",
            "Epoch 494: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 51.4390 - mean_absolute_error: 51.4390 - val_loss: 19.8273 - val_mean_absolute_error: 19.8273\n",
            "Epoch 495/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 89.4195 - mean_absolute_error: 89.4195\n",
            "Epoch 495: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 88.9411 - mean_absolute_error: 88.9411 - val_loss: 101.8520 - val_mean_absolute_error: 101.8520\n",
            "Epoch 496/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 104.8858 - mean_absolute_error: 104.8858\n",
            "Epoch 496: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 99.0360 - mean_absolute_error: 99.0360 - val_loss: 106.0990 - val_mean_absolute_error: 106.0990\n",
            "Epoch 497/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 71.6341 - mean_absolute_error: 71.6341\n",
            "Epoch 497: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 78.8186 - mean_absolute_error: 78.8186 - val_loss: 21.7184 - val_mean_absolute_error: 21.7184\n",
            "Epoch 498/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 119.9126 - mean_absolute_error: 119.9126\n",
            "Epoch 498: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 110.5831 - mean_absolute_error: 110.5831 - val_loss: 49.6927 - val_mean_absolute_error: 49.6927\n",
            "Epoch 499/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 162.0582 - mean_absolute_error: 162.0582\n",
            "Epoch 499: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 163.2692 - mean_absolute_error: 163.2692 - val_loss: 205.7762 - val_mean_absolute_error: 205.7762\n",
            "Epoch 500/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 183.2662 - mean_absolute_error: 183.2662\n",
            "Epoch 500: val_loss did not improve from 0.92161\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 191.3929 - mean_absolute_error: 191.3929 - val_loss: 5.3656 - val_mean_absolute_error: 5.3656\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9c502e7a10>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = NN_model.predict(test)"
      ],
      "metadata": {
        "id": "qxLFR9BKhZpv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_1920wWPjNov"
      },
      "execution_count": 34,
      "outputs": []
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "interpreter": {
      "hash": "b207a59bfd57b21c402ddff15795247c8e2f2ee069fae01181918acf1411bba0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "predict_notebook.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}